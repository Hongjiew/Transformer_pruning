{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5a61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from tokenrank_vit import TokenRankVisionTransformer\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table, flop_count_str\n",
    "from beit import BeitTeacher\n",
    "from tokenrank_vit import TokenRankVisionTransformer\n",
    "from tokenrank_beit import TokenRankBeit\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor\n",
    "import requests\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c231cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "feature_extractor = ViTFeatureExtractor()\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea190c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 3.555G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  3.497G     |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.338G    |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.34M    |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    0.128G   |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     78.299M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     26.1M   |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.34M    |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.209G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     0.104G  |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     0.104G  |\n",
      "|   blocks.4                 |   1.774M               |   0.338G    |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.34M    |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    0.128G   |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     78.299M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     26.1M   |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.34M    |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.209G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     0.104G  |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     0.104G  |\n",
      "|   blocks.5                 |   1.774M               |   0.338G    |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.34M    |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    0.129G   |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     78.299M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     26.1M   |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.34M    |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.209G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     0.104G  |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     0.104G  |\n",
      "|   blocks.6                 |   1.774M               |   0.265G    |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.271M   |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    98.434M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     62.374M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     20.791M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.271M   |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    0.166G   |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     83.165M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     83.165M |\n",
      "|   blocks.7                 |   1.774M               |   0.265G    |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.271M   |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    98.434M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     62.374M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     20.791M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.271M   |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    0.166G   |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     83.165M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     83.165M |\n",
      "|   blocks.8                 |   1.774M               |   0.265G    |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.271M   |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    98.533M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     62.374M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     20.791M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.271M   |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    0.166G   |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     83.165M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     83.165M |\n",
      "|   blocks.9                 |   1.774M               |   0.183G    |\n",
      "|    blocks.9.norm1          |    0.768K              |    0.19M    |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    65.92M   |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     43.794M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     14.598M |\n",
      "|    blocks.9.norm2          |    0.768K              |    0.19M    |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    0.117G   |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     58.393M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     58.393M |\n",
      "|   blocks.10                |   1.774M               |   0.183G    |\n",
      "|    blocks.10.norm1         |    0.768K              |    0.19M    |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    65.92M   |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     43.794M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     14.598M |\n",
      "|    blocks.10.norm2         |    0.768K              |    0.19M    |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    0.117G   |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     58.393M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     58.393M |\n",
      "|   blocks.11                |   1.774M               |   0.183G    |\n",
      "|    blocks.11.norm1         |    0.768K              |    0.19M    |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    65.92M   |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     43.794M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     14.598M |\n",
      "|    blocks.11.norm2         |    0.768K              |    0.19M    |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    0.117G   |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     58.393M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     58.393M |\n",
      "|  norm                      |  0.768K                |  0.19M      |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 3.56G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 3.5G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.34G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.13G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 78.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.21G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.34G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.13G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 78.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.21G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.34G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.13G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 78.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.21G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.27G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 98.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 62.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.17G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.27G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 98.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 62.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.17G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.27G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 98.53M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 62.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.17G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 65.92M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 43.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 65.92M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 43.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 65.92M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 43.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.19M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "3555252911\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "#prune_list = [2,3,4,5,6,7,8,9,10]\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "#retain_rate_list = [0.99,0.98,0.99,0.98,0.95,0.90,0.85,0.9,0.85]\n",
    "retain_rate_list=[0.9,0.8,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd3ab417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.980 ( 1.980)\tLoss 1.8879e+00 (1.8879e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  81.25 ( 81.25)\n",
      "Test: [ 20/391]\tTime  0.958 ( 0.217)\tLoss 1.7934e+00 (1.7024e+00)\tAcc@1  67.19 ( 66.22)\tAcc@5  80.47 ( 84.82)\n",
      "Test: [ 40/391]\tTime  0.934 ( 0.172)\tLoss 2.4957e+00 (1.6417e+00)\tAcc@1  50.78 ( 67.44)\tAcc@5  71.09 ( 85.63)\n",
      "Test: [ 60/391]\tTime  0.803 ( 0.151)\tLoss 1.7242e+00 (1.6112e+00)\tAcc@1  63.28 ( 68.35)\tAcc@5  82.81 ( 85.89)\n",
      "Test: [ 80/391]\tTime  0.771 ( 0.142)\tLoss 2.0455e+00 (1.7491e+00)\tAcc@1  53.91 ( 64.37)\tAcc@5  83.59 ( 84.38)\n",
      "Test: [100/391]\tTime  0.834 ( 0.136)\tLoss 1.7534e+00 (1.7456e+00)\tAcc@1  57.81 ( 63.71)\tAcc@5  85.16 ( 84.86)\n",
      "Test: [120/391]\tTime  0.726 ( 0.134)\tLoss 1.8857e+00 (1.7480e+00)\tAcc@1  57.81 ( 63.62)\tAcc@5  82.03 ( 84.93)\n",
      "Test: [140/391]\tTime  0.709 ( 0.132)\tLoss 2.0885e+00 (1.7565e+00)\tAcc@1  52.34 ( 63.51)\tAcc@5  84.38 ( 84.83)\n",
      "Test: [160/391]\tTime  0.943 ( 0.134)\tLoss 2.4019e+00 (1.7834e+00)\tAcc@1  49.22 ( 63.03)\tAcc@5  75.00 ( 84.39)\n",
      "Test: [180/391]\tTime  0.711 ( 0.131)\tLoss 3.2839e+00 (1.8837e+00)\tAcc@1  28.12 ( 61.14)\tAcc@5  66.41 ( 82.97)\n",
      "Test: [200/391]\tTime  0.399 ( 0.127)\tLoss 2.3848e+00 (1.9686e+00)\tAcc@1  48.44 ( 59.55)\tAcc@5  73.44 ( 81.62)\n",
      "Test: [220/391]\tTime  0.455 ( 0.126)\tLoss 1.8475e+00 (2.0221e+00)\tAcc@1  64.84 ( 58.69)\tAcc@5  82.03 ( 80.77)\n",
      "Test: [240/391]\tTime  0.434 ( 0.125)\tLoss 2.2580e+00 (2.0672e+00)\tAcc@1  60.16 ( 58.04)\tAcc@5  74.22 ( 80.03)\n",
      "Test: [260/391]\tTime  0.449 ( 0.124)\tLoss 2.9018e+00 (2.1274e+00)\tAcc@1  44.53 ( 56.88)\tAcc@5  69.53 ( 79.12)\n",
      "Test: [280/391]\tTime  0.151 ( 0.124)\tLoss 2.6128e+00 (2.1635e+00)\tAcc@1  41.41 ( 56.20)\tAcc@5  71.88 ( 78.54)\n",
      "Test: [300/391]\tTime  0.025 ( 0.122)\tLoss 2.2319e+00 (2.2037e+00)\tAcc@1  54.69 ( 55.55)\tAcc@5  78.12 ( 77.88)\n",
      "Test: [320/391]\tTime  0.025 ( 0.122)\tLoss 2.0694e+00 (2.2393e+00)\tAcc@1  53.91 ( 54.88)\tAcc@5  78.12 ( 77.30)\n",
      "Test: [340/391]\tTime  0.032 ( 0.121)\tLoss 2.5365e+00 (2.2747e+00)\tAcc@1  44.53 ( 54.18)\tAcc@5  75.00 ( 76.75)\n",
      "Test: [360/391]\tTime  0.026 ( 0.121)\tLoss 2.9823e+00 (2.2999e+00)\tAcc@1  37.50 ( 53.70)\tAcc@5  70.31 ( 76.35)\n",
      "Test: [380/391]\tTime  0.036 ( 0.120)\tLoss 1.9855e+00 (2.2904e+00)\tAcc@1  61.72 ( 53.88)\tAcc@5  84.38 ( 76.52)\n",
      " * Acc@1 54.116 Acc@5 76.658\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b7c11a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.929 ( 1.929)\tLoss 1.2412e+00 (1.2412e+00)\tAcc@1  78.91 ( 78.91)\tAcc@5  92.19 ( 92.19)\n",
      "Test: [ 20/391]\tTime  0.891 ( 0.204)\tLoss 1.9465e+00 (1.4719e+00)\tAcc@1  64.06 ( 70.98)\tAcc@5  77.34 ( 87.83)\n",
      "Test: [ 40/391]\tTime  0.849 ( 0.163)\tLoss 2.0460e+00 (1.4491e+00)\tAcc@1  61.72 ( 71.59)\tAcc@5  78.12 ( 88.38)\n",
      "Test: [ 60/391]\tTime  0.766 ( 0.144)\tLoss 1.7800e+00 (1.4472e+00)\tAcc@1  64.06 ( 71.64)\tAcc@5  81.25 ( 88.10)\n",
      "Test: [ 80/391]\tTime  0.769 ( 0.134)\tLoss 2.1462e+00 (1.5937e+00)\tAcc@1  50.78 ( 67.32)\tAcc@5  76.56 ( 86.49)\n",
      "Test: [100/391]\tTime  0.777 ( 0.128)\tLoss 1.6680e+00 (1.6273e+00)\tAcc@1  60.16 ( 65.86)\tAcc@5  85.16 ( 86.46)\n",
      "Test: [120/391]\tTime  0.698 ( 0.127)\tLoss 1.3875e+00 (1.6174e+00)\tAcc@1  71.88 ( 65.93)\tAcc@5  91.41 ( 86.94)\n",
      "Test: [140/391]\tTime  0.677 ( 0.125)\tLoss 1.6497e+00 (1.6099e+00)\tAcc@1  60.16 ( 66.10)\tAcc@5  90.62 ( 87.13)\n",
      "Test: [160/391]\tTime  0.977 ( 0.127)\tLoss 2.1652e+00 (1.6340e+00)\tAcc@1  55.47 ( 65.84)\tAcc@5  82.81 ( 86.66)\n",
      "Test: [180/391]\tTime  0.807 ( 0.124)\tLoss 3.0002e+00 (1.7260e+00)\tAcc@1  37.50 ( 64.05)\tAcc@5  67.19 ( 85.29)\n",
      "Test: [200/391]\tTime  0.624 ( 0.121)\tLoss 1.9609e+00 (1.8025e+00)\tAcc@1  57.81 ( 62.53)\tAcc@5  80.47 ( 84.12)\n",
      "Test: [220/391]\tTime  0.645 ( 0.120)\tLoss 1.7168e+00 (1.8518e+00)\tAcc@1  67.97 ( 61.74)\tAcc@5  83.59 ( 83.39)\n",
      "Test: [240/391]\tTime  0.328 ( 0.119)\tLoss 2.0393e+00 (1.8887e+00)\tAcc@1  64.06 ( 61.25)\tAcc@5  78.12 ( 82.80)\n",
      "Test: [260/391]\tTime  0.363 ( 0.119)\tLoss 2.8195e+00 (1.9437e+00)\tAcc@1  42.19 ( 60.06)\tAcc@5  67.97 ( 82.03)\n",
      "Test: [280/391]\tTime  0.190 ( 0.118)\tLoss 2.3419e+00 (1.9687e+00)\tAcc@1  48.44 ( 59.65)\tAcc@5  82.03 ( 81.63)\n",
      "Test: [300/391]\tTime  0.034 ( 0.117)\tLoss 1.8863e+00 (2.0028e+00)\tAcc@1  62.50 ( 59.12)\tAcc@5  82.03 ( 81.13)\n",
      "Test: [320/391]\tTime  0.027 ( 0.116)\tLoss 1.7193e+00 (2.0287e+00)\tAcc@1  67.19 ( 58.72)\tAcc@5  86.72 ( 80.69)\n",
      "Test: [340/391]\tTime  0.027 ( 0.116)\tLoss 2.1388e+00 (2.0566e+00)\tAcc@1  54.69 ( 58.12)\tAcc@5  78.12 ( 80.26)\n",
      "Test: [360/391]\tTime  0.026 ( 0.116)\tLoss 2.5963e+00 (2.0786e+00)\tAcc@1  46.09 ( 57.74)\tAcc@5  77.34 ( 79.97)\n",
      "Test: [380/391]\tTime  0.035 ( 0.115)\tLoss 1.8291e+00 (2.0775e+00)\tAcc@1  57.81 ( 57.75)\tAcc@5  86.72 ( 80.02)\n",
      " * Acc@1 57.946 Acc@5 80.152\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4995005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.980 ( 1.980)\tLoss 1.5944e+00 (1.5944e+00)\tAcc@1  71.88 ( 71.88)\tAcc@5  85.94 ( 85.94)\n",
      "Test: [ 20/391]\tTime  0.906 ( 0.213)\tLoss 1.6819e+00 (1.5359e+00)\tAcc@1  68.75 ( 68.68)\tAcc@5  82.03 ( 86.90)\n",
      "Test: [ 40/391]\tTime  0.951 ( 0.171)\tLoss 1.9657e+00 (1.4803e+00)\tAcc@1  64.06 ( 69.87)\tAcc@5  79.69 ( 87.92)\n",
      "Test: [ 60/391]\tTime  0.816 ( 0.151)\tLoss 1.6340e+00 (1.4574e+00)\tAcc@1  63.28 ( 70.77)\tAcc@5  84.38 ( 87.95)\n",
      "Test: [ 80/391]\tTime  0.825 ( 0.141)\tLoss 1.9996e+00 (1.5920e+00)\tAcc@1  53.91 ( 66.80)\tAcc@5  82.03 ( 86.68)\n",
      "Test: [100/391]\tTime  0.622 ( 0.134)\tLoss 1.4859e+00 (1.6000e+00)\tAcc@1  66.41 ( 66.13)\tAcc@5  90.62 ( 86.93)\n",
      "Test: [120/391]\tTime  0.464 ( 0.132)\tLoss 1.7419e+00 (1.5923e+00)\tAcc@1  63.28 ( 66.11)\tAcc@5  85.16 ( 87.16)\n",
      "Test: [140/391]\tTime  0.327 ( 0.129)\tLoss 1.9677e+00 (1.5887e+00)\tAcc@1  53.12 ( 66.32)\tAcc@5  86.72 ( 87.16)\n",
      "Test: [160/391]\tTime  0.870 ( 0.130)\tLoss 2.1959e+00 (1.6063e+00)\tAcc@1  53.12 ( 66.09)\tAcc@5  81.25 ( 86.79)\n",
      "Test: [180/391]\tTime  0.587 ( 0.127)\tLoss 2.7050e+00 (1.6833e+00)\tAcc@1  37.50 ( 64.69)\tAcc@5  69.53 ( 85.62)\n",
      "Test: [200/391]\tTime  0.053 ( 0.124)\tLoss 2.3062e+00 (1.7525e+00)\tAcc@1  46.88 ( 63.34)\tAcc@5  76.56 ( 84.63)\n",
      "Test: [220/391]\tTime  0.025 ( 0.123)\tLoss 1.4229e+00 (1.7909e+00)\tAcc@1  75.00 ( 62.74)\tAcc@5  89.84 ( 84.07)\n",
      "Test: [240/391]\tTime  0.237 ( 0.122)\tLoss 2.2215e+00 (1.8266e+00)\tAcc@1  63.28 ( 62.29)\tAcc@5  74.22 ( 83.53)\n",
      "Test: [260/391]\tTime  0.234 ( 0.122)\tLoss 2.4781e+00 (1.8740e+00)\tAcc@1  52.34 ( 61.25)\tAcc@5  73.44 ( 82.95)\n",
      "Test: [280/391]\tTime  0.029 ( 0.121)\tLoss 2.2940e+00 (1.8995e+00)\tAcc@1  47.66 ( 60.79)\tAcc@5  79.69 ( 82.54)\n",
      "Test: [300/391]\tTime  0.033 ( 0.121)\tLoss 1.8954e+00 (1.9284e+00)\tAcc@1  66.41 ( 60.31)\tAcc@5  80.47 ( 82.10)\n",
      "Test: [320/391]\tTime  0.025 ( 0.122)\tLoss 1.5292e+00 (1.9541e+00)\tAcc@1  68.75 ( 59.85)\tAcc@5  92.19 ( 81.68)\n",
      "Test: [340/391]\tTime  0.025 ( 0.121)\tLoss 1.8509e+00 (1.9849e+00)\tAcc@1  57.81 ( 59.23)\tAcc@5  85.94 ( 81.19)\n",
      "Test: [360/391]\tTime  0.026 ( 0.121)\tLoss 2.0713e+00 (2.0016e+00)\tAcc@1  56.25 ( 58.87)\tAcc@5  82.03 ( 80.94)\n",
      "Test: [380/391]\tTime  0.027 ( 0.121)\tLoss 1.6238e+00 (1.9885e+00)\tAcc@1  64.84 ( 59.12)\tAcc@5  89.84 ( 81.15)\n",
      " * Acc@1 59.390 Acc@5 81.298\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cbd4260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.988 ( 1.988)\tLoss 4.4891e-01 (4.4891e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.690 ( 0.206)\tLoss 6.0852e-01 (6.6687e-01)\tAcc@1  86.72 ( 85.45)\tAcc@5  95.31 ( 97.06)\n",
      "Test: [ 40/391]\tTime  0.686 ( 0.165)\tLoss 6.6657e-01 (6.9179e-01)\tAcc@1  85.94 ( 84.74)\tAcc@5  96.09 ( 96.93)\n",
      "Test: [ 60/391]\tTime  0.592 ( 0.145)\tLoss 8.5541e-01 (6.5545e-01)\tAcc@1  82.03 ( 86.23)\tAcc@5  94.53 ( 96.99)\n",
      "Test: [ 80/391]\tTime  0.607 ( 0.135)\tLoss 8.2408e-01 (7.1391e-01)\tAcc@1  78.12 ( 84.51)\tAcc@5  96.09 ( 96.76)\n",
      "Test: [100/391]\tTime  0.499 ( 0.128)\tLoss 5.8585e-01 (7.1670e-01)\tAcc@1  89.06 ( 84.26)\tAcc@5  96.09 ( 96.77)\n",
      "Test: [120/391]\tTime  0.509 ( 0.127)\tLoss 5.8293e-01 (7.2028e-01)\tAcc@1  90.62 ( 84.14)\tAcc@5  97.66 ( 96.86)\n",
      "Test: [140/391]\tTime  0.459 ( 0.124)\tLoss 9.9558e-01 (7.0931e-01)\tAcc@1  75.78 ( 84.32)\tAcc@5  97.66 ( 97.00)\n",
      "Test: [160/391]\tTime  0.732 ( 0.126)\tLoss 8.4159e-01 (7.1901e-01)\tAcc@1  78.91 ( 84.12)\tAcc@5  95.31 ( 96.85)\n",
      "Test: [180/391]\tTime  0.622 ( 0.124)\tLoss 1.5546e+00 (7.6004e-01)\tAcc@1  61.72 ( 83.04)\tAcc@5  90.62 ( 96.36)\n",
      "Test: [200/391]\tTime  0.450 ( 0.121)\tLoss 6.5930e-01 (8.0433e-01)\tAcc@1  81.25 ( 81.92)\tAcc@5  98.44 ( 95.90)\n",
      "Test: [220/391]\tTime  0.582 ( 0.120)\tLoss 5.0241e-01 (8.2212e-01)\tAcc@1  89.84 ( 81.52)\tAcc@5  97.66 ( 95.73)\n",
      "Test: [240/391]\tTime  0.389 ( 0.119)\tLoss 9.4424e-01 (8.3279e-01)\tAcc@1  80.47 ( 81.37)\tAcc@5  90.62 ( 95.53)\n",
      "Test: [260/391]\tTime  0.476 ( 0.118)\tLoss 9.3092e-01 (8.6392e-01)\tAcc@1  78.12 ( 80.47)\tAcc@5  96.09 ( 95.21)\n",
      "Test: [280/391]\tTime  0.269 ( 0.118)\tLoss 1.0233e+00 (8.7488e-01)\tAcc@1  75.00 ( 80.27)\tAcc@5  92.97 ( 95.10)\n",
      "Test: [300/391]\tTime  0.048 ( 0.117)\tLoss 7.5923e-01 (8.8897e-01)\tAcc@1  84.38 ( 79.94)\tAcc@5  94.53 ( 94.89)\n",
      "Test: [320/391]\tTime  0.047 ( 0.116)\tLoss 6.7843e-01 (9.0217e-01)\tAcc@1  85.94 ( 79.69)\tAcc@5  96.88 ( 94.74)\n",
      "Test: [340/391]\tTime  0.047 ( 0.116)\tLoss 7.2675e-01 (9.1416e-01)\tAcc@1  82.03 ( 79.35)\tAcc@5  96.88 ( 94.66)\n",
      "Test: [360/391]\tTime  0.048 ( 0.115)\tLoss 1.3458e+00 (9.2365e-01)\tAcc@1  71.88 ( 79.10)\tAcc@5  96.09 ( 94.60)\n",
      "Test: [380/391]\tTime  0.048 ( 0.115)\tLoss 8.5499e-01 (9.2149e-01)\tAcc@1  79.69 ( 79.13)\tAcc@5  97.66 ( 94.64)\n",
      " * Acc@1 79.192 Acc@5 94.674\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79e522ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.951 ( 1.951)\tLoss 4.6820e-01 (4.6820e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.667 ( 0.206)\tLoss 5.9990e-01 (6.5778e-01)\tAcc@1  86.72 ( 85.75)\tAcc@5  96.09 ( 97.32)\n",
      "Test: [ 40/391]\tTime  0.651 ( 0.163)\tLoss 6.6262e-01 (6.8607e-01)\tAcc@1  88.28 ( 84.95)\tAcc@5  96.09 ( 97.08)\n",
      "Test: [ 60/391]\tTime  0.577 ( 0.143)\tLoss 8.4649e-01 (6.4977e-01)\tAcc@1  82.03 ( 86.41)\tAcc@5  95.31 ( 97.12)\n",
      "Test: [ 80/391]\tTime  0.630 ( 0.135)\tLoss 7.9279e-01 (7.0600e-01)\tAcc@1  78.12 ( 84.59)\tAcc@5  97.66 ( 96.91)\n",
      "Test: [100/391]\tTime  0.607 ( 0.129)\tLoss 5.6565e-01 (7.0959e-01)\tAcc@1  89.84 ( 84.34)\tAcc@5  96.09 ( 96.92)\n",
      "Test: [120/391]\tTime  0.630 ( 0.128)\tLoss 5.5398e-01 (7.1394e-01)\tAcc@1  91.41 ( 84.30)\tAcc@5  96.88 ( 96.96)\n",
      "Test: [140/391]\tTime  0.544 ( 0.126)\tLoss 9.9865e-01 (7.0295e-01)\tAcc@1  74.22 ( 84.54)\tAcc@5  96.88 ( 97.10)\n",
      "Test: [160/391]\tTime  0.737 ( 0.128)\tLoss 8.5081e-01 (7.1312e-01)\tAcc@1  76.56 ( 84.32)\tAcc@5  96.09 ( 96.94)\n",
      "Test: [180/391]\tTime  0.596 ( 0.125)\tLoss 1.5574e+00 (7.5501e-01)\tAcc@1  61.72 ( 83.22)\tAcc@5  90.62 ( 96.46)\n",
      "Test: [200/391]\tTime  0.433 ( 0.121)\tLoss 6.3523e-01 (7.9933e-01)\tAcc@1  82.03 ( 82.09)\tAcc@5  97.66 ( 95.97)\n",
      "Test: [220/391]\tTime  0.255 ( 0.120)\tLoss 4.6875e-01 (8.1662e-01)\tAcc@1  91.41 ( 81.64)\tAcc@5  98.44 ( 95.82)\n",
      "Test: [240/391]\tTime  0.260 ( 0.119)\tLoss 9.1361e-01 (8.2762e-01)\tAcc@1  83.59 ( 81.48)\tAcc@5  90.62 ( 95.59)\n",
      "Test: [260/391]\tTime  0.176 ( 0.119)\tLoss 9.0054e-01 (8.5784e-01)\tAcc@1  78.12 ( 80.55)\tAcc@5  96.09 ( 95.29)\n",
      "Test: [280/391]\tTime  0.167 ( 0.118)\tLoss 1.0418e+00 (8.6964e-01)\tAcc@1  73.44 ( 80.29)\tAcc@5  93.75 ( 95.16)\n",
      "Test: [300/391]\tTime  0.047 ( 0.116)\tLoss 7.3867e-01 (8.8396e-01)\tAcc@1  83.59 ( 80.00)\tAcc@5  95.31 ( 94.95)\n",
      "Test: [320/391]\tTime  0.056 ( 0.116)\tLoss 6.8002e-01 (8.9660e-01)\tAcc@1  88.28 ( 79.77)\tAcc@5  96.88 ( 94.78)\n",
      "Test: [340/391]\tTime  0.056 ( 0.115)\tLoss 7.3818e-01 (9.0858e-01)\tAcc@1  82.03 ( 79.43)\tAcc@5  96.88 ( 94.69)\n",
      "Test: [360/391]\tTime  0.048 ( 0.115)\tLoss 1.3240e+00 (9.1847e-01)\tAcc@1  69.53 ( 79.17)\tAcc@5  95.31 ( 94.61)\n",
      "Test: [380/391]\tTime  0.047 ( 0.115)\tLoss 8.6641e-01 (9.1588e-01)\tAcc@1  80.47 ( 79.22)\tAcc@5  97.66 ( 94.64)\n",
      " * Acc@1 79.280 Acc@5 94.682\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b463e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.014 ( 2.014)\tLoss 4.5570e-01 (4.5570e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.739 ( 0.212)\tLoss 6.2221e-01 (6.6225e-01)\tAcc@1  87.50 ( 86.09)\tAcc@5  95.31 ( 97.21)\n",
      "Test: [ 40/391]\tTime  0.752 ( 0.171)\tLoss 6.7690e-01 (6.9097e-01)\tAcc@1  86.72 ( 85.02)\tAcc@5  95.31 ( 97.01)\n",
      "Test: [ 60/391]\tTime  0.664 ( 0.151)\tLoss 8.5970e-01 (6.5578e-01)\tAcc@1  82.81 ( 86.33)\tAcc@5  95.31 ( 97.05)\n",
      "Test: [ 80/391]\tTime  0.617 ( 0.141)\tLoss 7.9230e-01 (7.1255e-01)\tAcc@1  78.91 ( 84.56)\tAcc@5  97.66 ( 96.86)\n",
      "Test: [100/391]\tTime  0.628 ( 0.135)\tLoss 5.5982e-01 (7.1554e-01)\tAcc@1  90.62 ( 84.31)\tAcc@5  96.09 ( 96.88)\n",
      "Test: [120/391]\tTime  0.614 ( 0.133)\tLoss 5.7293e-01 (7.1967e-01)\tAcc@1  91.41 ( 84.23)\tAcc@5  96.09 ( 96.94)\n",
      "Test: [140/391]\tTime  0.536 ( 0.130)\tLoss 9.8568e-01 (7.0816e-01)\tAcc@1  73.44 ( 84.42)\tAcc@5  97.66 ( 97.08)\n",
      "Test: [160/391]\tTime  0.799 ( 0.133)\tLoss 8.5174e-01 (7.1731e-01)\tAcc@1  77.34 ( 84.19)\tAcc@5  96.09 ( 96.91)\n",
      "Test: [180/391]\tTime  0.597 ( 0.130)\tLoss 1.5561e+00 (7.5839e-01)\tAcc@1  60.94 ( 83.08)\tAcc@5  90.62 ( 96.42)\n",
      "Test: [200/391]\tTime  0.442 ( 0.126)\tLoss 6.4368e-01 (8.0234e-01)\tAcc@1  82.03 ( 81.95)\tAcc@5  97.66 ( 95.94)\n",
      "Test: [220/391]\tTime  0.607 ( 0.125)\tLoss 4.6212e-01 (8.1923e-01)\tAcc@1  91.41 ( 81.57)\tAcc@5  97.66 ( 95.76)\n",
      "Test: [240/391]\tTime  0.243 ( 0.124)\tLoss 9.2629e-01 (8.2960e-01)\tAcc@1  82.81 ( 81.47)\tAcc@5  91.41 ( 95.56)\n",
      "Test: [260/391]\tTime  0.270 ( 0.123)\tLoss 9.3095e-01 (8.5903e-01)\tAcc@1  78.12 ( 80.56)\tAcc@5  94.53 ( 95.26)\n",
      "Test: [280/391]\tTime  0.048 ( 0.122)\tLoss 1.0101e+00 (8.6948e-01)\tAcc@1  74.22 ( 80.34)\tAcc@5  93.75 ( 95.14)\n",
      "Test: [300/391]\tTime  0.047 ( 0.122)\tLoss 7.2644e-01 (8.8284e-01)\tAcc@1  84.38 ( 80.04)\tAcc@5  95.31 ( 94.94)\n",
      "Test: [320/391]\tTime  0.049 ( 0.121)\tLoss 6.6464e-01 (8.9488e-01)\tAcc@1  87.50 ( 79.80)\tAcc@5  96.88 ( 94.79)\n",
      "Test: [340/391]\tTime  0.048 ( 0.121)\tLoss 7.3123e-01 (9.0669e-01)\tAcc@1  83.59 ( 79.47)\tAcc@5  96.88 ( 94.69)\n",
      "Test: [360/391]\tTime  0.060 ( 0.120)\tLoss 1.2844e+00 (9.1587e-01)\tAcc@1  71.88 ( 79.25)\tAcc@5  96.09 ( 94.64)\n",
      "Test: [380/391]\tTime  0.055 ( 0.120)\tLoss 8.6344e-01 (9.1316e-01)\tAcc@1  79.69 ( 79.29)\tAcc@5  96.88 ( 94.68)\n",
      " * Acc@1 79.374 Acc@5 94.720\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bae1fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.161 ( 2.161)\tLoss 4.3257e-01 (4.3257e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.650 ( 0.223)\tLoss 5.9604e-01 (6.4916e-01)\tAcc@1  86.72 ( 85.71)\tAcc@5  96.09 ( 97.28)\n",
      "Test: [ 40/391]\tTime  0.609 ( 0.176)\tLoss 6.2325e-01 (6.7726e-01)\tAcc@1  86.72 ( 84.78)\tAcc@5  96.88 ( 97.10)\n",
      "Test: [ 60/391]\tTime  0.522 ( 0.156)\tLoss 8.3152e-01 (6.4015e-01)\tAcc@1  82.03 ( 86.28)\tAcc@5  95.31 ( 97.13)\n",
      "Test: [ 80/391]\tTime  0.561 ( 0.145)\tLoss 7.5286e-01 (6.9579e-01)\tAcc@1  80.47 ( 84.69)\tAcc@5  97.66 ( 96.90)\n",
      "Test: [100/391]\tTime  0.532 ( 0.138)\tLoss 5.5953e-01 (6.9885e-01)\tAcc@1  89.06 ( 84.44)\tAcc@5  96.09 ( 96.88)\n",
      "Test: [120/391]\tTime  0.612 ( 0.136)\tLoss 5.5679e-01 (7.0265e-01)\tAcc@1  89.84 ( 84.40)\tAcc@5  97.66 ( 96.95)\n",
      "Test: [140/391]\tTime  0.488 ( 0.133)\tLoss 1.0149e+00 (6.9270e-01)\tAcc@1  74.22 ( 84.59)\tAcc@5  98.44 ( 97.09)\n",
      "Test: [160/391]\tTime  0.700 ( 0.135)\tLoss 7.7989e-01 (7.0204e-01)\tAcc@1  80.47 ( 84.41)\tAcc@5  96.88 ( 96.95)\n",
      "Test: [180/391]\tTime  0.538 ( 0.132)\tLoss 1.5493e+00 (7.4047e-01)\tAcc@1  61.72 ( 83.34)\tAcc@5  89.84 ( 96.49)\n",
      "Test: [200/391]\tTime  0.281 ( 0.129)\tLoss 6.5076e-01 (7.8193e-01)\tAcc@1  79.69 ( 82.26)\tAcc@5  98.44 ( 96.02)\n",
      "Test: [220/391]\tTime  0.404 ( 0.127)\tLoss 4.3297e-01 (7.9691e-01)\tAcc@1  92.19 ( 81.90)\tAcc@5  98.44 ( 95.88)\n",
      "Test: [240/391]\tTime  0.065 ( 0.126)\tLoss 8.6810e-01 (8.0583e-01)\tAcc@1  83.59 ( 81.83)\tAcc@5  90.62 ( 95.68)\n",
      "Test: [260/391]\tTime  0.091 ( 0.125)\tLoss 8.8418e-01 (8.3531e-01)\tAcc@1  78.91 ( 80.95)\tAcc@5  95.31 ( 95.38)\n",
      "Test: [280/391]\tTime  0.154 ( 0.124)\tLoss 1.0027e+00 (8.4498e-01)\tAcc@1  75.78 ( 80.73)\tAcc@5  93.75 ( 95.27)\n",
      "Test: [300/391]\tTime  0.143 ( 0.123)\tLoss 7.4938e-01 (8.5679e-01)\tAcc@1  84.38 ( 80.49)\tAcc@5  96.88 ( 95.08)\n",
      "Test: [320/391]\tTime  0.059 ( 0.122)\tLoss 6.1438e-01 (8.6744e-01)\tAcc@1  89.06 ( 80.26)\tAcc@5  96.88 ( 94.97)\n",
      "Test: [340/391]\tTime  0.101 ( 0.121)\tLoss 7.8142e-01 (8.7862e-01)\tAcc@1  82.03 ( 79.94)\tAcc@5  96.88 ( 94.91)\n",
      "Test: [360/391]\tTime  0.060 ( 0.122)\tLoss 1.2436e+00 (8.8669e-01)\tAcc@1  71.88 ( 79.73)\tAcc@5  96.09 ( 94.88)\n",
      "Test: [380/391]\tTime  0.059 ( 0.121)\tLoss 7.9419e-01 (8.8441e-01)\tAcc@1  80.47 ( 79.76)\tAcc@5  97.66 ( 94.91)\n",
      " * Acc@1 79.826 Acc@5 94.952\n"
     ]
    }
   ],
   "source": [
    "#no pruning\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 1 --retain_rate_list 1 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7086a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.082 ( 2.082)\tLoss 4.4167e-01 (4.4167e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.598 ( 0.214)\tLoss 5.9877e-01 (6.5262e-01)\tAcc@1  86.72 ( 85.83)\tAcc@5  95.31 ( 97.25)\n",
      "Test: [ 40/391]\tTime  0.551 ( 0.168)\tLoss 6.4187e-01 (6.8102e-01)\tAcc@1  89.06 ( 84.98)\tAcc@5  96.09 ( 97.03)\n",
      "Test: [ 60/391]\tTime  0.480 ( 0.147)\tLoss 8.4044e-01 (6.4450e-01)\tAcc@1  82.03 ( 86.44)\tAcc@5  95.31 ( 97.11)\n",
      "Test: [ 80/391]\tTime  0.510 ( 0.137)\tLoss 7.6192e-01 (7.0062e-01)\tAcc@1  80.47 ( 84.84)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [100/391]\tTime  0.489 ( 0.131)\tLoss 5.6046e-01 (7.0370e-01)\tAcc@1  89.84 ( 84.61)\tAcc@5  96.09 ( 96.88)\n",
      "Test: [120/391]\tTime  0.514 ( 0.129)\tLoss 5.5962e-01 (7.0763e-01)\tAcc@1  89.84 ( 84.51)\tAcc@5  97.66 ( 96.96)\n",
      "Test: [140/391]\tTime  0.471 ( 0.126)\tLoss 1.0081e+00 (6.9716e-01)\tAcc@1  74.22 ( 84.69)\tAcc@5  98.44 ( 97.12)\n",
      "Test: [160/391]\tTime  0.666 ( 0.128)\tLoss 7.9913e-01 (7.0632e-01)\tAcc@1  81.25 ( 84.47)\tAcc@5  96.88 ( 96.98)\n",
      "Test: [180/391]\tTime  0.478 ( 0.124)\tLoss 1.5504e+00 (7.4602e-01)\tAcc@1  62.50 ( 83.42)\tAcc@5  88.28 ( 96.50)\n",
      "Test: [200/391]\tTime  0.329 ( 0.121)\tLoss 6.5556e-01 (7.8826e-01)\tAcc@1  79.69 ( 82.31)\tAcc@5  97.66 ( 96.03)\n",
      "Test: [220/391]\tTime  0.065 ( 0.121)\tLoss 4.5983e-01 (8.0441e-01)\tAcc@1  91.41 ( 81.93)\tAcc@5  98.44 ( 95.90)\n",
      "Test: [240/391]\tTime  0.058 ( 0.120)\tLoss 8.8865e-01 (8.1403e-01)\tAcc@1  84.38 ( 81.83)\tAcc@5  91.41 ( 95.69)\n",
      "Test: [260/391]\tTime  0.058 ( 0.119)\tLoss 8.6655e-01 (8.4391e-01)\tAcc@1  78.12 ( 80.92)\tAcc@5  95.31 ( 95.38)\n",
      "Test: [280/391]\tTime  0.058 ( 0.118)\tLoss 1.0125e+00 (8.5411e-01)\tAcc@1  75.78 ( 80.70)\tAcc@5  93.75 ( 95.28)\n",
      "Test: [300/391]\tTime  0.060 ( 0.117)\tLoss 7.1595e-01 (8.6665e-01)\tAcc@1  83.59 ( 80.42)\tAcc@5  96.88 ( 95.10)\n",
      "Test: [320/391]\tTime  0.059 ( 0.116)\tLoss 6.5285e-01 (8.7806e-01)\tAcc@1  87.50 ( 80.18)\tAcc@5  96.88 ( 94.99)\n",
      "Test: [340/391]\tTime  0.058 ( 0.116)\tLoss 7.4731e-01 (8.8931e-01)\tAcc@1  82.81 ( 79.85)\tAcc@5  96.88 ( 94.93)\n",
      "Test: [360/391]\tTime  0.060 ( 0.116)\tLoss 1.2982e+00 (8.9802e-01)\tAcc@1  71.09 ( 79.62)\tAcc@5  96.09 ( 94.88)\n",
      "Test: [380/391]\tTime  0.059 ( 0.115)\tLoss 8.3956e-01 (8.9570e-01)\tAcc@1  80.47 ( 79.67)\tAcc@5  97.66 ( 94.92)\n",
      " * Acc@1 79.732 Acc@5 94.958\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 3 4 5 6 7 8 9 10 --retain_rate_list 0.99 0.98 0.99 0.98 0.95 0.90 0.85 0.9 0.85 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352f0787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 3.344G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  3.286G     |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.297G    |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.301M   |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    0.112G   |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     69.452M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     23.151M |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.301M   |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.185G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     92.602M |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     92.602M |\n",
      "|   blocks.4                 |   1.774M               |   0.297G    |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.301M   |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    0.112G   |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     69.452M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     23.151M |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.301M   |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.185G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     92.602M |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     92.602M |\n",
      "|   blocks.5                 |   1.774M               |   0.297G    |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.301M   |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    0.112G   |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     69.452M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     23.151M |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.301M   |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.185G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     92.602M |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     92.602M |\n",
      "|   blocks.6                 |   1.774M               |   0.234G    |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.24M    |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    85.728M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     55.296M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     18.432M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.24M    |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    0.147G   |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     73.728M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     73.728M |\n",
      "|   blocks.7                 |   1.774M               |   0.234G    |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.24M    |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    85.728M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     55.296M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     18.432M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.24M    |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    0.147G   |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     73.728M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     73.728M |\n",
      "|   blocks.8                 |   1.774M               |   0.234G    |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.24M    |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    85.806M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     55.296M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     18.432M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.24M    |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    0.147G   |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     73.728M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     73.728M |\n",
      "|   blocks.9                 |   1.774M               |   0.185G    |\n",
      "|    blocks.9.norm1          |    0.768K              |    0.192M   |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    66.662M  |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     44.237M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     14.746M |\n",
      "|    blocks.9.norm2          |    0.768K              |    0.192M   |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    0.118G   |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     58.982M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     58.982M |\n",
      "|   blocks.10                |   1.774M               |   0.185G    |\n",
      "|    blocks.10.norm1         |    0.768K              |    0.192M   |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    66.662M  |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     44.237M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     14.746M |\n",
      "|    blocks.10.norm2         |    0.768K              |    0.192M   |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    0.118G   |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     58.982M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     58.982M |\n",
      "|   blocks.11                |   1.774M               |   0.185G    |\n",
      "|    blocks.11.norm1         |    0.768K              |    0.192M   |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    66.662M  |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     44.237M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     14.746M |\n",
      "|    blocks.11.norm2         |    0.768K              |    0.192M   |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    0.118G   |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     58.982M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     58.982M |\n",
      "|  norm                      |  0.768K                |  0.192M     |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 3.34G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 3.29G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.11G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 69.45M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.19G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.11G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 69.45M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.19G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.11G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 69.45M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.19G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.23G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 85.73M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 55.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.15G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.23G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 85.73M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 55.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.15G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.23G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 85.81M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 55.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.15G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.19G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 66.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 44.24M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.75M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.19G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 66.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 44.24M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.75M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.19G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 66.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 44.24M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.75M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.19M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "3344264343\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.8, 0.8, 0.8]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ccfa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 2.873G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  2.815G     |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.259G    |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.265M   |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    96.022M  |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     61.047M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     20.349M |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.265M   |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.163G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     81.396M |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     81.396M |\n",
      "|   blocks.4                 |   1.774M               |   0.259G    |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.265M   |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    96.022M  |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     61.047M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     20.349M |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.265M   |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.163G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     81.396M |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     81.396M |\n",
      "|   blocks.5                 |   1.774M               |   0.259G    |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.265M   |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    96.117M  |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     61.047M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     20.349M |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.265M   |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.163G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     81.396M |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     81.396M |\n",
      "|   blocks.6                 |   1.774M               |   0.177G    |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.184M   |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    63.701M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     42.467M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     14.156M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.184M   |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    0.113G   |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     56.623M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     56.623M |\n",
      "|   blocks.7                 |   1.774M               |   0.177G    |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.184M   |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    63.701M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     42.467M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     14.156M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.184M   |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    0.113G   |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     56.623M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     56.623M |\n",
      "|   blocks.8                 |   1.774M               |   0.177G    |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.184M   |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    63.747M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     42.467M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     14.156M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.184M   |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    0.113G   |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     56.623M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     56.623M |\n",
      "|   blocks.9                 |   1.774M               |   0.122G    |\n",
      "|    blocks.9.norm1          |    0.768K              |    0.129M   |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    42.966M  |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     29.639M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     9.88M   |\n",
      "|    blocks.9.norm2          |    0.768K              |    0.129M   |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    79.036M  |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     39.518M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     39.518M |\n",
      "|   blocks.10                |   1.774M               |   0.122G    |\n",
      "|    blocks.10.norm1         |    0.768K              |    0.129M   |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    42.966M  |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     29.639M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     9.88M   |\n",
      "|    blocks.10.norm2         |    0.768K              |    0.129M   |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    79.036M  |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     39.518M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     39.518M |\n",
      "|   blocks.11                |   1.774M               |   0.122G    |\n",
      "|    blocks.11.norm1         |    0.768K              |    0.129M   |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    42.966M  |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     29.639M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     9.88M   |\n",
      "|    blocks.11.norm2         |    0.768K              |    0.129M   |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    79.036M  |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     39.518M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     39.518M |\n",
      "|  norm                      |  0.768K                |  0.129M     |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 2.87G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 2.81G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.12M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 63.7M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.47M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 63.7M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.47M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 63.75M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.47M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 42.97M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 29.64M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 9.88M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 79.04M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 42.97M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 29.64M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 9.88M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 79.04M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 42.97M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 29.64M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 9.88M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 79.04M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.13M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "2872848497\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.7,0.7,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef006c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 2.478G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  2.42G      |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.22G     |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.227M   |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    80.293M  |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     52.199M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     17.4M   |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.227M   |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.139G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     69.599M |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     69.599M |\n",
      "|   blocks.4                 |   1.774M               |   0.22G     |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.227M   |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    80.293M  |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     52.199M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     17.4M   |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.227M   |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.139G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     69.599M |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     69.599M |\n",
      "|   blocks.5                 |   1.774M               |   0.22G     |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.227M   |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    80.362M  |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     52.199M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     17.4M   |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.227M   |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.139G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     69.599M |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     69.599M |\n",
      "|   blocks.6                 |   1.774M               |   0.13G     |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.136M   |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    45.749M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     31.408M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     10.469M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.136M   |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    83.755M  |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     41.878M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     41.878M |\n",
      "|   blocks.7                 |   1.774M               |   0.13G     |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.136M   |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    45.749M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     31.408M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     10.469M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.136M   |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    83.755M  |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     41.878M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     41.878M |\n",
      "|   blocks.8                 |   1.774M               |   0.13G     |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.136M   |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    45.774M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     31.408M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     10.469M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.136M   |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    83.755M  |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     41.878M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     41.878M |\n",
      "|   blocks.9                 |   1.774M               |   77.672M   |\n",
      "|    blocks.9.norm1          |    0.768K              |    82.56K   |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    26.782M  |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     19.022M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     6.341M  |\n",
      "|    blocks.9.norm2          |    0.768K              |    82.56K   |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    50.725M  |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     25.362M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     25.362M |\n",
      "|   blocks.10                |   1.774M               |   77.672M   |\n",
      "|    blocks.10.norm1         |    0.768K              |    82.56K   |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    26.782M  |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     19.022M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     6.341M  |\n",
      "|    blocks.10.norm2         |    0.768K              |    82.56K   |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    50.725M  |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     25.362M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     25.362M |\n",
      "|   blocks.11                |   1.774M               |   77.672M   |\n",
      "|    blocks.11.norm1         |    0.768K              |    82.56K   |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    26.782M  |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     19.022M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     6.341M  |\n",
      "|    blocks.11.norm2         |    0.768K              |    82.56K   |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    50.725M  |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     25.362M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     25.362M |\n",
      "|  norm                      |  0.768K                |  82.56K     |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 2.48G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 2.42G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.22G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 80.29M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 52.2M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 17.4M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.14G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.22G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 80.29M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 52.2M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 17.4M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.14G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.22G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 80.36M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 52.2M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 17.4M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.14G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.13G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 45.75M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 31.41M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.47M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 83.76M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.13G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 45.75M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 31.41M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.47M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 83.76M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.13G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 45.77M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 31.41M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.47M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 83.76M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 77.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 26.78M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 19.02M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 50.72M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 77.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 26.78M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 19.02M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 50.72M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 77.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 26.78M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 19.02M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 50.72M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 82.56K\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "2478182118\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.6,0.6,0.6]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3f4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops     |\n",
      "|:---------------------------|:-----------------------|:-----------|\n",
      "| model                      | 86.568M                | 17.583G    |\n",
      "|  cls_token                 |  (1, 1, 768)           |            |\n",
      "|  pos_embed                 |  (1, 197, 768)         |            |\n",
      "|  patch_embed.proj          |  0.591M                |  0.116G    |\n",
      "|   patch_embed.proj.weight  |   (768, 3, 16, 16)     |            |\n",
      "|   patch_embed.proj.bias    |   (768,)               |            |\n",
      "|  blocks                    |  85.054M               |  17.466G   |\n",
      "|   blocks.0                 |   7.088M               |   1.455G   |\n",
      "|    blocks.0.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.0.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.0.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.0.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.0.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.0.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.0.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.0.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.0.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.0.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.0.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.0.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.1                 |   7.088M               |   1.455G   |\n",
      "|    blocks.1.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.1.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.1.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.1.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.1.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.1.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.1.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.1.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.1.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.1.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.1.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.1.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.2                 |   7.088M               |   1.456G   |\n",
      "|    blocks.2.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.2.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.2.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.2.attn           |    2.362M              |    0.525G  |\n",
      "|     blocks.2.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.2.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.2.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.2.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.2.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.2.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.2.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.2.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.3                 |   7.088M               |   1.455G   |\n",
      "|    blocks.3.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.3.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.3.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.3.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.3.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.3.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.3.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.3.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.3.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.3.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.3.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.3.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.4                 |   7.088M               |   1.455G   |\n",
      "|    blocks.4.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.4.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.4.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.4.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.4.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.4.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.4.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.4.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.4.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.4.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.4.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.4.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.5                 |   7.088M               |   1.456G   |\n",
      "|    blocks.5.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.5.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.5.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.5.attn           |    2.362M              |    0.525G  |\n",
      "|     blocks.5.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.5.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.5.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.5.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.5.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.5.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.5.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.5.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.6                 |   7.088M               |   1.455G   |\n",
      "|    blocks.6.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.6.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.6.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.6.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.6.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.6.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.6.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.6.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.6.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.6.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.6.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.6.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.7                 |   7.088M               |   1.455G   |\n",
      "|    blocks.7.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.7.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.7.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.7.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.7.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.7.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.7.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.7.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.7.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.7.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.7.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.7.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.8                 |   7.088M               |   1.456G   |\n",
      "|    blocks.8.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.8.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.8.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.8.attn           |    2.362M              |    0.525G  |\n",
      "|     blocks.8.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.8.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.8.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.8.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.8.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.8.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.8.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.8.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.9                 |   7.088M               |   1.455G   |\n",
      "|    blocks.9.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.9.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.9.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.9.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.9.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.9.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.9.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.9.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.9.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.9.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.9.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.9.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.10                |   7.088M               |   1.455G   |\n",
      "|    blocks.10.norm1         |    1.536K              |    0.756M  |\n",
      "|     blocks.10.norm1.weight |     (768,)             |            |\n",
      "|     blocks.10.norm1.bias   |     (768,)             |            |\n",
      "|    blocks.10.attn          |    2.362M              |    0.524G  |\n",
      "|     blocks.10.attn.qkv     |     1.772M             |     0.349G |\n",
      "|     blocks.10.attn.proj    |     0.591M             |     0.116G |\n",
      "|    blocks.10.norm2         |    1.536K              |    0.756M  |\n",
      "|     blocks.10.norm2.weight |     (768,)             |            |\n",
      "|     blocks.10.norm2.bias   |     (768,)             |            |\n",
      "|    blocks.10.mlp           |    4.722M              |    0.93G   |\n",
      "|     blocks.10.mlp.fc1      |     2.362M             |     0.465G |\n",
      "|     blocks.10.mlp.fc2      |     2.36M              |     0.465G |\n",
      "|   blocks.11                |   7.088M               |   1.455G   |\n",
      "|    blocks.11.norm1         |    1.536K              |    0.756M  |\n",
      "|     blocks.11.norm1.weight |     (768,)             |            |\n",
      "|     blocks.11.norm1.bias   |     (768,)             |            |\n",
      "|    blocks.11.attn          |    2.362M              |    0.524G  |\n",
      "|     blocks.11.attn.qkv     |     1.772M             |     0.349G |\n",
      "|     blocks.11.attn.proj    |     0.591M             |     0.116G |\n",
      "|    blocks.11.norm2         |    1.536K              |    0.756M  |\n",
      "|     blocks.11.norm2.weight |     (768,)             |            |\n",
      "|     blocks.11.norm2.bias   |     (768,)             |            |\n",
      "|    blocks.11.mlp           |    4.722M              |    0.93G   |\n",
      "|     blocks.11.mlp.fc1      |     2.362M             |     0.465G |\n",
      "|     blocks.11.mlp.fc2      |     2.36M              |     0.465G |\n",
      "|  norm                      |  1.536K                |  0.756M    |\n",
      "|   norm.weight              |   (768,)               |            |\n",
      "|   norm.bias                |   (768,)               |            |\n",
      "|  head                      |  0.769M                |  0.768M    |\n",
      "|   head.weight              |   (1000, 768)          |            |\n",
      "|   head.bias                |   (1000,)              |            |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 86.57M, #flops: 17.58G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.59M, #flops: 0.12G\n",
      "    (proj): Conv2d(\n",
      "      3, 768, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.59M, #flops: 0.12G\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 85.05M, #flops: 17.47G\n",
      "    (0): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (768,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 1.54K, #flops: 0.76M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=768, out_features=1000, bias=True\n",
      "    #params: 0.77M, #flops: 0.77M\n",
      "  )\n",
      ")\n",
      "17583322359\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -B\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 12\n",
    "mlp_ratio = 4.\n",
    "dims = 768\n",
    "qkv_bias = True\n",
    "retain_rate_list = [1,1,1]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51845a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 86.568M                | 15.02G      |\n",
      "|  cls_token                 |  (1, 1, 768)           |             |\n",
      "|  pos_embed                 |  (1, 197, 768)         |             |\n",
      "|  patch_embed.proj          |  0.591M                |  0.116G     |\n",
      "|   patch_embed.proj.weight  |   (768, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (768,)               |             |\n",
      "|  blocks                    |  85.054M               |  14.903G    |\n",
      "|   blocks.0                 |   7.088M               |   1.455G    |\n",
      "|    blocks.0.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.0.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.0.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.0.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.0.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.0.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.0.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.0.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.1                 |   7.088M               |   1.455G    |\n",
      "|    blocks.1.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.1.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.1.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.1.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.1.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.1.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.1.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.1.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.2                 |   7.088M               |   1.455G    |\n",
      "|    blocks.2.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.2.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.2.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.2.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.2.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.2.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.2.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.2.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.3                 |   7.088M               |   1.302G    |\n",
      "|    blocks.3.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.3.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.3.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.3.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.3.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.3.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.3.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.3.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.4                 |   7.088M               |   1.302G    |\n",
      "|    blocks.4.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.4.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.4.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.4.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.4.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.4.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.4.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.4.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.5                 |   7.088M               |   1.302G    |\n",
      "|    blocks.5.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.5.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.5.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.5.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.5.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.5.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.5.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.5.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.6                 |   7.088M               |   1.165G    |\n",
      "|    blocks.6.norm1          |    1.536K              |    0.611M   |\n",
      "|     blocks.6.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.6.attn           |    2.362M              |    0.414G   |\n",
      "|     blocks.6.attn.qkv      |     1.772M             |     0.281G  |\n",
      "|     blocks.6.attn.proj     |     0.591M             |     93.782M |\n",
      "|    blocks.6.norm2          |    1.536K              |    0.611M   |\n",
      "|     blocks.6.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.6.mlp            |    4.722M              |    0.75G    |\n",
      "|     blocks.6.mlp.fc1       |     2.362M             |     0.375G  |\n",
      "|     blocks.6.mlp.fc2       |     2.36M              |     0.375G  |\n",
      "|   blocks.7                 |   7.088M               |   1.165G    |\n",
      "|    blocks.7.norm1          |    1.536K              |    0.611M   |\n",
      "|     blocks.7.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.7.attn           |    2.362M              |    0.414G   |\n",
      "|     blocks.7.attn.qkv      |     1.772M             |     0.281G  |\n",
      "|     blocks.7.attn.proj     |     0.591M             |     93.782M |\n",
      "|    blocks.7.norm2          |    1.536K              |    0.611M   |\n",
      "|     blocks.7.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.7.mlp            |    4.722M              |    0.75G    |\n",
      "|     blocks.7.mlp.fc1       |     2.362M             |     0.375G  |\n",
      "|     blocks.7.mlp.fc2       |     2.36M              |     0.375G  |\n",
      "|   blocks.8                 |   7.088M               |   1.165G    |\n",
      "|    blocks.8.norm1          |    1.536K              |    0.611M   |\n",
      "|     blocks.8.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.8.attn           |    2.362M              |    0.414G   |\n",
      "|     blocks.8.attn.qkv      |     1.772M             |     0.281G  |\n",
      "|     blocks.8.attn.proj     |     0.591M             |     93.782M |\n",
      "|    blocks.8.norm2          |    1.536K              |    0.611M   |\n",
      "|     blocks.8.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.8.mlp            |    4.722M              |    0.75G    |\n",
      "|     blocks.8.mlp.fc1       |     2.362M             |     0.375G  |\n",
      "|     blocks.8.mlp.fc2       |     2.36M              |     0.375G  |\n",
      "|   blocks.9                 |   7.088M               |   1.045G    |\n",
      "|    blocks.9.norm1          |    1.536K              |    0.549M   |\n",
      "|     blocks.9.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.9.attn           |    2.362M              |    0.369G   |\n",
      "|     blocks.9.attn.qkv      |     1.772M             |     0.253G  |\n",
      "|     blocks.9.attn.proj     |     0.591M             |     84.345M |\n",
      "|    blocks.9.norm2          |    1.536K              |    0.549M   |\n",
      "|     blocks.9.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.9.mlp            |    4.722M              |    0.675G   |\n",
      "|     blocks.9.mlp.fc1       |     2.362M             |     0.337G  |\n",
      "|     blocks.9.mlp.fc2       |     2.36M              |     0.337G  |\n",
      "|   blocks.10                |   7.088M               |   1.045G    |\n",
      "|    blocks.10.norm1         |    1.536K              |    0.549M   |\n",
      "|     blocks.10.norm1.weight |     (768,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.10.attn          |    2.362M              |    0.369G   |\n",
      "|     blocks.10.attn.qkv     |     1.772M             |     0.253G  |\n",
      "|     blocks.10.attn.proj    |     0.591M             |     84.345M |\n",
      "|    blocks.10.norm2         |    1.536K              |    0.549M   |\n",
      "|     blocks.10.norm2.weight |     (768,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.10.mlp           |    4.722M              |    0.675G   |\n",
      "|     blocks.10.mlp.fc1      |     2.362M             |     0.337G  |\n",
      "|     blocks.10.mlp.fc2      |     2.36M              |     0.337G  |\n",
      "|   blocks.11                |   7.088M               |   1.045G    |\n",
      "|    blocks.11.norm1         |    1.536K              |    0.549M   |\n",
      "|     blocks.11.norm1.weight |     (768,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.11.attn          |    2.362M              |    0.369G   |\n",
      "|     blocks.11.attn.qkv     |     1.772M             |     0.253G  |\n",
      "|     blocks.11.attn.proj    |     0.591M             |     84.345M |\n",
      "|    blocks.11.norm2         |    1.536K              |    0.549M   |\n",
      "|     blocks.11.norm2.weight |     (768,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.11.mlp           |    4.722M              |    0.675G   |\n",
      "|     blocks.11.mlp.fc1      |     2.362M             |     0.337G  |\n",
      "|     blocks.11.mlp.fc2      |     2.36M              |     0.337G  |\n",
      "|  norm                      |  1.536K                |  0.549M     |\n",
      "|   norm.weight              |   (768,)               |             |\n",
      "|   norm.bias                |   (768,)               |             |\n",
      "|  head                      |  0.769M                |  0.768M     |\n",
      "|   head.weight              |   (1000, 768)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 86.57M, #flops: 15.02G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.59M, #flops: 0.12G\n",
      "    (proj): Conv2d(\n",
      "      3, 768, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.59M, #flops: 0.12G\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 85.05M, #flops: 14.9G\n",
      "    (0): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 7.09M, #flops: 1.17G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.41G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.28G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 93.78M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.75G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 7.09M, #flops: 1.17G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.41G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.28G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 93.78M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.75G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 7.09M, #flops: 1.17G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.41G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.28G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 93.78M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.75G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 7.09M, #flops: 1.04G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.37G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 84.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 7.09M, #flops: 1.04G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.37G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 84.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 7.09M, #flops: 1.04G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.37G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 84.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (768,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 1.54K, #flops: 0.55M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=768, out_features=1000, bias=True\n",
      "    #params: 0.77M, #flops: 0.77M\n",
      "  )\n",
      ")\n",
      "15020374272\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -B\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 12\n",
    "mlp_ratio = 4.\n",
    "dims = 768\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.9,0.9]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "769672e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 86.568M                | 13.628G     |\n",
      "|  cls_token                 |  (1, 1, 768)           |             |\n",
      "|  pos_embed                 |  (1, 197, 768)         |             |\n",
      "|  patch_embed.proj          |  0.591M                |  0.116G     |\n",
      "|   patch_embed.proj.weight  |   (768, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (768,)               |             |\n",
      "|  blocks                    |  85.054M               |  13.512G    |\n",
      "|   blocks.0                 |   7.088M               |   1.455G    |\n",
      "|    blocks.0.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.0.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.0.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.0.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.0.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.0.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.0.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.0.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.1                 |   7.088M               |   1.455G    |\n",
      "|    blocks.1.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.1.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.1.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.1.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.1.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.1.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.1.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.1.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.2                 |   7.088M               |   1.455G    |\n",
      "|    blocks.2.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.2.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.2.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.2.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.2.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.2.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.2.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.2.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.3                 |   7.088M               |   1.302G    |\n",
      "|    blocks.3.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.3.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.3.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.3.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.3.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.3.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.3.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.3.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.4                 |   7.088M               |   1.302G    |\n",
      "|    blocks.4.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.4.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.4.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.4.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.4.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.4.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.4.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.4.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.5                 |   7.088M               |   1.302G    |\n",
      "|    blocks.5.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.5.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.5.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.5.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.5.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.5.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.5.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.5.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.6                 |   7.088M               |   1.03G     |\n",
      "|    blocks.6.norm1          |    1.536K              |    0.541M   |\n",
      "|     blocks.6.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.6.attn           |    2.362M              |    0.363G   |\n",
      "|     blocks.6.attn.qkv      |     1.772M             |     0.249G  |\n",
      "|     blocks.6.attn.proj     |     0.591M             |     83.165M |\n",
      "|    blocks.6.norm2          |    1.536K              |    0.541M   |\n",
      "|     blocks.6.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.6.mlp            |    4.722M              |    0.665G   |\n",
      "|     blocks.6.mlp.fc1       |     2.362M             |     0.333G  |\n",
      "|     blocks.6.mlp.fc2       |     2.36M              |     0.333G  |\n",
      "|   blocks.7                 |   7.088M               |   1.03G     |\n",
      "|    blocks.7.norm1          |    1.536K              |    0.541M   |\n",
      "|     blocks.7.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.7.attn           |    2.362M              |    0.363G   |\n",
      "|     blocks.7.attn.qkv      |     1.772M             |     0.249G  |\n",
      "|     blocks.7.attn.proj     |     0.591M             |     83.165M |\n",
      "|    blocks.7.norm2          |    1.536K              |    0.541M   |\n",
      "|     blocks.7.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.7.mlp            |    4.722M              |    0.665G   |\n",
      "|     blocks.7.mlp.fc1       |     2.362M             |     0.333G  |\n",
      "|     blocks.7.mlp.fc2       |     2.36M              |     0.333G  |\n",
      "|   blocks.8                 |   7.088M               |   1.03G     |\n",
      "|    blocks.8.norm1          |    1.536K              |    0.541M   |\n",
      "|     blocks.8.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.8.attn           |    2.362M              |    0.363G   |\n",
      "|     blocks.8.attn.qkv      |     1.772M             |     0.249G  |\n",
      "|     blocks.8.attn.proj     |     0.591M             |     83.165M |\n",
      "|    blocks.8.norm2          |    1.536K              |    0.541M   |\n",
      "|     blocks.8.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.8.mlp            |    4.722M              |    0.665G   |\n",
      "|     blocks.8.mlp.fc1       |     2.362M             |     0.333G  |\n",
      "|     blocks.8.mlp.fc2       |     2.36M              |     0.333G  |\n",
      "|   blocks.9                 |   7.088M               |   0.717G    |\n",
      "|    blocks.9.norm1          |    1.536K              |    0.38M    |\n",
      "|     blocks.9.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.9.attn           |    2.362M              |    0.249G   |\n",
      "|     blocks.9.attn.qkv      |     1.772M             |     0.175G  |\n",
      "|     blocks.9.attn.proj     |     0.591M             |     58.393M |\n",
      "|    blocks.9.norm2          |    1.536K              |    0.38M    |\n",
      "|     blocks.9.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.9.mlp            |    4.722M              |    0.467G   |\n",
      "|     blocks.9.mlp.fc1       |     2.362M             |     0.234G  |\n",
      "|     blocks.9.mlp.fc2       |     2.36M              |     0.234G  |\n",
      "|   blocks.10                |   7.088M               |   0.717G    |\n",
      "|    blocks.10.norm1         |    1.536K              |    0.38M    |\n",
      "|     blocks.10.norm1.weight |     (768,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.10.attn          |    2.362M              |    0.249G   |\n",
      "|     blocks.10.attn.qkv     |     1.772M             |     0.175G  |\n",
      "|     blocks.10.attn.proj    |     0.591M             |     58.393M |\n",
      "|    blocks.10.norm2         |    1.536K              |    0.38M    |\n",
      "|     blocks.10.norm2.weight |     (768,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.10.mlp           |    4.722M              |    0.467G   |\n",
      "|     blocks.10.mlp.fc1      |     2.362M             |     0.234G  |\n",
      "|     blocks.10.mlp.fc2      |     2.36M              |     0.234G  |\n",
      "|   blocks.11                |   7.088M               |   0.717G    |\n",
      "|    blocks.11.norm1         |    1.536K              |    0.38M    |\n",
      "|     blocks.11.norm1.weight |     (768,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.11.attn          |    2.362M              |    0.249G   |\n",
      "|     blocks.11.attn.qkv     |     1.772M             |     0.175G  |\n",
      "|     blocks.11.attn.proj    |     0.591M             |     58.393M |\n",
      "|    blocks.11.norm2         |    1.536K              |    0.38M    |\n",
      "|     blocks.11.norm2.weight |     (768,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.11.mlp           |    4.722M              |    0.467G   |\n",
      "|     blocks.11.mlp.fc1      |     2.362M             |     0.234G  |\n",
      "|     blocks.11.mlp.fc2      |     2.36M              |     0.234G  |\n",
      "|  norm                      |  1.536K                |  0.38M      |\n",
      "|   norm.weight              |   (768,)               |             |\n",
      "|   norm.bias                |   (768,)               |             |\n",
      "|  head                      |  0.769M                |  0.768M     |\n",
      "|   head.weight              |   (1000, 768)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 86.57M, #flops: 13.63G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.59M, #flops: 0.12G\n",
      "    (proj): Conv2d(\n",
      "      3, 768, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.59M, #flops: 0.12G\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 85.05M, #flops: 13.51G\n",
      "    (0): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 7.09M, #flops: 1.03G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.36G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 7.09M, #flops: 1.03G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.36G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 7.09M, #flops: 1.03G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.36G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 7.09M, #flops: 0.72G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.25G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.18G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.47G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 7.09M, #flops: 0.72G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.25G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.18G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.47G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 7.09M, #flops: 0.72G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.25G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.18G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.47G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (768,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 1.54K, #flops: 0.38M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=768, out_features=1000, bias=True\n",
      "    #params: 0.77M, #flops: 0.77M\n",
      "  )\n",
      ")\n",
      "13628340480\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -B\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 12\n",
    "mlp_ratio = 4.\n",
    "dims = 768\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.8,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c98bc542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Test: [  0/391]\tTime  2.094 ( 2.094)\tLoss 2.9283e-01 (2.9283e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 20/391]\tTime  0.201 ( 0.216)\tLoss 5.0721e-01 (5.9673e-01)\tAcc@1  89.06 ( 87.28)\tAcc@5  98.44 ( 97.81)\n",
      "Test: [ 40/391]\tTime  0.173 ( 0.172)\tLoss 5.9670e-01 (6.4031e-01)\tAcc@1  91.41 ( 86.20)\tAcc@5  93.75 ( 97.37)\n",
      "Test: [ 60/391]\tTime  0.101 ( 0.152)\tLoss 7.8141e-01 (5.9826e-01)\tAcc@1  83.59 ( 87.69)\tAcc@5  96.09 ( 97.61)\n",
      "Test: [ 80/391]\tTime  0.108 ( 0.141)\tLoss 6.6434e-01 (6.3982e-01)\tAcc@1  84.38 ( 86.44)\tAcc@5  98.44 ( 97.48)\n",
      "Test: [100/391]\tTime  0.100 ( 0.134)\tLoss 6.3170e-01 (6.4460e-01)\tAcc@1  89.84 ( 86.25)\tAcc@5  96.88 ( 97.52)\n",
      "Test: [120/391]\tTime  0.114 ( 0.134)\tLoss 5.2331e-01 (6.5416e-01)\tAcc@1  89.84 ( 85.95)\tAcc@5  99.22 ( 97.56)\n",
      "Test: [140/391]\tTime  0.109 ( 0.132)\tLoss 9.4773e-01 (6.4558e-01)\tAcc@1  75.78 ( 86.00)\tAcc@5  99.22 ( 97.66)\n",
      "Test: [160/391]\tTime  0.252 ( 0.131)\tLoss 8.6179e-01 (6.5758e-01)\tAcc@1  82.03 ( 85.85)\tAcc@5  93.75 ( 97.47)\n",
      "Test: [180/391]\tTime  0.120 ( 0.129)\tLoss 1.5193e+00 (6.9847e-01)\tAcc@1  63.28 ( 84.83)\tAcc@5  91.41 ( 97.05)\n",
      "Test: [200/391]\tTime  0.102 ( 0.127)\tLoss 8.2187e-01 (7.4016e-01)\tAcc@1  78.12 ( 83.78)\tAcc@5  95.31 ( 96.58)\n",
      "Test: [220/391]\tTime  0.102 ( 0.125)\tLoss 4.0536e-01 (7.5636e-01)\tAcc@1  91.41 ( 83.41)\tAcc@5  99.22 ( 96.44)\n",
      "Test: [240/391]\tTime  0.102 ( 0.124)\tLoss 8.4871e-01 (7.6530e-01)\tAcc@1  87.50 ( 83.31)\tAcc@5  92.97 ( 96.27)\n",
      "Test: [260/391]\tTime  0.102 ( 0.123)\tLoss 7.4227e-01 (7.9608e-01)\tAcc@1  83.59 ( 82.46)\tAcc@5  98.44 ( 95.97)\n",
      "Test: [280/391]\tTime  0.101 ( 0.123)\tLoss 9.6135e-01 (8.0625e-01)\tAcc@1  79.69 ( 82.23)\tAcc@5  94.53 ( 95.80)\n",
      "Test: [300/391]\tTime  0.108 ( 0.122)\tLoss 7.2787e-01 (8.2059e-01)\tAcc@1  89.06 ( 81.99)\tAcc@5  96.09 ( 95.61)\n",
      "Test: [320/391]\tTime  0.101 ( 0.123)\tLoss 5.3282e-01 (8.3354e-01)\tAcc@1  89.06 ( 81.74)\tAcc@5  97.66 ( 95.42)\n",
      "Test: [340/391]\tTime  0.110 ( 0.122)\tLoss 6.5285e-01 (8.4634e-01)\tAcc@1  83.59 ( 81.40)\tAcc@5  98.44 ( 95.30)\n",
      "Test: [360/391]\tTime  0.109 ( 0.121)\tLoss 1.2307e+00 (8.5773e-01)\tAcc@1  73.44 ( 81.11)\tAcc@5  94.53 ( 95.24)\n",
      "Test: [380/391]\tTime  0.102 ( 0.122)\tLoss 7.3511e-01 (8.5710e-01)\tAcc@1  82.03 ( 81.12)\tAcc@5  99.22 ( 95.27)\n",
      " * Acc@1 81.156 Acc@5 95.304\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9e5ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 5.717M                 | 1.259G      |\n",
      "|  cls_token                 |  (1, 1, 192)           |             |\n",
      "|  pos_embed                 |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj          |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight  |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (192,)               |             |\n",
      "|  blocks                    |  5.338M                |  1.23G      |\n",
      "|   blocks.0                 |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.0.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.0.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.1                 |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.1.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.1.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.2                 |   0.445M               |   0.103G    |\n",
      "|    blocks.2.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.2.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.2.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.2.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.3                 |   0.445M               |   0.102G    |\n",
      "|    blocks.3.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.3.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.3.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.3.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.3.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.3.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.3.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.3.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.3.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.3.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.4                 |   0.445M               |   0.102G    |\n",
      "|    blocks.4.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.4.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.4.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.4.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.4.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.4.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.4.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.4.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.4.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.4.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.5                 |   0.445M               |   0.103G    |\n",
      "|    blocks.5.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.5.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.5.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.5.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.5.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.5.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.5.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.5.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.5.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.5.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.6                 |   0.445M               |   0.102G    |\n",
      "|    blocks.6.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.6.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.6.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.6.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.6.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.6.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.6.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.6.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.6.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.6.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.7                 |   0.445M               |   0.102G    |\n",
      "|    blocks.7.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.7.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.7.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.7.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.7.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.7.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.7.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.7.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.7.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.7.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.8                 |   0.445M               |   0.103G    |\n",
      "|    blocks.8.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.8.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.8.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.8.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.8.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.8.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.8.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.8.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.8.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.8.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.9                 |   0.445M               |   0.102G    |\n",
      "|    blocks.9.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.9.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.9.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.9.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.9.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.9.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.9.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.9.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.9.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.9.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.10                |   0.445M               |   0.102G    |\n",
      "|    blocks.10.norm1         |    0.384K              |    0.189M   |\n",
      "|     blocks.10.norm1.weight |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.10.attn          |    0.148M              |    43.951M  |\n",
      "|     blocks.10.attn.qkv     |     0.111M             |     21.787M |\n",
      "|     blocks.10.attn.proj    |     37.056K            |     7.262M  |\n",
      "|    blocks.10.norm2         |    0.384K              |    0.189M   |\n",
      "|     blocks.10.norm2.weight |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.10.mlp           |    0.296M              |    58.098M  |\n",
      "|     blocks.10.mlp.fc1      |     0.148M             |     29.049M |\n",
      "|     blocks.10.mlp.fc2      |     0.148M             |     29.049M |\n",
      "|   blocks.11                |   0.445M               |   0.102G    |\n",
      "|    blocks.11.norm1         |    0.384K              |    0.189M   |\n",
      "|     blocks.11.norm1.weight |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.11.attn          |    0.148M              |    43.951M  |\n",
      "|     blocks.11.attn.qkv     |     0.111M             |     21.787M |\n",
      "|     blocks.11.attn.proj    |     37.056K            |     7.262M  |\n",
      "|    blocks.11.norm2         |    0.384K              |    0.189M   |\n",
      "|     blocks.11.norm2.weight |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.11.mlp           |    0.296M              |    58.098M  |\n",
      "|     blocks.11.mlp.fc1      |     0.148M             |     29.049M |\n",
      "|     blocks.11.mlp.fc2      |     0.148M             |     29.049M |\n",
      "|  norm                      |  0.384K                |  0.189M     |\n",
      "|   norm.weight              |   (192,)               |             |\n",
      "|   norm.bias                |   (192,)               |             |\n",
      "|  head                      |  0.193M                |  0.192M     |\n",
      "|   head.weight              |   (1000, 192)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 5.72M, #flops: 1.26G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 1.23G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 0.19M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      ")\n",
      "1258993335\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -T\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 3\n",
    "mlp_ratio = 4.\n",
    "dims = 192\n",
    "qkv_bias = True\n",
    "retain_rate_list = [1,1,1]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "907a34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 5.717M                 | 1.064G      |\n",
      "|  cls_token                 |  (1, 1, 192)           |             |\n",
      "|  pos_embed                 |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj          |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight  |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (192,)               |             |\n",
      "|  blocks                    |  5.338M                |  1.035G     |\n",
      "|   blocks.0                 |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.0.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.0.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.1                 |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.1.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.1.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.2                 |   0.445M               |   0.103G    |\n",
      "|    blocks.2.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.2.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.2.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.2.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.3                 |   0.445M               |   90.669M   |\n",
      "|    blocks.3.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.3.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.3.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.3.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.3.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.3.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.3.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.3.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.4                 |   0.445M               |   90.669M   |\n",
      "|    blocks.4.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.4.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.4.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.4.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.4.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.4.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.4.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.4.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.5                 |   0.445M               |   90.826M   |\n",
      "|    blocks.5.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.5.attn           |    0.148M              |    38.287M  |\n",
      "|     blocks.5.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.5.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.5.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.5.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.5.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.5.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.6                 |   0.445M               |   80.35M    |\n",
      "|    blocks.6.norm1          |    0.384K              |    0.153M   |\n",
      "|     blocks.6.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.6.attn           |    0.148M              |    33.153M  |\n",
      "|     blocks.6.attn.qkv      |     0.111M             |     17.584M |\n",
      "|     blocks.6.attn.proj     |     37.056K            |     5.861M  |\n",
      "|    blocks.6.norm2          |    0.384K              |    0.153M   |\n",
      "|     blocks.6.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.6.mlp            |    0.296M              |    46.891M  |\n",
      "|     blocks.6.mlp.fc1       |     0.148M             |     23.446M |\n",
      "|     blocks.6.mlp.fc2       |     0.148M             |     23.446M |\n",
      "|   blocks.7                 |   0.445M               |   80.35M    |\n",
      "|    blocks.7.norm1          |    0.384K              |    0.153M   |\n",
      "|     blocks.7.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.7.attn           |    0.148M              |    33.153M  |\n",
      "|     blocks.7.attn.qkv      |     0.111M             |     17.584M |\n",
      "|     blocks.7.attn.proj     |     37.056K            |     5.861M  |\n",
      "|    blocks.7.norm2          |    0.384K              |    0.153M   |\n",
      "|     blocks.7.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.7.mlp            |    0.296M              |    46.891M  |\n",
      "|     blocks.7.mlp.fc1       |     0.148M             |     23.446M |\n",
      "|     blocks.7.mlp.fc2       |     0.148M             |     23.446M |\n",
      "|   blocks.8                 |   0.445M               |   80.476M   |\n",
      "|    blocks.8.norm1          |    0.384K              |    0.153M   |\n",
      "|     blocks.8.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.8.attn           |    0.148M              |    33.28M   |\n",
      "|     blocks.8.attn.qkv      |     0.111M             |     17.584M |\n",
      "|     blocks.8.attn.proj     |     37.056K            |     5.861M  |\n",
      "|    blocks.8.norm2          |    0.384K              |    0.153M   |\n",
      "|     blocks.8.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.8.mlp            |    0.296M              |    46.891M  |\n",
      "|     blocks.8.mlp.fc1       |     0.148M             |     23.446M |\n",
      "|     blocks.8.mlp.fc2       |     0.148M             |     23.446M |\n",
      "|   blocks.9                 |   0.445M               |   71.386M   |\n",
      "|    blocks.9.norm1          |    0.384K              |    0.137M   |\n",
      "|     blocks.9.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.9.attn           |    0.148M              |    28.939M  |\n",
      "|     blocks.9.attn.qkv      |     0.111M             |     15.815M |\n",
      "|     blocks.9.attn.proj     |     37.056K            |     5.272M  |\n",
      "|    blocks.9.norm2          |    0.384K              |    0.137M   |\n",
      "|     blocks.9.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.9.mlp            |    0.296M              |    42.172M  |\n",
      "|     blocks.9.mlp.fc1       |     0.148M             |     21.086M |\n",
      "|     blocks.9.mlp.fc2       |     0.148M             |     21.086M |\n",
      "|   blocks.10                |   0.445M               |   71.386M   |\n",
      "|    blocks.10.norm1         |    0.384K              |    0.137M   |\n",
      "|     blocks.10.norm1.weight |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.10.attn          |    0.148M              |    28.939M  |\n",
      "|     blocks.10.attn.qkv     |     0.111M             |     15.815M |\n",
      "|     blocks.10.attn.proj    |     37.056K            |     5.272M  |\n",
      "|    blocks.10.norm2         |    0.384K              |    0.137M   |\n",
      "|     blocks.10.norm2.weight |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.10.mlp           |    0.296M              |    42.172M  |\n",
      "|     blocks.10.mlp.fc1      |     0.148M             |     21.086M |\n",
      "|     blocks.10.mlp.fc2      |     0.148M             |     21.086M |\n",
      "|   blocks.11                |   0.445M               |   71.386M   |\n",
      "|    blocks.11.norm1         |    0.384K              |    0.137M   |\n",
      "|     blocks.11.norm1.weight |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.11.attn          |    0.148M              |    28.939M  |\n",
      "|     blocks.11.attn.qkv     |     0.111M             |     15.815M |\n",
      "|     blocks.11.attn.proj    |     37.056K            |     5.272M  |\n",
      "|    blocks.11.norm2         |    0.384K              |    0.137M   |\n",
      "|     blocks.11.norm2.weight |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.11.mlp           |    0.296M              |    42.172M  |\n",
      "|     blocks.11.mlp.fc1      |     0.148M             |     21.086M |\n",
      "|     blocks.11.mlp.fc2      |     0.148M             |     21.086M |\n",
      "|  norm                      |  0.384K                |  0.137M     |\n",
      "|   norm.weight              |   (192,)               |             |\n",
      "|   norm.bias                |   (192,)               |             |\n",
      "|  head                      |  0.193M                |  0.192M     |\n",
      "|   head.weight              |   (1000, 192)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 5.72M, #flops: 1.06G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 1.03G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 90.83M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.29M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 80.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 33.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.58M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.86M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.89M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 80.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 33.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.58M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.86M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.89M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 80.48M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 33.28M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.58M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.86M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.89M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 71.39M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.94M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.81M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.27M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 42.17M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 71.39M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.94M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.81M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.27M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 42.17M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 71.39M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.94M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.81M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.27M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 42.17M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 0.14M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      ")\n",
      "1064203751\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -T\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 3\n",
    "mlp_ratio = 4.\n",
    "dims = 192\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.9,0.9]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2314d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 5.717M                 | 0.988G      |\n",
      "|  cls_token                 |  (1, 1, 192)           |             |\n",
      "|  pos_embed                 |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj          |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight  |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (192,)               |             |\n",
      "|  blocks                    |  5.338M                |  0.958G     |\n",
      "|   blocks.0                 |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.0.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.0.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.1                 |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.1.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.1.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.2                 |   0.445M               |   0.103G    |\n",
      "|    blocks.2.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.2.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.2.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.2.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.3                 |   0.445M               |   90.669M   |\n",
      "|    blocks.3.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.3.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.3.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.3.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.3.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.3.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.3.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.3.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.4                 |   0.445M               |   90.669M   |\n",
      "|    blocks.4.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.4.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.4.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.4.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.4.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.4.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.4.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.4.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.5                 |   0.445M               |   90.826M   |\n",
      "|    blocks.5.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.5.attn           |    0.148M              |    38.287M  |\n",
      "|     blocks.5.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.5.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.5.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.5.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.5.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.5.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.6                 |   0.445M               |   70.279M   |\n",
      "|    blocks.6.norm1          |    0.384K              |    0.135M   |\n",
      "|     blocks.6.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.6.attn           |    0.148M              |    28.426M  |\n",
      "|     blocks.6.attn.qkv      |     0.111M             |     15.593M |\n",
      "|     blocks.6.attn.proj     |     37.056K            |     5.198M  |\n",
      "|    blocks.6.norm2          |    0.384K              |    0.135M   |\n",
      "|     blocks.6.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.6.mlp            |    0.296M              |    41.583M  |\n",
      "|     blocks.6.mlp.fc1       |     0.148M             |     20.791M |\n",
      "|     blocks.6.mlp.fc2       |     0.148M             |     20.791M |\n",
      "|   blocks.7                 |   0.445M               |   70.279M   |\n",
      "|    blocks.7.norm1          |    0.384K              |    0.135M   |\n",
      "|     blocks.7.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.7.attn           |    0.148M              |    28.426M  |\n",
      "|     blocks.7.attn.qkv      |     0.111M             |     15.593M |\n",
      "|     blocks.7.attn.proj     |     37.056K            |     5.198M  |\n",
      "|    blocks.7.norm2          |    0.384K              |    0.135M   |\n",
      "|     blocks.7.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.7.mlp            |    0.296M              |    41.583M  |\n",
      "|     blocks.7.mlp.fc1       |     0.148M             |     20.791M |\n",
      "|     blocks.7.mlp.fc2       |     0.148M             |     20.791M |\n",
      "|   blocks.8                 |   0.445M               |   70.378M   |\n",
      "|    blocks.8.norm1          |    0.384K              |    0.135M   |\n",
      "|     blocks.8.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.8.attn           |    0.148M              |    28.525M  |\n",
      "|     blocks.8.attn.qkv      |     0.111M             |     15.593M |\n",
      "|     blocks.8.attn.proj     |     37.056K            |     5.198M  |\n",
      "|    blocks.8.norm2          |    0.384K              |    0.135M   |\n",
      "|     blocks.8.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.8.mlp            |    0.296M              |    41.583M  |\n",
      "|     blocks.8.mlp.fc1       |     0.148M             |     20.791M |\n",
      "|     blocks.8.mlp.fc2       |     0.148M             |     20.791M |\n",
      "|   blocks.9                 |   0.445M               |   62.699M   |\n",
      "|    blocks.9.norm1          |    0.384K              |    0.122M   |\n",
      "|     blocks.9.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.9.attn           |    0.148M              |    25.001M  |\n",
      "|     blocks.9.attn.qkv      |     0.111M             |     14.045M |\n",
      "|     blocks.9.attn.proj     |     37.056K            |     4.682M  |\n",
      "|    blocks.9.norm2          |    0.384K              |    0.122M   |\n",
      "|     blocks.9.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.9.mlp            |    0.296M              |    37.454M  |\n",
      "|     blocks.9.mlp.fc1       |     0.148M             |     18.727M |\n",
      "|     blocks.9.mlp.fc2       |     0.148M             |     18.727M |\n",
      "|   blocks.10                |   0.445M               |   55.704M   |\n",
      "|    blocks.10.norm1         |    0.384K              |    0.109M   |\n",
      "|     blocks.10.norm1.weight |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.10.attn          |    0.148M              |    21.865M  |\n",
      "|     blocks.10.attn.qkv     |     0.111M             |     12.607M |\n",
      "|     blocks.10.attn.proj    |     37.056K            |     4.202M  |\n",
      "|    blocks.10.norm2         |    0.384K              |    0.109M   |\n",
      "|     blocks.10.norm2.weight |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.10.mlp           |    0.296M              |    33.62M   |\n",
      "|     blocks.10.mlp.fc1      |     0.148M             |     16.81M  |\n",
      "|     blocks.10.mlp.fc2      |     0.148M             |     16.81M  |\n",
      "|   blocks.11                |   0.445M               |   49.365M   |\n",
      "|    blocks.11.norm1         |    0.384K              |    97.92K   |\n",
      "|     blocks.11.norm1.weight |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.11.attn          |    0.148M              |    19.088M  |\n",
      "|     blocks.11.attn.qkv     |     0.111M             |     11.28M  |\n",
      "|     blocks.11.attn.proj    |     37.056K            |     3.76M   |\n",
      "|    blocks.11.norm2         |    0.384K              |    97.92K   |\n",
      "|     blocks.11.norm2.weight |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.11.mlp           |    0.296M              |    30.081M  |\n",
      "|     blocks.11.mlp.fc1      |     0.148M             |     15.041M |\n",
      "|     blocks.11.mlp.fc2      |     0.148M             |     15.041M |\n",
      "|  norm                      |  0.384K                |  87.36K     |\n",
      "|   norm.weight              |   (192,)               |             |\n",
      "|   norm.bias                |   (192,)               |             |\n",
      "|  head                      |  0.193M                |  0.192M     |\n",
      "|   head.weight              |   (1000, 192)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 5.72M, #flops: 0.99G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 0.96G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 90.83M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.29M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 70.28M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.59M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 41.58M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 70.28M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.59M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 41.58M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 70.38M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.53M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.59M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 41.58M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 62.7M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 25M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 14.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 4.68M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 37.45M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 18.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 18.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 55.7M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.11M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 21.87M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 12.61M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 4.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.11M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 33.62M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 16.81M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 16.81M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 49.36M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 97.92K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 19.09M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 11.28M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 3.76M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 97.92K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 30.08M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 15.04M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 15.04M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 87.36K\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      ")\n",
      "987525244\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -T\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8,9,10,11]\n",
    "heads = 3\n",
    "mlp_ratio = 4.\n",
    "dims = 192\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.8,0.9,0.9,0.9,0.9]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b02a41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  1.849 ( 1.849)\tLoss 6.0974e-01 (6.0974e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  97.66 ( 97.66)\n",
      "Test: [ 20/391]\tTime  0.839 ( 0.202)\tLoss 1.0564e+00 (9.0846e-01)\tAcc@1  80.47 ( 80.28)\tAcc@5  92.19 ( 94.87)\n",
      "Test: [ 40/391]\tTime  0.822 ( 0.161)\tLoss 1.0035e+00 (9.3634e-01)\tAcc@1  80.47 ( 79.63)\tAcc@5  91.41 ( 94.38)\n",
      "Test: [ 60/391]\tTime  0.724 ( 0.142)\tLoss 1.0974e+00 (9.1319e-01)\tAcc@1  78.12 ( 80.62)\tAcc@5  92.97 ( 94.43)\n",
      "Test: [ 80/391]\tTime  0.775 ( 0.133)\tLoss 1.1898e+00 (9.8976e-01)\tAcc@1  71.09 ( 77.87)\tAcc@5  90.62 ( 94.21)\n",
      "Test: [100/391]\tTime  0.718 ( 0.127)\tLoss 9.3719e-01 (9.9358e-01)\tAcc@1  80.47 ( 77.58)\tAcc@5  96.09 ( 94.29)\n",
      "Test: [120/391]\tTime  0.479 ( 0.126)\tLoss 8.2607e-01 (9.9684e-01)\tAcc@1  82.03 ( 77.29)\tAcc@5  95.31 ( 94.38)\n",
      "Test: [140/391]\tTime  0.341 ( 0.124)\tLoss 1.2332e+00 (9.8485e-01)\tAcc@1  72.66 ( 77.52)\tAcc@5  95.31 ( 94.54)\n",
      "Test: [160/391]\tTime  0.783 ( 0.126)\tLoss 1.1631e+00 (9.9423e-01)\tAcc@1  71.88 ( 77.33)\tAcc@5  92.97 ( 94.33)\n",
      "Test: [180/391]\tTime  0.541 ( 0.123)\tLoss 1.7999e+00 (1.0534e+00)\tAcc@1  53.91 ( 75.98)\tAcc@5  85.16 ( 93.54)\n",
      "Test: [200/391]\tTime  0.305 ( 0.120)\tLoss 1.2524e+00 (1.1147e+00)\tAcc@1  67.97 ( 74.62)\tAcc@5  92.97 ( 92.72)\n",
      "Test: [220/391]\tTime  0.330 ( 0.119)\tLoss 8.6000e-01 (1.1418e+00)\tAcc@1  83.59 ( 74.06)\tAcc@5  95.31 ( 92.25)\n",
      "Test: [240/391]\tTime  0.147 ( 0.118)\tLoss 1.3714e+00 (1.1609e+00)\tAcc@1  75.00 ( 73.76)\tAcc@5  85.16 ( 91.95)\n",
      "Test: [260/391]\tTime  0.231 ( 0.118)\tLoss 1.5420e+00 (1.2016e+00)\tAcc@1  64.06 ( 72.68)\tAcc@5  88.28 ( 91.46)\n",
      "Test: [280/391]\tTime  0.029 ( 0.117)\tLoss 1.4846e+00 (1.2222e+00)\tAcc@1  60.16 ( 72.24)\tAcc@5  90.62 ( 91.19)\n",
      "Test: [300/391]\tTime  0.037 ( 0.117)\tLoss 1.2402e+00 (1.2445e+00)\tAcc@1  76.56 ( 71.85)\tAcc@5  88.28 ( 90.85)\n",
      "Test: [320/391]\tTime  0.030 ( 0.116)\tLoss 8.1152e-01 (1.2622e+00)\tAcc@1  87.50 ( 71.55)\tAcc@5  95.31 ( 90.55)\n",
      "Test: [340/391]\tTime  0.029 ( 0.116)\tLoss 1.0735e+00 (1.2855e+00)\tAcc@1  73.44 ( 70.97)\tAcc@5  96.09 ( 90.24)\n",
      "Test: [360/391]\tTime  0.029 ( 0.116)\tLoss 1.6762e+00 (1.3007e+00)\tAcc@1  58.59 ( 70.58)\tAcc@5  90.62 ( 90.06)\n",
      "Test: [380/391]\tTime  0.028 ( 0.115)\tLoss 1.2447e+00 (1.2917e+00)\tAcc@1  70.31 ( 70.78)\tAcc@5  92.19 ( 90.20)\n",
      " * Acc@1 70.950 Acc@5 90.280\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 9 10 11 --retain_rate_list 0.9 0.8 0.9 0.9 0.9 0.9 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
