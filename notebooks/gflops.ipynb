{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5a61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from tokenrank_vit import TokenRankVisionTransformer\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table, flop_count_str\n",
    "from beit import BeitTeacher\n",
    "from tokenrank_vit import TokenRankVisionTransformer\n",
    "from tokenrank_beit import TokenRankBeit\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor\n",
    "import requests\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c231cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "feature_extractor = ViTFeatureExtractor()\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea190c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 3.555G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  3.496G     |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.338G    |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.34M    |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    0.128G   |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     78.299M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     26.1M   |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.34M    |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.209G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     0.104G  |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     0.104G  |\n",
      "|   blocks.4                 |   1.774M               |   0.338G    |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.34M    |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    0.128G   |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     78.299M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     26.1M   |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.34M    |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.209G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     0.104G  |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     0.104G  |\n",
      "|   blocks.5                 |   1.774M               |   0.338G    |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.34M    |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    0.128G   |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     78.299M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     26.1M   |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.34M    |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.209G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     0.104G  |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     0.104G  |\n",
      "|   blocks.6                 |   1.774M               |   0.265G    |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.271M   |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    98.434M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     62.374M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     20.791M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.271M   |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    0.166G   |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     83.165M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     83.165M |\n",
      "|   blocks.7                 |   1.774M               |   0.265G    |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.271M   |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    98.434M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     62.374M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     20.791M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.271M   |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    0.166G   |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     83.165M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     83.165M |\n",
      "|   blocks.8                 |   1.774M               |   0.265G    |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.271M   |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    98.434M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     62.374M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     20.791M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.271M   |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    0.166G   |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     83.165M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     83.165M |\n",
      "|   blocks.9                 |   1.774M               |   0.183G    |\n",
      "|    blocks.9.norm1          |    0.768K              |    0.19M    |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    65.92M   |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     43.794M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     14.598M |\n",
      "|    blocks.9.norm2          |    0.768K              |    0.19M    |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    0.117G   |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     58.393M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     58.393M |\n",
      "|   blocks.10                |   1.774M               |   0.183G    |\n",
      "|    blocks.10.norm1         |    0.768K              |    0.19M    |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    65.92M   |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     43.794M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     14.598M |\n",
      "|    blocks.10.norm2         |    0.768K              |    0.19M    |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    0.117G   |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     58.393M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     58.393M |\n",
      "|   blocks.11                |   1.774M               |   0.183G    |\n",
      "|    blocks.11.norm1         |    0.768K              |    0.19M    |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    65.92M   |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     43.794M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     14.598M |\n",
      "|    blocks.11.norm2         |    0.768K              |    0.19M    |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    0.117G   |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     58.393M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     58.393M |\n",
      "|  norm                      |  0.768K                |  0.19M      |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 3.55G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 3.5G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.34G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.13G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 78.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.21G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.34G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.13G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 78.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.21G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.34G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.13G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 78.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.34M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.21G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.27G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 98.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 62.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.17G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.27G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 98.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 62.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.17G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.27G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 98.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 62.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.27M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.17G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 65.92M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 43.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 65.92M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 43.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 65.92M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 43.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.19M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "3554802816\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "#prune_list = [2,3,4,5,6,7,8,9,10]\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "#retain_rate_list = [0.99,0.98,0.99,0.98,0.95,0.90,0.85,0.9,0.85]\n",
    "retain_rate_list=[0.9,0.8,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd3ab417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.980 ( 1.980)\tLoss 1.8879e+00 (1.8879e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  81.25 ( 81.25)\n",
      "Test: [ 20/391]\tTime  0.958 ( 0.217)\tLoss 1.7934e+00 (1.7024e+00)\tAcc@1  67.19 ( 66.22)\tAcc@5  80.47 ( 84.82)\n",
      "Test: [ 40/391]\tTime  0.934 ( 0.172)\tLoss 2.4957e+00 (1.6417e+00)\tAcc@1  50.78 ( 67.44)\tAcc@5  71.09 ( 85.63)\n",
      "Test: [ 60/391]\tTime  0.803 ( 0.151)\tLoss 1.7242e+00 (1.6112e+00)\tAcc@1  63.28 ( 68.35)\tAcc@5  82.81 ( 85.89)\n",
      "Test: [ 80/391]\tTime  0.771 ( 0.142)\tLoss 2.0455e+00 (1.7491e+00)\tAcc@1  53.91 ( 64.37)\tAcc@5  83.59 ( 84.38)\n",
      "Test: [100/391]\tTime  0.834 ( 0.136)\tLoss 1.7534e+00 (1.7456e+00)\tAcc@1  57.81 ( 63.71)\tAcc@5  85.16 ( 84.86)\n",
      "Test: [120/391]\tTime  0.726 ( 0.134)\tLoss 1.8857e+00 (1.7480e+00)\tAcc@1  57.81 ( 63.62)\tAcc@5  82.03 ( 84.93)\n",
      "Test: [140/391]\tTime  0.709 ( 0.132)\tLoss 2.0885e+00 (1.7565e+00)\tAcc@1  52.34 ( 63.51)\tAcc@5  84.38 ( 84.83)\n",
      "Test: [160/391]\tTime  0.943 ( 0.134)\tLoss 2.4019e+00 (1.7834e+00)\tAcc@1  49.22 ( 63.03)\tAcc@5  75.00 ( 84.39)\n",
      "Test: [180/391]\tTime  0.711 ( 0.131)\tLoss 3.2839e+00 (1.8837e+00)\tAcc@1  28.12 ( 61.14)\tAcc@5  66.41 ( 82.97)\n",
      "Test: [200/391]\tTime  0.399 ( 0.127)\tLoss 2.3848e+00 (1.9686e+00)\tAcc@1  48.44 ( 59.55)\tAcc@5  73.44 ( 81.62)\n",
      "Test: [220/391]\tTime  0.455 ( 0.126)\tLoss 1.8475e+00 (2.0221e+00)\tAcc@1  64.84 ( 58.69)\tAcc@5  82.03 ( 80.77)\n",
      "Test: [240/391]\tTime  0.434 ( 0.125)\tLoss 2.2580e+00 (2.0672e+00)\tAcc@1  60.16 ( 58.04)\tAcc@5  74.22 ( 80.03)\n",
      "Test: [260/391]\tTime  0.449 ( 0.124)\tLoss 2.9018e+00 (2.1274e+00)\tAcc@1  44.53 ( 56.88)\tAcc@5  69.53 ( 79.12)\n",
      "Test: [280/391]\tTime  0.151 ( 0.124)\tLoss 2.6128e+00 (2.1635e+00)\tAcc@1  41.41 ( 56.20)\tAcc@5  71.88 ( 78.54)\n",
      "Test: [300/391]\tTime  0.025 ( 0.122)\tLoss 2.2319e+00 (2.2037e+00)\tAcc@1  54.69 ( 55.55)\tAcc@5  78.12 ( 77.88)\n",
      "Test: [320/391]\tTime  0.025 ( 0.122)\tLoss 2.0694e+00 (2.2393e+00)\tAcc@1  53.91 ( 54.88)\tAcc@5  78.12 ( 77.30)\n",
      "Test: [340/391]\tTime  0.032 ( 0.121)\tLoss 2.5365e+00 (2.2747e+00)\tAcc@1  44.53 ( 54.18)\tAcc@5  75.00 ( 76.75)\n",
      "Test: [360/391]\tTime  0.026 ( 0.121)\tLoss 2.9823e+00 (2.2999e+00)\tAcc@1  37.50 ( 53.70)\tAcc@5  70.31 ( 76.35)\n",
      "Test: [380/391]\tTime  0.036 ( 0.120)\tLoss 1.9855e+00 (2.2904e+00)\tAcc@1  61.72 ( 53.88)\tAcc@5  84.38 ( 76.52)\n",
      " * Acc@1 54.116 Acc@5 76.658\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c46c68a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.929 ( 1.929)\tLoss 1.2412e+00 (1.2412e+00)\tAcc@1  78.91 ( 78.91)\tAcc@5  92.19 ( 92.19)\n",
      "Test: [ 20/391]\tTime  0.891 ( 0.204)\tLoss 1.9465e+00 (1.4719e+00)\tAcc@1  64.06 ( 70.98)\tAcc@5  77.34 ( 87.83)\n",
      "Test: [ 40/391]\tTime  0.849 ( 0.163)\tLoss 2.0460e+00 (1.4491e+00)\tAcc@1  61.72 ( 71.59)\tAcc@5  78.12 ( 88.38)\n",
      "Test: [ 60/391]\tTime  0.766 ( 0.144)\tLoss 1.7800e+00 (1.4472e+00)\tAcc@1  64.06 ( 71.64)\tAcc@5  81.25 ( 88.10)\n",
      "Test: [ 80/391]\tTime  0.769 ( 0.134)\tLoss 2.1462e+00 (1.5937e+00)\tAcc@1  50.78 ( 67.32)\tAcc@5  76.56 ( 86.49)\n",
      "Test: [100/391]\tTime  0.777 ( 0.128)\tLoss 1.6680e+00 (1.6273e+00)\tAcc@1  60.16 ( 65.86)\tAcc@5  85.16 ( 86.46)\n",
      "Test: [120/391]\tTime  0.698 ( 0.127)\tLoss 1.3875e+00 (1.6174e+00)\tAcc@1  71.88 ( 65.93)\tAcc@5  91.41 ( 86.94)\n",
      "Test: [140/391]\tTime  0.677 ( 0.125)\tLoss 1.6497e+00 (1.6099e+00)\tAcc@1  60.16 ( 66.10)\tAcc@5  90.62 ( 87.13)\n",
      "Test: [160/391]\tTime  0.977 ( 0.127)\tLoss 2.1652e+00 (1.6340e+00)\tAcc@1  55.47 ( 65.84)\tAcc@5  82.81 ( 86.66)\n",
      "Test: [180/391]\tTime  0.807 ( 0.124)\tLoss 3.0002e+00 (1.7260e+00)\tAcc@1  37.50 ( 64.05)\tAcc@5  67.19 ( 85.29)\n",
      "Test: [200/391]\tTime  0.624 ( 0.121)\tLoss 1.9609e+00 (1.8025e+00)\tAcc@1  57.81 ( 62.53)\tAcc@5  80.47 ( 84.12)\n",
      "Test: [220/391]\tTime  0.645 ( 0.120)\tLoss 1.7168e+00 (1.8518e+00)\tAcc@1  67.97 ( 61.74)\tAcc@5  83.59 ( 83.39)\n",
      "Test: [240/391]\tTime  0.328 ( 0.119)\tLoss 2.0393e+00 (1.8887e+00)\tAcc@1  64.06 ( 61.25)\tAcc@5  78.12 ( 82.80)\n",
      "Test: [260/391]\tTime  0.363 ( 0.119)\tLoss 2.8195e+00 (1.9437e+00)\tAcc@1  42.19 ( 60.06)\tAcc@5  67.97 ( 82.03)\n",
      "Test: [280/391]\tTime  0.190 ( 0.118)\tLoss 2.3419e+00 (1.9687e+00)\tAcc@1  48.44 ( 59.65)\tAcc@5  82.03 ( 81.63)\n",
      "Test: [300/391]\tTime  0.034 ( 0.117)\tLoss 1.8863e+00 (2.0028e+00)\tAcc@1  62.50 ( 59.12)\tAcc@5  82.03 ( 81.13)\n",
      "Test: [320/391]\tTime  0.027 ( 0.116)\tLoss 1.7193e+00 (2.0287e+00)\tAcc@1  67.19 ( 58.72)\tAcc@5  86.72 ( 80.69)\n",
      "Test: [340/391]\tTime  0.027 ( 0.116)\tLoss 2.1388e+00 (2.0566e+00)\tAcc@1  54.69 ( 58.12)\tAcc@5  78.12 ( 80.26)\n",
      "Test: [360/391]\tTime  0.026 ( 0.116)\tLoss 2.5963e+00 (2.0786e+00)\tAcc@1  46.09 ( 57.74)\tAcc@5  77.34 ( 79.97)\n",
      "Test: [380/391]\tTime  0.035 ( 0.115)\tLoss 1.8291e+00 (2.0775e+00)\tAcc@1  57.81 ( 57.75)\tAcc@5  86.72 ( 80.02)\n",
      " * Acc@1 57.946 Acc@5 80.152\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b3c55ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.980 ( 1.980)\tLoss 1.5944e+00 (1.5944e+00)\tAcc@1  71.88 ( 71.88)\tAcc@5  85.94 ( 85.94)\n",
      "Test: [ 20/391]\tTime  0.906 ( 0.213)\tLoss 1.6819e+00 (1.5359e+00)\tAcc@1  68.75 ( 68.68)\tAcc@5  82.03 ( 86.90)\n",
      "Test: [ 40/391]\tTime  0.951 ( 0.171)\tLoss 1.9657e+00 (1.4803e+00)\tAcc@1  64.06 ( 69.87)\tAcc@5  79.69 ( 87.92)\n",
      "Test: [ 60/391]\tTime  0.816 ( 0.151)\tLoss 1.6340e+00 (1.4574e+00)\tAcc@1  63.28 ( 70.77)\tAcc@5  84.38 ( 87.95)\n",
      "Test: [ 80/391]\tTime  0.825 ( 0.141)\tLoss 1.9996e+00 (1.5920e+00)\tAcc@1  53.91 ( 66.80)\tAcc@5  82.03 ( 86.68)\n",
      "Test: [100/391]\tTime  0.622 ( 0.134)\tLoss 1.4859e+00 (1.6000e+00)\tAcc@1  66.41 ( 66.13)\tAcc@5  90.62 ( 86.93)\n",
      "Test: [120/391]\tTime  0.464 ( 0.132)\tLoss 1.7419e+00 (1.5923e+00)\tAcc@1  63.28 ( 66.11)\tAcc@5  85.16 ( 87.16)\n",
      "Test: [140/391]\tTime  0.327 ( 0.129)\tLoss 1.9677e+00 (1.5887e+00)\tAcc@1  53.12 ( 66.32)\tAcc@5  86.72 ( 87.16)\n",
      "Test: [160/391]\tTime  0.870 ( 0.130)\tLoss 2.1959e+00 (1.6063e+00)\tAcc@1  53.12 ( 66.09)\tAcc@5  81.25 ( 86.79)\n",
      "Test: [180/391]\tTime  0.587 ( 0.127)\tLoss 2.7050e+00 (1.6833e+00)\tAcc@1  37.50 ( 64.69)\tAcc@5  69.53 ( 85.62)\n",
      "Test: [200/391]\tTime  0.053 ( 0.124)\tLoss 2.3062e+00 (1.7525e+00)\tAcc@1  46.88 ( 63.34)\tAcc@5  76.56 ( 84.63)\n",
      "Test: [220/391]\tTime  0.025 ( 0.123)\tLoss 1.4229e+00 (1.7909e+00)\tAcc@1  75.00 ( 62.74)\tAcc@5  89.84 ( 84.07)\n",
      "Test: [240/391]\tTime  0.237 ( 0.122)\tLoss 2.2215e+00 (1.8266e+00)\tAcc@1  63.28 ( 62.29)\tAcc@5  74.22 ( 83.53)\n",
      "Test: [260/391]\tTime  0.234 ( 0.122)\tLoss 2.4781e+00 (1.8740e+00)\tAcc@1  52.34 ( 61.25)\tAcc@5  73.44 ( 82.95)\n",
      "Test: [280/391]\tTime  0.029 ( 0.121)\tLoss 2.2940e+00 (1.8995e+00)\tAcc@1  47.66 ( 60.79)\tAcc@5  79.69 ( 82.54)\n",
      "Test: [300/391]\tTime  0.033 ( 0.121)\tLoss 1.8954e+00 (1.9284e+00)\tAcc@1  66.41 ( 60.31)\tAcc@5  80.47 ( 82.10)\n",
      "Test: [320/391]\tTime  0.025 ( 0.122)\tLoss 1.5292e+00 (1.9541e+00)\tAcc@1  68.75 ( 59.85)\tAcc@5  92.19 ( 81.68)\n",
      "Test: [340/391]\tTime  0.025 ( 0.121)\tLoss 1.8509e+00 (1.9849e+00)\tAcc@1  57.81 ( 59.23)\tAcc@5  85.94 ( 81.19)\n",
      "Test: [360/391]\tTime  0.026 ( 0.121)\tLoss 2.0713e+00 (2.0016e+00)\tAcc@1  56.25 ( 58.87)\tAcc@5  82.03 ( 80.94)\n",
      "Test: [380/391]\tTime  0.027 ( 0.121)\tLoss 1.6238e+00 (1.9885e+00)\tAcc@1  64.84 ( 59.12)\tAcc@5  89.84 ( 81.15)\n",
      " * Acc@1 59.390 Acc@5 81.298\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cbd4260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.988 ( 1.988)\tLoss 4.4891e-01 (4.4891e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.690 ( 0.206)\tLoss 6.0852e-01 (6.6687e-01)\tAcc@1  86.72 ( 85.45)\tAcc@5  95.31 ( 97.06)\n",
      "Test: [ 40/391]\tTime  0.686 ( 0.165)\tLoss 6.6657e-01 (6.9179e-01)\tAcc@1  85.94 ( 84.74)\tAcc@5  96.09 ( 96.93)\n",
      "Test: [ 60/391]\tTime  0.592 ( 0.145)\tLoss 8.5541e-01 (6.5545e-01)\tAcc@1  82.03 ( 86.23)\tAcc@5  94.53 ( 96.99)\n",
      "Test: [ 80/391]\tTime  0.607 ( 0.135)\tLoss 8.2408e-01 (7.1391e-01)\tAcc@1  78.12 ( 84.51)\tAcc@5  96.09 ( 96.76)\n",
      "Test: [100/391]\tTime  0.499 ( 0.128)\tLoss 5.8585e-01 (7.1670e-01)\tAcc@1  89.06 ( 84.26)\tAcc@5  96.09 ( 96.77)\n",
      "Test: [120/391]\tTime  0.509 ( 0.127)\tLoss 5.8293e-01 (7.2028e-01)\tAcc@1  90.62 ( 84.14)\tAcc@5  97.66 ( 96.86)\n",
      "Test: [140/391]\tTime  0.459 ( 0.124)\tLoss 9.9558e-01 (7.0931e-01)\tAcc@1  75.78 ( 84.32)\tAcc@5  97.66 ( 97.00)\n",
      "Test: [160/391]\tTime  0.732 ( 0.126)\tLoss 8.4159e-01 (7.1901e-01)\tAcc@1  78.91 ( 84.12)\tAcc@5  95.31 ( 96.85)\n",
      "Test: [180/391]\tTime  0.622 ( 0.124)\tLoss 1.5546e+00 (7.6004e-01)\tAcc@1  61.72 ( 83.04)\tAcc@5  90.62 ( 96.36)\n",
      "Test: [200/391]\tTime  0.450 ( 0.121)\tLoss 6.5930e-01 (8.0433e-01)\tAcc@1  81.25 ( 81.92)\tAcc@5  98.44 ( 95.90)\n",
      "Test: [220/391]\tTime  0.582 ( 0.120)\tLoss 5.0241e-01 (8.2212e-01)\tAcc@1  89.84 ( 81.52)\tAcc@5  97.66 ( 95.73)\n",
      "Test: [240/391]\tTime  0.389 ( 0.119)\tLoss 9.4424e-01 (8.3279e-01)\tAcc@1  80.47 ( 81.37)\tAcc@5  90.62 ( 95.53)\n",
      "Test: [260/391]\tTime  0.476 ( 0.118)\tLoss 9.3092e-01 (8.6392e-01)\tAcc@1  78.12 ( 80.47)\tAcc@5  96.09 ( 95.21)\n",
      "Test: [280/391]\tTime  0.269 ( 0.118)\tLoss 1.0233e+00 (8.7488e-01)\tAcc@1  75.00 ( 80.27)\tAcc@5  92.97 ( 95.10)\n",
      "Test: [300/391]\tTime  0.048 ( 0.117)\tLoss 7.5923e-01 (8.8897e-01)\tAcc@1  84.38 ( 79.94)\tAcc@5  94.53 ( 94.89)\n",
      "Test: [320/391]\tTime  0.047 ( 0.116)\tLoss 6.7843e-01 (9.0217e-01)\tAcc@1  85.94 ( 79.69)\tAcc@5  96.88 ( 94.74)\n",
      "Test: [340/391]\tTime  0.047 ( 0.116)\tLoss 7.2675e-01 (9.1416e-01)\tAcc@1  82.03 ( 79.35)\tAcc@5  96.88 ( 94.66)\n",
      "Test: [360/391]\tTime  0.048 ( 0.115)\tLoss 1.3458e+00 (9.2365e-01)\tAcc@1  71.88 ( 79.10)\tAcc@5  96.09 ( 94.60)\n",
      "Test: [380/391]\tTime  0.048 ( 0.115)\tLoss 8.5499e-01 (9.2149e-01)\tAcc@1  79.69 ( 79.13)\tAcc@5  97.66 ( 94.64)\n",
      " * Acc@1 79.192 Acc@5 94.674\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f06f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.242 ( 2.242)\tLoss 4.7083e-01 (4.7083e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.478 ( 0.500)\tLoss 6.1043e-01 (6.5887e-01)\tAcc@1  85.16 ( 85.86)\tAcc@5  96.09 ( 97.36)\n",
      "Test: [ 40/391]\tTime  0.393 ( 0.433)\tLoss 6.6422e-01 (6.8760e-01)\tAcc@1  88.28 ( 84.89)\tAcc@5  95.31 ( 97.08)\n",
      "Test: [ 60/391]\tTime  0.368 ( 0.415)\tLoss 8.3477e-01 (6.5151e-01)\tAcc@1  82.81 ( 86.40)\tAcc@5  94.53 ( 97.14)\n",
      "Test: [ 80/391]\tTime  0.202 ( 0.405)\tLoss 7.9118e-01 (7.0846e-01)\tAcc@1  77.34 ( 84.69)\tAcc@5  97.66 ( 96.88)\n",
      "Test: [100/391]\tTime  0.144 ( 0.362)\tLoss 5.6696e-01 (7.1231e-01)\tAcc@1  90.62 ( 84.42)\tAcc@5  96.09 ( 96.92)\n",
      "Test: [120/391]\tTime  0.114 ( 0.340)\tLoss 5.5845e-01 (7.1649e-01)\tAcc@1  89.84 ( 84.27)\tAcc@5  97.66 ( 96.99)\n",
      "Test: [140/391]\tTime  0.153 ( 0.326)\tLoss 1.0032e+00 (7.0574e-01)\tAcc@1  75.00 ( 84.50)\tAcc@5  97.66 ( 97.13)\n",
      "Test: [160/391]\tTime  0.124 ( 0.317)\tLoss 8.3963e-01 (7.1563e-01)\tAcc@1  78.12 ( 84.27)\tAcc@5  96.88 ( 96.96)\n",
      "Test: [180/391]\tTime  0.192 ( 0.309)\tLoss 1.5545e+00 (7.5738e-01)\tAcc@1  62.50 ( 83.15)\tAcc@5  88.28 ( 96.46)\n",
      "Test: [200/391]\tTime  0.230 ( 0.306)\tLoss 6.4806e-01 (8.0195e-01)\tAcc@1  83.59 ( 82.05)\tAcc@5  98.44 ( 95.99)\n",
      "Test: [220/391]\tTime  0.300 ( 0.303)\tLoss 4.6700e-01 (8.1941e-01)\tAcc@1  90.62 ( 81.65)\tAcc@5  98.44 ( 95.83)\n",
      "Test: [240/391]\tTime  0.113 ( 0.299)\tLoss 9.1023e-01 (8.2960e-01)\tAcc@1  85.16 ( 81.52)\tAcc@5  90.62 ( 95.60)\n",
      "Test: [260/391]\tTime  0.281 ( 0.294)\tLoss 9.0270e-01 (8.6030e-01)\tAcc@1  78.12 ( 80.58)\tAcc@5  95.31 ( 95.29)\n",
      "Test: [280/391]\tTime  0.364 ( 0.297)\tLoss 1.0203e+00 (8.7148e-01)\tAcc@1  75.00 ( 80.34)\tAcc@5  93.75 ( 95.17)\n",
      "Test: [300/391]\tTime  0.395 ( 0.305)\tLoss 7.1447e-01 (8.8546e-01)\tAcc@1  84.38 ( 80.05)\tAcc@5  96.09 ( 94.93)\n",
      "Test: [320/391]\tTime  0.436 ( 0.310)\tLoss 6.7995e-01 (8.9759e-01)\tAcc@1  86.72 ( 79.80)\tAcc@5  96.88 ( 94.79)\n",
      "Test: [340/391]\tTime  0.399 ( 0.316)\tLoss 7.6236e-01 (9.1016e-01)\tAcc@1  82.81 ( 79.42)\tAcc@5  96.88 ( 94.71)\n",
      "Test: [360/391]\tTime  0.381 ( 0.319)\tLoss 1.3516e+00 (9.1946e-01)\tAcc@1  67.19 ( 79.17)\tAcc@5  95.31 ( 94.64)\n",
      "Test: [380/391]\tTime  0.329 ( 0.322)\tLoss 8.7935e-01 (9.1711e-01)\tAcc@1  79.69 ( 79.20)\tAcc@5  96.88 ( 94.66)\n",
      " * Acc@1 79.260 Acc@5 94.692\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1670598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.077 ( 2.077)\tLoss 4.6547e-01 (4.6547e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  98.44 ( 98.44)\n",
      "Test: [ 20/391]\tTime  0.837 ( 0.228)\tLoss 6.2792e-01 (6.6237e-01)\tAcc@1  85.94 ( 85.83)\tAcc@5  95.31 ( 97.17)\n",
      "Test: [ 40/391]\tTime  1.038 ( 0.189)\tLoss 6.9242e-01 (6.9257e-01)\tAcc@1  86.72 ( 84.87)\tAcc@5  95.31 ( 97.01)\n",
      "Test: [ 60/391]\tTime  0.912 ( 0.172)\tLoss 8.6182e-01 (6.5671e-01)\tAcc@1  82.81 ( 86.21)\tAcc@5  95.31 ( 97.04)\n",
      "Test: [ 80/391]\tTime  0.897 ( 0.163)\tLoss 7.9644e-01 (7.1278e-01)\tAcc@1  78.12 ( 84.53)\tAcc@5  96.88 ( 96.80)\n",
      "Test: [100/391]\tTime  0.783 ( 0.156)\tLoss 5.6916e-01 (7.1571e-01)\tAcc@1  89.06 ( 84.30)\tAcc@5  96.09 ( 96.81)\n",
      "Test: [120/391]\tTime  0.900 ( 0.153)\tLoss 5.6734e-01 (7.1983e-01)\tAcc@1  90.62 ( 84.19)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [140/391]\tTime  0.919 ( 0.150)\tLoss 9.8857e-01 (7.0834e-01)\tAcc@1  75.00 ( 84.40)\tAcc@5  97.66 ( 97.02)\n",
      "Test: [160/391]\tTime  1.056 ( 0.153)\tLoss 8.4610e-01 (7.1704e-01)\tAcc@1  78.12 ( 84.18)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [180/391]\tTime  0.736 ( 0.149)\tLoss 1.5641e+00 (7.5777e-01)\tAcc@1  60.94 ( 83.09)\tAcc@5  90.62 ( 96.37)\n",
      "Test: [200/391]\tTime  0.627 ( 0.145)\tLoss 6.5816e-01 (8.0165e-01)\tAcc@1  82.81 ( 81.98)\tAcc@5  97.66 ( 95.88)\n",
      "Test: [220/391]\tTime  0.790 ( 0.144)\tLoss 4.8150e-01 (8.1864e-01)\tAcc@1  89.84 ( 81.58)\tAcc@5  98.44 ( 95.73)\n",
      "Test: [240/391]\tTime  0.519 ( 0.143)\tLoss 9.2860e-01 (8.2877e-01)\tAcc@1  82.81 ( 81.45)\tAcc@5  90.62 ( 95.54)\n",
      "Test: [260/391]\tTime  0.609 ( 0.142)\tLoss 9.2503e-01 (8.5846e-01)\tAcc@1  78.91 ( 80.53)\tAcc@5  94.53 ( 95.24)\n",
      "Test: [280/391]\tTime  0.622 ( 0.141)\tLoss 1.0022e+00 (8.6890e-01)\tAcc@1  75.78 ( 80.32)\tAcc@5  94.53 ( 95.12)\n",
      "Test: [300/391]\tTime  0.422 ( 0.140)\tLoss 7.3524e-01 (8.8211e-01)\tAcc@1  82.81 ( 80.05)\tAcc@5  95.31 ( 94.90)\n",
      "Test: [320/391]\tTime  0.302 ( 0.138)\tLoss 6.6638e-01 (8.9407e-01)\tAcc@1  88.28 ( 79.80)\tAcc@5  96.88 ( 94.76)\n",
      "Test: [340/391]\tTime  0.468 ( 0.138)\tLoss 7.3893e-01 (9.0643e-01)\tAcc@1  83.59 ( 79.45)\tAcc@5  96.88 ( 94.68)\n",
      "Test: [360/391]\tTime  0.097 ( 0.137)\tLoss 1.2896e+00 (9.1562e-01)\tAcc@1  71.09 ( 79.22)\tAcc@5  96.09 ( 94.63)\n",
      "Test: [380/391]\tTime  0.324 ( 0.138)\tLoss 8.6037e-01 (9.1291e-01)\tAcc@1  79.69 ( 79.26)\tAcc@5  96.88 ( 94.66)\n",
      " * Acc@1 79.340 Acc@5 94.702\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07eab6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  3 03:07:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:21:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    39W / 250W |  38863MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     56778      C   ...orge3/envs/qmc/bin/python      483MiB |\n",
      "|    0   N/A  N/A     69205      C   ...orge3/envs/qmc/bin/python    36367MiB |\n",
      "|    0   N/A  N/A    636161      C   ...envs/tokenrank/bin/python     2011MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720aea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.158 ( 2.158)\tLoss 4.6547e-01 (4.6547e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  98.44 ( 98.44)\n",
      "Test: [ 20/391]\tTime  0.924 ( 0.240)\tLoss 6.2792e-01 (6.6237e-01)\tAcc@1  85.94 ( 85.83)\tAcc@5  95.31 ( 97.17)\n",
      "Test: [ 40/391]\tTime  0.874 ( 0.192)\tLoss 6.9242e-01 (6.9257e-01)\tAcc@1  86.72 ( 84.87)\tAcc@5  95.31 ( 97.01)\n",
      "Test: [ 60/391]\tTime  0.699 ( 0.167)\tLoss 8.6182e-01 (6.5671e-01)\tAcc@1  82.81 ( 86.21)\tAcc@5  95.31 ( 97.04)\n",
      "Test: [ 80/391]\tTime  0.822 ( 0.157)\tLoss 7.9644e-01 (7.1278e-01)\tAcc@1  78.12 ( 84.53)\tAcc@5  96.88 ( 96.80)\n",
      "Test: [100/391]\tTime  0.806 ( 0.151)\tLoss 5.6916e-01 (7.1571e-01)\tAcc@1  89.06 ( 84.30)\tAcc@5  96.09 ( 96.81)\n",
      "Test: [120/391]\tTime  0.761 ( 0.149)\tLoss 5.6734e-01 (7.1983e-01)\tAcc@1  90.62 ( 84.19)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [140/391]\tTime  0.582 ( 0.145)\tLoss 9.8857e-01 (7.0834e-01)\tAcc@1  75.00 ( 84.40)\tAcc@5  97.66 ( 97.02)\n",
      "Test: [160/391]\tTime  0.984 ( 0.147)\tLoss 8.4610e-01 (7.1704e-01)\tAcc@1  78.12 ( 84.18)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [180/391]\tTime  0.701 ( 0.145)\tLoss 1.5641e+00 (7.5777e-01)\tAcc@1  60.94 ( 83.09)\tAcc@5  90.62 ( 96.37)\n",
      "Test: [200/391]\tTime  0.431 ( 0.141)\tLoss 6.5816e-01 (8.0165e-01)\tAcc@1  82.81 ( 81.98)\tAcc@5  97.66 ( 95.88)\n",
      "Test: [220/391]\tTime  0.415 ( 0.140)\tLoss 4.8150e-01 (8.1864e-01)\tAcc@1  89.84 ( 81.58)\tAcc@5  98.44 ( 95.73)\n",
      "Test: [240/391]\tTime  0.335 ( 0.139)\tLoss 9.2860e-01 (8.2877e-01)\tAcc@1  82.81 ( 81.45)\tAcc@5  90.62 ( 95.54)\n",
      "Test: [260/391]\tTime  0.331 ( 0.138)\tLoss 9.2503e-01 (8.5846e-01)\tAcc@1  78.91 ( 80.53)\tAcc@5  94.53 ( 95.24)\n",
      "Test: [280/391]\tTime  0.047 ( 0.137)\tLoss 1.0022e+00 (8.6890e-01)\tAcc@1  75.78 ( 80.32)\tAcc@5  94.53 ( 95.12)\n",
      "Test: [300/391]\tTime  0.048 ( 0.137)\tLoss 7.3524e-01 (8.8211e-01)\tAcc@1  82.81 ( 80.05)\tAcc@5  95.31 ( 94.90)\n",
      "Test: [320/391]\tTime  0.057 ( 0.136)\tLoss 6.6638e-01 (8.9407e-01)\tAcc@1  88.28 ( 79.80)\tAcc@5  96.88 ( 94.76)\n",
      "Test: [340/391]\tTime  0.047 ( 0.136)\tLoss 7.3893e-01 (9.0643e-01)\tAcc@1  83.59 ( 79.45)\tAcc@5  96.88 ( 94.68)\n",
      "Test: [360/391]\tTime  0.049 ( 0.135)\tLoss 1.2896e+00 (9.1562e-01)\tAcc@1  71.09 ( 79.22)\tAcc@5  96.09 ( 94.63)\n",
      "Test: [380/391]\tTime  0.047 ( 0.134)\tLoss 8.6037e-01 (9.1291e-01)\tAcc@1  79.69 ( 79.26)\tAcc@5  96.88 ( 94.66)\n",
      " * Acc@1 79.340 Acc@5 94.702\n"
     ]
    }
   ],
   "source": [
    "# emphasizing after average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07e0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  1.984 ( 1.984)\tLoss 5.0387e-01 (5.0387e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.845 ( 0.213)\tLoss 6.8122e-01 (7.1524e-01)\tAcc@1  88.28 ( 84.71)\tAcc@5  95.31 ( 96.61)\n",
      "Test: [ 40/391]\tTime  0.866 ( 0.176)\tLoss 6.6253e-01 (7.4280e-01)\tAcc@1  87.50 ( 83.59)\tAcc@5  94.53 ( 96.57)\n",
      "Test: [ 60/391]\tTime  0.810 ( 0.160)\tLoss 8.1139e-01 (7.0165e-01)\tAcc@1  81.25 ( 85.14)\tAcc@5  96.88 ( 96.67)\n",
      "Test: [ 80/391]\tTime  0.802 ( 0.151)\tLoss 8.3225e-01 (7.6430e-01)\tAcc@1  78.12 ( 83.16)\tAcc@5  96.88 ( 96.40)\n",
      "Test: [100/391]\tTime  0.804 ( 0.146)\tLoss 5.7633e-01 (7.6976e-01)\tAcc@1  89.84 ( 82.90)\tAcc@5  96.09 ( 96.40)\n",
      "Test: [120/391]\tTime  0.222 ( 0.144)\tLoss 6.4271e-01 (7.6914e-01)\tAcc@1  88.28 ( 82.86)\tAcc@5  97.66 ( 96.53)\n",
      "Test: [140/391]\tTime  0.122 ( 0.142)\tLoss 1.1322e+00 (7.5928e-01)\tAcc@1  73.44 ( 83.08)\tAcc@5  96.09 ( 96.64)\n",
      "Test: [160/391]\tTime  0.746 ( 0.145)\tLoss 8.1318e-01 (7.6675e-01)\tAcc@1  82.03 ( 82.99)\tAcc@5  95.31 ( 96.51)\n",
      "Test: [180/391]\tTime  0.311 ( 0.142)\tLoss 1.6438e+00 (8.0682e-01)\tAcc@1  59.38 ( 81.86)\tAcc@5  89.06 ( 96.06)\n",
      "Test: [200/391]\tTime  0.048 ( 0.140)\tLoss 8.3527e-01 (8.4907e-01)\tAcc@1  78.12 ( 80.80)\tAcc@5  96.88 ( 95.59)\n",
      "Test: [220/391]\tTime  0.047 ( 0.139)\tLoss 4.9818e-01 (8.6570e-01)\tAcc@1  91.41 ( 80.46)\tAcc@5  98.44 ( 95.40)\n",
      "Test: [240/391]\tTime  0.054 ( 0.138)\tLoss 8.8217e-01 (8.7533e-01)\tAcc@1  82.03 ( 80.33)\tAcc@5  91.41 ( 95.21)\n",
      "Test: [260/391]\tTime  0.048 ( 0.137)\tLoss 1.0331e+00 (9.0375e-01)\tAcc@1  75.78 ( 79.46)\tAcc@5  93.75 ( 94.93)\n",
      "Test: [280/391]\tTime  0.049 ( 0.137)\tLoss 1.0295e+00 (9.1466e-01)\tAcc@1  74.22 ( 79.25)\tAcc@5  93.75 ( 94.81)\n",
      "Test: [300/391]\tTime  0.054 ( 0.136)\tLoss 8.1589e-01 (9.2676e-01)\tAcc@1  84.38 ( 79.01)\tAcc@5  93.75 ( 94.61)\n",
      "Test: [320/391]\tTime  0.047 ( 0.135)\tLoss 6.7974e-01 (9.3797e-01)\tAcc@1  87.50 ( 78.77)\tAcc@5  96.09 ( 94.47)\n",
      "Test: [340/391]\tTime  0.047 ( 0.135)\tLoss 8.7307e-01 (9.5079e-01)\tAcc@1  77.34 ( 78.43)\tAcc@5  96.88 ( 94.37)\n",
      "Test: [360/391]\tTime  0.047 ( 0.135)\tLoss 1.2947e+00 (9.5878e-01)\tAcc@1  71.09 ( 78.26)\tAcc@5  95.31 ( 94.35)\n",
      "Test: [380/391]\tTime  0.048 ( 0.135)\tLoss 7.7052e-01 (9.5533e-01)\tAcc@1  84.38 ( 78.35)\tAcc@5  98.44 ( 94.40)\n",
      " * Acc@1 78.436 Acc@5 94.460\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a24d9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.161 ( 2.161)\tLoss 4.3257e-01 (4.3257e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.650 ( 0.223)\tLoss 5.9604e-01 (6.4916e-01)\tAcc@1  86.72 ( 85.71)\tAcc@5  96.09 ( 97.28)\n",
      "Test: [ 40/391]\tTime  0.609 ( 0.176)\tLoss 6.2325e-01 (6.7726e-01)\tAcc@1  86.72 ( 84.78)\tAcc@5  96.88 ( 97.10)\n",
      "Test: [ 60/391]\tTime  0.522 ( 0.156)\tLoss 8.3152e-01 (6.4015e-01)\tAcc@1  82.03 ( 86.28)\tAcc@5  95.31 ( 97.13)\n",
      "Test: [ 80/391]\tTime  0.561 ( 0.145)\tLoss 7.5286e-01 (6.9579e-01)\tAcc@1  80.47 ( 84.69)\tAcc@5  97.66 ( 96.90)\n",
      "Test: [100/391]\tTime  0.532 ( 0.138)\tLoss 5.5953e-01 (6.9885e-01)\tAcc@1  89.06 ( 84.44)\tAcc@5  96.09 ( 96.88)\n",
      "Test: [120/391]\tTime  0.612 ( 0.136)\tLoss 5.5679e-01 (7.0265e-01)\tAcc@1  89.84 ( 84.40)\tAcc@5  97.66 ( 96.95)\n",
      "Test: [140/391]\tTime  0.488 ( 0.133)\tLoss 1.0149e+00 (6.9270e-01)\tAcc@1  74.22 ( 84.59)\tAcc@5  98.44 ( 97.09)\n",
      "Test: [160/391]\tTime  0.700 ( 0.135)\tLoss 7.7989e-01 (7.0204e-01)\tAcc@1  80.47 ( 84.41)\tAcc@5  96.88 ( 96.95)\n",
      "Test: [180/391]\tTime  0.538 ( 0.132)\tLoss 1.5493e+00 (7.4047e-01)\tAcc@1  61.72 ( 83.34)\tAcc@5  89.84 ( 96.49)\n",
      "Test: [200/391]\tTime  0.281 ( 0.129)\tLoss 6.5076e-01 (7.8193e-01)\tAcc@1  79.69 ( 82.26)\tAcc@5  98.44 ( 96.02)\n",
      "Test: [220/391]\tTime  0.404 ( 0.127)\tLoss 4.3297e-01 (7.9691e-01)\tAcc@1  92.19 ( 81.90)\tAcc@5  98.44 ( 95.88)\n",
      "Test: [240/391]\tTime  0.065 ( 0.126)\tLoss 8.6810e-01 (8.0583e-01)\tAcc@1  83.59 ( 81.83)\tAcc@5  90.62 ( 95.68)\n",
      "Test: [260/391]\tTime  0.091 ( 0.125)\tLoss 8.8418e-01 (8.3531e-01)\tAcc@1  78.91 ( 80.95)\tAcc@5  95.31 ( 95.38)\n",
      "Test: [280/391]\tTime  0.154 ( 0.124)\tLoss 1.0027e+00 (8.4498e-01)\tAcc@1  75.78 ( 80.73)\tAcc@5  93.75 ( 95.27)\n",
      "Test: [300/391]\tTime  0.143 ( 0.123)\tLoss 7.4938e-01 (8.5679e-01)\tAcc@1  84.38 ( 80.49)\tAcc@5  96.88 ( 95.08)\n",
      "Test: [320/391]\tTime  0.059 ( 0.122)\tLoss 6.1438e-01 (8.6744e-01)\tAcc@1  89.06 ( 80.26)\tAcc@5  96.88 ( 94.97)\n",
      "Test: [340/391]\tTime  0.101 ( 0.121)\tLoss 7.8142e-01 (8.7862e-01)\tAcc@1  82.03 ( 79.94)\tAcc@5  96.88 ( 94.91)\n",
      "Test: [360/391]\tTime  0.060 ( 0.122)\tLoss 1.2436e+00 (8.8669e-01)\tAcc@1  71.88 ( 79.73)\tAcc@5  96.09 ( 94.88)\n",
      "Test: [380/391]\tTime  0.059 ( 0.121)\tLoss 7.9419e-01 (8.8441e-01)\tAcc@1  80.47 ( 79.76)\tAcc@5  97.66 ( 94.91)\n",
      " * Acc@1 79.826 Acc@5 94.952\n"
     ]
    }
   ],
   "source": [
    "#no pruning\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 1 --retain_rate_list 1 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be7e1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.082 ( 2.082)\tLoss 4.4167e-01 (4.4167e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.598 ( 0.214)\tLoss 5.9877e-01 (6.5262e-01)\tAcc@1  86.72 ( 85.83)\tAcc@5  95.31 ( 97.25)\n",
      "Test: [ 40/391]\tTime  0.551 ( 0.168)\tLoss 6.4187e-01 (6.8102e-01)\tAcc@1  89.06 ( 84.98)\tAcc@5  96.09 ( 97.03)\n",
      "Test: [ 60/391]\tTime  0.480 ( 0.147)\tLoss 8.4044e-01 (6.4450e-01)\tAcc@1  82.03 ( 86.44)\tAcc@5  95.31 ( 97.11)\n",
      "Test: [ 80/391]\tTime  0.510 ( 0.137)\tLoss 7.6192e-01 (7.0062e-01)\tAcc@1  80.47 ( 84.84)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [100/391]\tTime  0.489 ( 0.131)\tLoss 5.6046e-01 (7.0370e-01)\tAcc@1  89.84 ( 84.61)\tAcc@5  96.09 ( 96.88)\n",
      "Test: [120/391]\tTime  0.514 ( 0.129)\tLoss 5.5962e-01 (7.0763e-01)\tAcc@1  89.84 ( 84.51)\tAcc@5  97.66 ( 96.96)\n",
      "Test: [140/391]\tTime  0.471 ( 0.126)\tLoss 1.0081e+00 (6.9716e-01)\tAcc@1  74.22 ( 84.69)\tAcc@5  98.44 ( 97.12)\n",
      "Test: [160/391]\tTime  0.666 ( 0.128)\tLoss 7.9913e-01 (7.0632e-01)\tAcc@1  81.25 ( 84.47)\tAcc@5  96.88 ( 96.98)\n",
      "Test: [180/391]\tTime  0.478 ( 0.124)\tLoss 1.5504e+00 (7.4602e-01)\tAcc@1  62.50 ( 83.42)\tAcc@5  88.28 ( 96.50)\n",
      "Test: [200/391]\tTime  0.329 ( 0.121)\tLoss 6.5556e-01 (7.8826e-01)\tAcc@1  79.69 ( 82.31)\tAcc@5  97.66 ( 96.03)\n",
      "Test: [220/391]\tTime  0.065 ( 0.121)\tLoss 4.5983e-01 (8.0441e-01)\tAcc@1  91.41 ( 81.93)\tAcc@5  98.44 ( 95.90)\n",
      "Test: [240/391]\tTime  0.058 ( 0.120)\tLoss 8.8865e-01 (8.1403e-01)\tAcc@1  84.38 ( 81.83)\tAcc@5  91.41 ( 95.69)\n",
      "Test: [260/391]\tTime  0.058 ( 0.119)\tLoss 8.6655e-01 (8.4391e-01)\tAcc@1  78.12 ( 80.92)\tAcc@5  95.31 ( 95.38)\n",
      "Test: [280/391]\tTime  0.058 ( 0.118)\tLoss 1.0125e+00 (8.5411e-01)\tAcc@1  75.78 ( 80.70)\tAcc@5  93.75 ( 95.28)\n",
      "Test: [300/391]\tTime  0.060 ( 0.117)\tLoss 7.1595e-01 (8.6665e-01)\tAcc@1  83.59 ( 80.42)\tAcc@5  96.88 ( 95.10)\n",
      "Test: [320/391]\tTime  0.059 ( 0.116)\tLoss 6.5285e-01 (8.7806e-01)\tAcc@1  87.50 ( 80.18)\tAcc@5  96.88 ( 94.99)\n",
      "Test: [340/391]\tTime  0.058 ( 0.116)\tLoss 7.4731e-01 (8.8931e-01)\tAcc@1  82.81 ( 79.85)\tAcc@5  96.88 ( 94.93)\n",
      "Test: [360/391]\tTime  0.060 ( 0.116)\tLoss 1.2982e+00 (8.9802e-01)\tAcc@1  71.09 ( 79.62)\tAcc@5  96.09 ( 94.88)\n",
      "Test: [380/391]\tTime  0.059 ( 0.115)\tLoss 8.3956e-01 (8.9570e-01)\tAcc@1  80.47 ( 79.67)\tAcc@5  97.66 ( 94.92)\n",
      " * Acc@1 79.732 Acc@5 94.958\n"
     ]
    }
   ],
   "source": [
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 3 4 5 6 7 8 9 10 --retain_rate_list 0.99 0.98 0.99 0.98 0.95 0.90 0.85 0.9 0.85 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17600d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_small_patch16_224-cd65a155.pth\n",
      "number of params: 22050664\n",
      "Test: [  0/391]\tTime  2.118 ( 2.118)\tLoss 4.4143e-01 (4.4143e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.620 ( 0.215)\tLoss 5.9586e-01 (6.5236e-01)\tAcc@1  86.72 ( 85.79)\tAcc@5  95.31 ( 97.21)\n",
      "Test: [ 40/391]\tTime  0.587 ( 0.170)\tLoss 6.4995e-01 (6.8075e-01)\tAcc@1  88.28 ( 84.91)\tAcc@5  95.31 ( 96.99)\n",
      "Test: [ 60/391]\tTime  0.544 ( 0.151)\tLoss 8.3758e-01 (6.4397e-01)\tAcc@1  82.03 ( 86.40)\tAcc@5  95.31 ( 97.09)\n",
      "Test: [ 80/391]\tTime  0.508 ( 0.140)\tLoss 7.5973e-01 (7.0003e-01)\tAcc@1  80.47 ( 84.85)\tAcc@5  96.88 ( 96.87)\n",
      "Test: [100/391]\tTime  0.534 ( 0.134)\tLoss 5.5811e-01 (7.0333e-01)\tAcc@1  89.84 ( 84.62)\tAcc@5  96.09 ( 96.86)\n",
      "Test: [120/391]\tTime  0.467 ( 0.132)\tLoss 5.5139e-01 (7.0719e-01)\tAcc@1  89.84 ( 84.53)\tAcc@5  98.44 ( 96.94)\n",
      "Test: [140/391]\tTime  0.430 ( 0.129)\tLoss 1.0059e+00 (6.9665e-01)\tAcc@1  75.00 ( 84.71)\tAcc@5  98.44 ( 97.09)\n",
      "Test: [160/391]\tTime  0.642 ( 0.131)\tLoss 7.9667e-01 (7.0597e-01)\tAcc@1  79.69 ( 84.45)\tAcc@5  96.88 ( 96.95)\n",
      "Test: [180/391]\tTime  0.503 ( 0.128)\tLoss 1.5569e+00 (7.4579e-01)\tAcc@1  62.50 ( 83.37)\tAcc@5  88.28 ( 96.48)\n",
      "Test: [200/391]\tTime  0.306 ( 0.125)\tLoss 6.4940e-01 (7.8789e-01)\tAcc@1  79.69 ( 82.26)\tAcc@5  98.44 ( 96.02)\n",
      "Test: [220/391]\tTime  0.430 ( 0.124)\tLoss 4.5403e-01 (8.0381e-01)\tAcc@1  91.41 ( 81.88)\tAcc@5  98.44 ( 95.90)\n",
      "Test: [240/391]\tTime  0.092 ( 0.123)\tLoss 8.9152e-01 (8.1340e-01)\tAcc@1  84.38 ( 81.78)\tAcc@5  91.41 ( 95.68)\n",
      "Test: [260/391]\tTime  0.154 ( 0.122)\tLoss 8.7112e-01 (8.4319e-01)\tAcc@1  78.12 ( 80.88)\tAcc@5  95.31 ( 95.37)\n",
      "Test: [280/391]\tTime  0.117 ( 0.121)\tLoss 1.0147e+00 (8.5348e-01)\tAcc@1  75.00 ( 80.65)\tAcc@5  93.75 ( 95.26)\n",
      "Test: [300/391]\tTime  0.058 ( 0.120)\tLoss 7.1559e-01 (8.6599e-01)\tAcc@1  84.38 ( 80.38)\tAcc@5  96.88 ( 95.09)\n",
      "Test: [320/391]\tTime  0.059 ( 0.120)\tLoss 6.5847e-01 (8.7748e-01)\tAcc@1  87.50 ( 80.16)\tAcc@5  96.88 ( 94.97)\n",
      "Test: [340/391]\tTime  0.059 ( 0.120)\tLoss 7.4373e-01 (8.8870e-01)\tAcc@1  82.03 ( 79.83)\tAcc@5  96.88 ( 94.90)\n",
      "Test: [360/391]\tTime  0.059 ( 0.119)\tLoss 1.2953e+00 (8.9750e-01)\tAcc@1  69.53 ( 79.60)\tAcc@5  96.09 ( 94.85)\n",
      "Test: [380/391]\tTime  0.110 ( 0.118)\tLoss 8.3924e-01 (8.9514e-01)\tAcc@1  80.47 ( 79.64)\tAcc@5  97.66 ( 94.89)\n",
      " * Acc@1 79.702 Acc@5 94.936\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_small_3' --prune_list 2 3 4 5 6 7 8 9 10 --retain_rate_list 0.99 0.98 0.99 0.98 0.95 0.90 0.85 0.9 0.85 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352f0787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 3.344G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  3.286G     |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.297G    |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.301M   |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    0.112G   |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     69.452M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     23.151M |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.301M   |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.185G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     92.602M |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     92.602M |\n",
      "|   blocks.4                 |   1.774M               |   0.297G    |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.301M   |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    0.112G   |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     69.452M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     23.151M |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.301M   |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.185G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     92.602M |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     92.602M |\n",
      "|   blocks.5                 |   1.774M               |   0.297G    |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.301M   |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    0.112G   |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     69.452M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     23.151M |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.301M   |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.185G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     92.602M |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     92.602M |\n",
      "|   blocks.6                 |   1.774M               |   0.234G    |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.24M    |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    85.728M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     55.296M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     18.432M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.24M    |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    0.147G   |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     73.728M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     73.728M |\n",
      "|   blocks.7                 |   1.774M               |   0.234G    |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.24M    |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    85.728M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     55.296M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     18.432M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.24M    |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    0.147G   |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     73.728M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     73.728M |\n",
      "|   blocks.8                 |   1.774M               |   0.234G    |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.24M    |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    85.806M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     55.296M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     18.432M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.24M    |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    0.147G   |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     73.728M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     73.728M |\n",
      "|   blocks.9                 |   1.774M               |   0.185G    |\n",
      "|    blocks.9.norm1          |    0.768K              |    0.192M   |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    66.662M  |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     44.237M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     14.746M |\n",
      "|    blocks.9.norm2          |    0.768K              |    0.192M   |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    0.118G   |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     58.982M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     58.982M |\n",
      "|   blocks.10                |   1.774M               |   0.185G    |\n",
      "|    blocks.10.norm1         |    0.768K              |    0.192M   |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    66.662M  |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     44.237M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     14.746M |\n",
      "|    blocks.10.norm2         |    0.768K              |    0.192M   |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    0.118G   |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     58.982M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     58.982M |\n",
      "|   blocks.11                |   1.774M               |   0.185G    |\n",
      "|    blocks.11.norm1         |    0.768K              |    0.192M   |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    66.662M  |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     44.237M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     14.746M |\n",
      "|    blocks.11.norm2         |    0.768K              |    0.192M   |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    0.118G   |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     58.982M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     58.982M |\n",
      "|  norm                      |  0.768K                |  0.192M     |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 3.34G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 3.29G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.11G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 69.45M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.19G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.11G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 69.45M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.19G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.11G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 69.45M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.3M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.19G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 92.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.23G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 85.73M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 55.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.15G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.23G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 85.73M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 55.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.15G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.23G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 85.81M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 55.3M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.24M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.15G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 73.73M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.19G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 66.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 44.24M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.75M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.19G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 66.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 44.24M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.75M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.19G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 66.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 44.24M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.75M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.12G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 58.98M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.19M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "3344264343\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.8, 0.8, 0.8]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ccfa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 2.873G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  2.815G     |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.259G    |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.265M   |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    96.022M  |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     61.047M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     20.349M |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.265M   |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.163G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     81.396M |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     81.396M |\n",
      "|   blocks.4                 |   1.774M               |   0.259G    |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.265M   |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    96.022M  |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     61.047M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     20.349M |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.265M   |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.163G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     81.396M |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     81.396M |\n",
      "|   blocks.5                 |   1.774M               |   0.259G    |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.265M   |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    96.117M  |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     61.047M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     20.349M |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.265M   |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.163G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     81.396M |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     81.396M |\n",
      "|   blocks.6                 |   1.774M               |   0.177G    |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.184M   |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    63.701M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     42.467M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     14.156M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.184M   |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    0.113G   |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     56.623M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     56.623M |\n",
      "|   blocks.7                 |   1.774M               |   0.177G    |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.184M   |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    63.701M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     42.467M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     14.156M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.184M   |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    0.113G   |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     56.623M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     56.623M |\n",
      "|   blocks.8                 |   1.774M               |   0.177G    |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.184M   |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    63.747M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     42.467M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     14.156M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.184M   |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    0.113G   |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     56.623M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     56.623M |\n",
      "|   blocks.9                 |   1.774M               |   0.122G    |\n",
      "|    blocks.9.norm1          |    0.768K              |    0.129M   |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    42.966M  |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     29.639M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     9.88M   |\n",
      "|    blocks.9.norm2          |    0.768K              |    0.129M   |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    79.036M  |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     39.518M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     39.518M |\n",
      "|   blocks.10                |   1.774M               |   0.122G    |\n",
      "|    blocks.10.norm1         |    0.768K              |    0.129M   |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    42.966M  |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     29.639M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     9.88M   |\n",
      "|    blocks.10.norm2         |    0.768K              |    0.129M   |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    79.036M  |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     39.518M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     39.518M |\n",
      "|   blocks.11                |   1.774M               |   0.122G    |\n",
      "|    blocks.11.norm1         |    0.768K              |    0.129M   |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    42.966M  |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     29.639M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     9.88M   |\n",
      "|    blocks.11.norm2         |    0.768K              |    0.129M   |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    79.036M  |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     39.518M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     39.518M |\n",
      "|  norm                      |  0.768K                |  0.129M     |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 2.87G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 2.81G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.12M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 63.7M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.47M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 63.7M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.47M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 63.75M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.47M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 56.62M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 42.97M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 29.64M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 9.88M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 79.04M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 42.97M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 29.64M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 9.88M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 79.04M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 42.97M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 29.64M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 9.88M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 79.04M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 39.52M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.13M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "2872848497\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.7,0.7,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef006c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 22.051M                | 2.478G      |\n",
      "|  cls_token                 |  (1, 1, 384)           |             |\n",
      "|  pos_embed                 |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj          |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight  |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (384,)               |             |\n",
      "|  blocks                    |  21.294M               |  2.42G      |\n",
      "|   blocks.0                 |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.0.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.0.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.1                 |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.1.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.1.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.2                 |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.2.attn           |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv      |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj     |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2          |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.2.mlp            |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1       |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2       |     0.59M              |     0.116G  |\n",
      "|   blocks.3                 |   1.774M               |   0.22G     |\n",
      "|    blocks.3.norm1          |    0.768K              |    0.227M   |\n",
      "|     blocks.3.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.3.attn           |    0.591M              |    80.293M  |\n",
      "|     blocks.3.attn.qkv      |     0.444M             |     52.199M |\n",
      "|     blocks.3.attn.proj     |     0.148M             |     17.4M   |\n",
      "|    blocks.3.norm2          |    0.768K              |    0.227M   |\n",
      "|     blocks.3.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.3.mlp            |    1.182M              |    0.139G   |\n",
      "|     blocks.3.mlp.fc1       |     0.591M             |     69.599M |\n",
      "|     blocks.3.mlp.fc2       |     0.59M              |     69.599M |\n",
      "|   blocks.4                 |   1.774M               |   0.22G     |\n",
      "|    blocks.4.norm1          |    0.768K              |    0.227M   |\n",
      "|     blocks.4.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.4.attn           |    0.591M              |    80.293M  |\n",
      "|     blocks.4.attn.qkv      |     0.444M             |     52.199M |\n",
      "|     blocks.4.attn.proj     |     0.148M             |     17.4M   |\n",
      "|    blocks.4.norm2          |    0.768K              |    0.227M   |\n",
      "|     blocks.4.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.4.mlp            |    1.182M              |    0.139G   |\n",
      "|     blocks.4.mlp.fc1       |     0.591M             |     69.599M |\n",
      "|     blocks.4.mlp.fc2       |     0.59M              |     69.599M |\n",
      "|   blocks.5                 |   1.774M               |   0.22G     |\n",
      "|    blocks.5.norm1          |    0.768K              |    0.227M   |\n",
      "|     blocks.5.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.5.attn           |    0.591M              |    80.362M  |\n",
      "|     blocks.5.attn.qkv      |     0.444M             |     52.199M |\n",
      "|     blocks.5.attn.proj     |     0.148M             |     17.4M   |\n",
      "|    blocks.5.norm2          |    0.768K              |    0.227M   |\n",
      "|     blocks.5.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.5.mlp            |    1.182M              |    0.139G   |\n",
      "|     blocks.5.mlp.fc1       |     0.591M             |     69.599M |\n",
      "|     blocks.5.mlp.fc2       |     0.59M              |     69.599M |\n",
      "|   blocks.6                 |   1.774M               |   0.13G     |\n",
      "|    blocks.6.norm1          |    0.768K              |    0.136M   |\n",
      "|     blocks.6.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.6.attn           |    0.591M              |    45.749M  |\n",
      "|     blocks.6.attn.qkv      |     0.444M             |     31.408M |\n",
      "|     blocks.6.attn.proj     |     0.148M             |     10.469M |\n",
      "|    blocks.6.norm2          |    0.768K              |    0.136M   |\n",
      "|     blocks.6.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.6.mlp            |    1.182M              |    83.755M  |\n",
      "|     blocks.6.mlp.fc1       |     0.591M             |     41.878M |\n",
      "|     blocks.6.mlp.fc2       |     0.59M              |     41.878M |\n",
      "|   blocks.7                 |   1.774M               |   0.13G     |\n",
      "|    blocks.7.norm1          |    0.768K              |    0.136M   |\n",
      "|     blocks.7.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.7.attn           |    0.591M              |    45.749M  |\n",
      "|     blocks.7.attn.qkv      |     0.444M             |     31.408M |\n",
      "|     blocks.7.attn.proj     |     0.148M             |     10.469M |\n",
      "|    blocks.7.norm2          |    0.768K              |    0.136M   |\n",
      "|     blocks.7.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.7.mlp            |    1.182M              |    83.755M  |\n",
      "|     blocks.7.mlp.fc1       |     0.591M             |     41.878M |\n",
      "|     blocks.7.mlp.fc2       |     0.59M              |     41.878M |\n",
      "|   blocks.8                 |   1.774M               |   0.13G     |\n",
      "|    blocks.8.norm1          |    0.768K              |    0.136M   |\n",
      "|     blocks.8.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.8.attn           |    0.591M              |    45.774M  |\n",
      "|     blocks.8.attn.qkv      |     0.444M             |     31.408M |\n",
      "|     blocks.8.attn.proj     |     0.148M             |     10.469M |\n",
      "|    blocks.8.norm2          |    0.768K              |    0.136M   |\n",
      "|     blocks.8.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.8.mlp            |    1.182M              |    83.755M  |\n",
      "|     blocks.8.mlp.fc1       |     0.591M             |     41.878M |\n",
      "|     blocks.8.mlp.fc2       |     0.59M              |     41.878M |\n",
      "|   blocks.9                 |   1.774M               |   77.672M   |\n",
      "|    blocks.9.norm1          |    0.768K              |    82.56K   |\n",
      "|     blocks.9.norm1.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (384,)             |             |\n",
      "|    blocks.9.attn           |    0.591M              |    26.782M  |\n",
      "|     blocks.9.attn.qkv      |     0.444M             |     19.022M |\n",
      "|     blocks.9.attn.proj     |     0.148M             |     6.341M  |\n",
      "|    blocks.9.norm2          |    0.768K              |    82.56K   |\n",
      "|     blocks.9.norm2.weight  |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (384,)             |             |\n",
      "|    blocks.9.mlp            |    1.182M              |    50.725M  |\n",
      "|     blocks.9.mlp.fc1       |     0.591M             |     25.362M |\n",
      "|     blocks.9.mlp.fc2       |     0.59M              |     25.362M |\n",
      "|   blocks.10                |   1.774M               |   77.672M   |\n",
      "|    blocks.10.norm1         |    0.768K              |    82.56K   |\n",
      "|     blocks.10.norm1.weight |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.10.attn          |    0.591M              |    26.782M  |\n",
      "|     blocks.10.attn.qkv     |     0.444M             |     19.022M |\n",
      "|     blocks.10.attn.proj    |     0.148M             |     6.341M  |\n",
      "|    blocks.10.norm2         |    0.768K              |    82.56K   |\n",
      "|     blocks.10.norm2.weight |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.10.mlp           |    1.182M              |    50.725M  |\n",
      "|     blocks.10.mlp.fc1      |     0.591M             |     25.362M |\n",
      "|     blocks.10.mlp.fc2      |     0.59M              |     25.362M |\n",
      "|   blocks.11                |   1.774M               |   77.672M   |\n",
      "|    blocks.11.norm1         |    0.768K              |    82.56K   |\n",
      "|     blocks.11.norm1.weight |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (384,)             |             |\n",
      "|    blocks.11.attn          |    0.591M              |    26.782M  |\n",
      "|     blocks.11.attn.qkv     |     0.444M             |     19.022M |\n",
      "|     blocks.11.attn.proj    |     0.148M             |     6.341M  |\n",
      "|    blocks.11.norm2         |    0.768K              |    82.56K   |\n",
      "|     blocks.11.norm2.weight |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (384,)             |             |\n",
      "|    blocks.11.mlp           |    1.182M              |    50.725M  |\n",
      "|     blocks.11.mlp.fc1      |     0.591M             |     25.362M |\n",
      "|     blocks.11.mlp.fc2      |     0.59M              |     25.362M |\n",
      "|  norm                      |  0.768K                |  82.56K     |\n",
      "|   norm.weight              |   (384,)               |             |\n",
      "|   norm.bias                |   (384,)               |             |\n",
      "|  head                      |  0.385M                |  0.384M     |\n",
      "|   head.weight              |   (1000, 384)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 22.05M, #flops: 2.48G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 2.42G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.22G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 80.29M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 52.2M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 17.4M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.14G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.22G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 80.29M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 52.2M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 17.4M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.14G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.22G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 80.36M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 52.2M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 17.4M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.23M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.14G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 69.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.13G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 45.75M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 31.41M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.47M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 83.76M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.13G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 45.75M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 31.41M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.47M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 83.76M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.13G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 45.77M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 31.41M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.47M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 83.76M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 41.88M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 77.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 26.78M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 19.02M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 50.72M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 77.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 26.78M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 19.02M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 50.72M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 77.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 26.78M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 19.02M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 82.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 50.72M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 25.36M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 82.56K\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      ")\n",
      "2478182118\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -S\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 6\n",
    "mlp_ratio = 4.\n",
    "dims = 384\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.6,0.6,0.6]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3f4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops     |\n",
      "|:---------------------------|:-----------------------|:-----------|\n",
      "| model                      | 86.568M                | 17.583G    |\n",
      "|  cls_token                 |  (1, 1, 768)           |            |\n",
      "|  pos_embed                 |  (1, 197, 768)         |            |\n",
      "|  patch_embed.proj          |  0.591M                |  0.116G    |\n",
      "|   patch_embed.proj.weight  |   (768, 3, 16, 16)     |            |\n",
      "|   patch_embed.proj.bias    |   (768,)               |            |\n",
      "|  blocks                    |  85.054M               |  17.466G   |\n",
      "|   blocks.0                 |   7.088M               |   1.455G   |\n",
      "|    blocks.0.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.0.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.0.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.0.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.0.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.0.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.0.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.0.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.0.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.0.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.0.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.0.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.1                 |   7.088M               |   1.455G   |\n",
      "|    blocks.1.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.1.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.1.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.1.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.1.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.1.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.1.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.1.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.1.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.1.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.1.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.1.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.2                 |   7.088M               |   1.456G   |\n",
      "|    blocks.2.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.2.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.2.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.2.attn           |    2.362M              |    0.525G  |\n",
      "|     blocks.2.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.2.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.2.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.2.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.2.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.2.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.2.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.2.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.3                 |   7.088M               |   1.455G   |\n",
      "|    blocks.3.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.3.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.3.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.3.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.3.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.3.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.3.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.3.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.3.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.3.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.3.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.3.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.4                 |   7.088M               |   1.455G   |\n",
      "|    blocks.4.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.4.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.4.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.4.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.4.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.4.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.4.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.4.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.4.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.4.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.4.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.4.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.5                 |   7.088M               |   1.456G   |\n",
      "|    blocks.5.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.5.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.5.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.5.attn           |    2.362M              |    0.525G  |\n",
      "|     blocks.5.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.5.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.5.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.5.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.5.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.5.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.5.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.5.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.6                 |   7.088M               |   1.455G   |\n",
      "|    blocks.6.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.6.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.6.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.6.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.6.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.6.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.6.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.6.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.6.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.6.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.6.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.6.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.7                 |   7.088M               |   1.455G   |\n",
      "|    blocks.7.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.7.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.7.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.7.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.7.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.7.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.7.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.7.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.7.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.7.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.7.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.7.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.8                 |   7.088M               |   1.456G   |\n",
      "|    blocks.8.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.8.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.8.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.8.attn           |    2.362M              |    0.525G  |\n",
      "|     blocks.8.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.8.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.8.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.8.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.8.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.8.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.8.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.8.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.9                 |   7.088M               |   1.455G   |\n",
      "|    blocks.9.norm1          |    1.536K              |    0.756M  |\n",
      "|     blocks.9.norm1.weight  |     (768,)             |            |\n",
      "|     blocks.9.norm1.bias    |     (768,)             |            |\n",
      "|    blocks.9.attn           |    2.362M              |    0.524G  |\n",
      "|     blocks.9.attn.qkv      |     1.772M             |     0.349G |\n",
      "|     blocks.9.attn.proj     |     0.591M             |     0.116G |\n",
      "|    blocks.9.norm2          |    1.536K              |    0.756M  |\n",
      "|     blocks.9.norm2.weight  |     (768,)             |            |\n",
      "|     blocks.9.norm2.bias    |     (768,)             |            |\n",
      "|    blocks.9.mlp            |    4.722M              |    0.93G   |\n",
      "|     blocks.9.mlp.fc1       |     2.362M             |     0.465G |\n",
      "|     blocks.9.mlp.fc2       |     2.36M              |     0.465G |\n",
      "|   blocks.10                |   7.088M               |   1.455G   |\n",
      "|    blocks.10.norm1         |    1.536K              |    0.756M  |\n",
      "|     blocks.10.norm1.weight |     (768,)             |            |\n",
      "|     blocks.10.norm1.bias   |     (768,)             |            |\n",
      "|    blocks.10.attn          |    2.362M              |    0.524G  |\n",
      "|     blocks.10.attn.qkv     |     1.772M             |     0.349G |\n",
      "|     blocks.10.attn.proj    |     0.591M             |     0.116G |\n",
      "|    blocks.10.norm2         |    1.536K              |    0.756M  |\n",
      "|     blocks.10.norm2.weight |     (768,)             |            |\n",
      "|     blocks.10.norm2.bias   |     (768,)             |            |\n",
      "|    blocks.10.mlp           |    4.722M              |    0.93G   |\n",
      "|     blocks.10.mlp.fc1      |     2.362M             |     0.465G |\n",
      "|     blocks.10.mlp.fc2      |     2.36M              |     0.465G |\n",
      "|   blocks.11                |   7.088M               |   1.455G   |\n",
      "|    blocks.11.norm1         |    1.536K              |    0.756M  |\n",
      "|     blocks.11.norm1.weight |     (768,)             |            |\n",
      "|     blocks.11.norm1.bias   |     (768,)             |            |\n",
      "|    blocks.11.attn          |    2.362M              |    0.524G  |\n",
      "|     blocks.11.attn.qkv     |     1.772M             |     0.349G |\n",
      "|     blocks.11.attn.proj    |     0.591M             |     0.116G |\n",
      "|    blocks.11.norm2         |    1.536K              |    0.756M  |\n",
      "|     blocks.11.norm2.weight |     (768,)             |            |\n",
      "|     blocks.11.norm2.bias   |     (768,)             |            |\n",
      "|    blocks.11.mlp           |    4.722M              |    0.93G   |\n",
      "|     blocks.11.mlp.fc1      |     2.362M             |     0.465G |\n",
      "|     blocks.11.mlp.fc2      |     2.36M              |     0.465G |\n",
      "|  norm                      |  1.536K                |  0.756M    |\n",
      "|   norm.weight              |   (768,)               |            |\n",
      "|   norm.bias                |   (768,)               |            |\n",
      "|  head                      |  0.769M                |  0.768M    |\n",
      "|   head.weight              |   (1000, 768)          |            |\n",
      "|   head.bias                |   (1000,)              |            |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 86.57M, #flops: 17.58G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.59M, #flops: 0.12G\n",
      "    (proj): Conv2d(\n",
      "      3, 768, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.59M, #flops: 0.12G\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 85.05M, #flops: 17.47G\n",
      "    (0): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (768,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 1.54K, #flops: 0.76M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=768, out_features=1000, bias=True\n",
      "    #params: 0.77M, #flops: 0.77M\n",
      "  )\n",
      ")\n",
      "17583322359\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -B\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 12\n",
    "mlp_ratio = 4.\n",
    "dims = 768\n",
    "qkv_bias = True\n",
    "retain_rate_list = [1,1,1]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51845a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 86.568M                | 15.02G      |\n",
      "|  cls_token                 |  (1, 1, 768)           |             |\n",
      "|  pos_embed                 |  (1, 197, 768)         |             |\n",
      "|  patch_embed.proj          |  0.591M                |  0.116G     |\n",
      "|   patch_embed.proj.weight  |   (768, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (768,)               |             |\n",
      "|  blocks                    |  85.054M               |  14.903G    |\n",
      "|   blocks.0                 |   7.088M               |   1.455G    |\n",
      "|    blocks.0.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.0.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.0.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.0.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.0.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.0.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.0.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.0.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.1                 |   7.088M               |   1.455G    |\n",
      "|    blocks.1.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.1.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.1.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.1.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.1.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.1.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.1.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.1.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.2                 |   7.088M               |   1.455G    |\n",
      "|    blocks.2.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.2.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.2.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.2.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.2.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.2.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.2.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.2.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.3                 |   7.088M               |   1.302G    |\n",
      "|    blocks.3.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.3.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.3.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.3.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.3.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.3.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.3.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.3.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.4                 |   7.088M               |   1.302G    |\n",
      "|    blocks.4.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.4.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.4.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.4.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.4.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.4.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.4.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.4.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.5                 |   7.088M               |   1.302G    |\n",
      "|    blocks.5.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.5.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.5.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.5.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.5.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.5.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.5.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.5.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.6                 |   7.088M               |   1.165G    |\n",
      "|    blocks.6.norm1          |    1.536K              |    0.611M   |\n",
      "|     blocks.6.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.6.attn           |    2.362M              |    0.414G   |\n",
      "|     blocks.6.attn.qkv      |     1.772M             |     0.281G  |\n",
      "|     blocks.6.attn.proj     |     0.591M             |     93.782M |\n",
      "|    blocks.6.norm2          |    1.536K              |    0.611M   |\n",
      "|     blocks.6.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.6.mlp            |    4.722M              |    0.75G    |\n",
      "|     blocks.6.mlp.fc1       |     2.362M             |     0.375G  |\n",
      "|     blocks.6.mlp.fc2       |     2.36M              |     0.375G  |\n",
      "|   blocks.7                 |   7.088M               |   1.165G    |\n",
      "|    blocks.7.norm1          |    1.536K              |    0.611M   |\n",
      "|     blocks.7.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.7.attn           |    2.362M              |    0.414G   |\n",
      "|     blocks.7.attn.qkv      |     1.772M             |     0.281G  |\n",
      "|     blocks.7.attn.proj     |     0.591M             |     93.782M |\n",
      "|    blocks.7.norm2          |    1.536K              |    0.611M   |\n",
      "|     blocks.7.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.7.mlp            |    4.722M              |    0.75G    |\n",
      "|     blocks.7.mlp.fc1       |     2.362M             |     0.375G  |\n",
      "|     blocks.7.mlp.fc2       |     2.36M              |     0.375G  |\n",
      "|   blocks.8                 |   7.088M               |   1.165G    |\n",
      "|    blocks.8.norm1          |    1.536K              |    0.611M   |\n",
      "|     blocks.8.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.8.attn           |    2.362M              |    0.414G   |\n",
      "|     blocks.8.attn.qkv      |     1.772M             |     0.281G  |\n",
      "|     blocks.8.attn.proj     |     0.591M             |     93.782M |\n",
      "|    blocks.8.norm2          |    1.536K              |    0.611M   |\n",
      "|     blocks.8.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.8.mlp            |    4.722M              |    0.75G    |\n",
      "|     blocks.8.mlp.fc1       |     2.362M             |     0.375G  |\n",
      "|     blocks.8.mlp.fc2       |     2.36M              |     0.375G  |\n",
      "|   blocks.9                 |   7.088M               |   1.045G    |\n",
      "|    blocks.9.norm1          |    1.536K              |    0.549M   |\n",
      "|     blocks.9.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.9.attn           |    2.362M              |    0.369G   |\n",
      "|     blocks.9.attn.qkv      |     1.772M             |     0.253G  |\n",
      "|     blocks.9.attn.proj     |     0.591M             |     84.345M |\n",
      "|    blocks.9.norm2          |    1.536K              |    0.549M   |\n",
      "|     blocks.9.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.9.mlp            |    4.722M              |    0.675G   |\n",
      "|     blocks.9.mlp.fc1       |     2.362M             |     0.337G  |\n",
      "|     blocks.9.mlp.fc2       |     2.36M              |     0.337G  |\n",
      "|   blocks.10                |   7.088M               |   1.045G    |\n",
      "|    blocks.10.norm1         |    1.536K              |    0.549M   |\n",
      "|     blocks.10.norm1.weight |     (768,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.10.attn          |    2.362M              |    0.369G   |\n",
      "|     blocks.10.attn.qkv     |     1.772M             |     0.253G  |\n",
      "|     blocks.10.attn.proj    |     0.591M             |     84.345M |\n",
      "|    blocks.10.norm2         |    1.536K              |    0.549M   |\n",
      "|     blocks.10.norm2.weight |     (768,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.10.mlp           |    4.722M              |    0.675G   |\n",
      "|     blocks.10.mlp.fc1      |     2.362M             |     0.337G  |\n",
      "|     blocks.10.mlp.fc2      |     2.36M              |     0.337G  |\n",
      "|   blocks.11                |   7.088M               |   1.045G    |\n",
      "|    blocks.11.norm1         |    1.536K              |    0.549M   |\n",
      "|     blocks.11.norm1.weight |     (768,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.11.attn          |    2.362M              |    0.369G   |\n",
      "|     blocks.11.attn.qkv     |     1.772M             |     0.253G  |\n",
      "|     blocks.11.attn.proj    |     0.591M             |     84.345M |\n",
      "|    blocks.11.norm2         |    1.536K              |    0.549M   |\n",
      "|     blocks.11.norm2.weight |     (768,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.11.mlp           |    4.722M              |    0.675G   |\n",
      "|     blocks.11.mlp.fc1      |     2.362M             |     0.337G  |\n",
      "|     blocks.11.mlp.fc2      |     2.36M              |     0.337G  |\n",
      "|  norm                      |  1.536K                |  0.549M     |\n",
      "|   norm.weight              |   (768,)               |             |\n",
      "|   norm.bias                |   (768,)               |             |\n",
      "|  head                      |  0.769M                |  0.768M     |\n",
      "|   head.weight              |   (1000, 768)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 86.57M, #flops: 15.02G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.59M, #flops: 0.12G\n",
      "    (proj): Conv2d(\n",
      "      3, 768, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.59M, #flops: 0.12G\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 85.05M, #flops: 14.9G\n",
      "    (0): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 7.09M, #flops: 1.17G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.41G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.28G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 93.78M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.75G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 7.09M, #flops: 1.17G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.41G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.28G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 93.78M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.75G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 7.09M, #flops: 1.17G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.41G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.28G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 93.78M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.61M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.75G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.38G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 7.09M, #flops: 1.04G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.37G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 84.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 7.09M, #flops: 1.04G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.37G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 84.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 7.09M, #flops: 1.04G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.37G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 84.34M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.55M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.34G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (768,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 1.54K, #flops: 0.55M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=768, out_features=1000, bias=True\n",
      "    #params: 0.77M, #flops: 0.77M\n",
      "  )\n",
      ")\n",
      "15020374272\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -B\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 12\n",
    "mlp_ratio = 4.\n",
    "dims = 768\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.9,0.9]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "769672e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 86.568M                | 13.628G     |\n",
      "|  cls_token                 |  (1, 1, 768)           |             |\n",
      "|  pos_embed                 |  (1, 197, 768)         |             |\n",
      "|  patch_embed.proj          |  0.591M                |  0.116G     |\n",
      "|   patch_embed.proj.weight  |   (768, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (768,)               |             |\n",
      "|  blocks                    |  85.054M               |  13.512G    |\n",
      "|   blocks.0                 |   7.088M               |   1.455G    |\n",
      "|    blocks.0.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.0.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.0.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.0.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.0.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.0.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.0.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.0.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.0.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.1                 |   7.088M               |   1.455G    |\n",
      "|    blocks.1.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.1.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.1.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.1.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.1.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.1.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.1.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.1.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.1.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.2                 |   7.088M               |   1.455G    |\n",
      "|    blocks.2.norm1          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.2.attn           |    2.362M              |    0.524G   |\n",
      "|     blocks.2.attn.qkv      |     1.772M             |     0.349G  |\n",
      "|     blocks.2.attn.proj     |     0.591M             |     0.116G  |\n",
      "|    blocks.2.norm2          |    1.536K              |    0.756M   |\n",
      "|     blocks.2.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.2.mlp            |    4.722M              |    0.93G    |\n",
      "|     blocks.2.mlp.fc1       |     2.362M             |     0.465G  |\n",
      "|     blocks.2.mlp.fc2       |     2.36M              |     0.465G  |\n",
      "|   blocks.3                 |   7.088M               |   1.302G    |\n",
      "|    blocks.3.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.3.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.3.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.3.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.3.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.3.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.3.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.3.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.3.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.4                 |   7.088M               |   1.302G    |\n",
      "|    blocks.4.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.4.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.4.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.4.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.4.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.4.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.4.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.4.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.4.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.5                 |   7.088M               |   1.302G    |\n",
      "|    blocks.5.norm1          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.5.attn           |    2.362M              |    0.466G   |\n",
      "|     blocks.5.attn.qkv      |     1.772M             |     0.313G  |\n",
      "|     blocks.5.attn.proj     |     0.591M             |     0.104G  |\n",
      "|    blocks.5.norm2          |    1.536K              |    0.68M    |\n",
      "|     blocks.5.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.5.mlp            |    4.722M              |    0.835G   |\n",
      "|     blocks.5.mlp.fc1       |     2.362M             |     0.418G  |\n",
      "|     blocks.5.mlp.fc2       |     2.36M              |     0.418G  |\n",
      "|   blocks.6                 |   7.088M               |   1.03G     |\n",
      "|    blocks.6.norm1          |    1.536K              |    0.541M   |\n",
      "|     blocks.6.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.6.attn           |    2.362M              |    0.363G   |\n",
      "|     blocks.6.attn.qkv      |     1.772M             |     0.249G  |\n",
      "|     blocks.6.attn.proj     |     0.591M             |     83.165M |\n",
      "|    blocks.6.norm2          |    1.536K              |    0.541M   |\n",
      "|     blocks.6.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.6.mlp            |    4.722M              |    0.665G   |\n",
      "|     blocks.6.mlp.fc1       |     2.362M             |     0.333G  |\n",
      "|     blocks.6.mlp.fc2       |     2.36M              |     0.333G  |\n",
      "|   blocks.7                 |   7.088M               |   1.03G     |\n",
      "|    blocks.7.norm1          |    1.536K              |    0.541M   |\n",
      "|     blocks.7.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.7.attn           |    2.362M              |    0.363G   |\n",
      "|     blocks.7.attn.qkv      |     1.772M             |     0.249G  |\n",
      "|     blocks.7.attn.proj     |     0.591M             |     83.165M |\n",
      "|    blocks.7.norm2          |    1.536K              |    0.541M   |\n",
      "|     blocks.7.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.7.mlp            |    4.722M              |    0.665G   |\n",
      "|     blocks.7.mlp.fc1       |     2.362M             |     0.333G  |\n",
      "|     blocks.7.mlp.fc2       |     2.36M              |     0.333G  |\n",
      "|   blocks.8                 |   7.088M               |   1.03G     |\n",
      "|    blocks.8.norm1          |    1.536K              |    0.541M   |\n",
      "|     blocks.8.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.8.attn           |    2.362M              |    0.363G   |\n",
      "|     blocks.8.attn.qkv      |     1.772M             |     0.249G  |\n",
      "|     blocks.8.attn.proj     |     0.591M             |     83.165M |\n",
      "|    blocks.8.norm2          |    1.536K              |    0.541M   |\n",
      "|     blocks.8.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.8.mlp            |    4.722M              |    0.665G   |\n",
      "|     blocks.8.mlp.fc1       |     2.362M             |     0.333G  |\n",
      "|     blocks.8.mlp.fc2       |     2.36M              |     0.333G  |\n",
      "|   blocks.9                 |   7.088M               |   0.717G    |\n",
      "|    blocks.9.norm1          |    1.536K              |    0.38M    |\n",
      "|     blocks.9.norm1.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (768,)             |             |\n",
      "|    blocks.9.attn           |    2.362M              |    0.249G   |\n",
      "|     blocks.9.attn.qkv      |     1.772M             |     0.175G  |\n",
      "|     blocks.9.attn.proj     |     0.591M             |     58.393M |\n",
      "|    blocks.9.norm2          |    1.536K              |    0.38M    |\n",
      "|     blocks.9.norm2.weight  |     (768,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (768,)             |             |\n",
      "|    blocks.9.mlp            |    4.722M              |    0.467G   |\n",
      "|     blocks.9.mlp.fc1       |     2.362M             |     0.234G  |\n",
      "|     blocks.9.mlp.fc2       |     2.36M              |     0.234G  |\n",
      "|   blocks.10                |   7.088M               |   0.717G    |\n",
      "|    blocks.10.norm1         |    1.536K              |    0.38M    |\n",
      "|     blocks.10.norm1.weight |     (768,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.10.attn          |    2.362M              |    0.249G   |\n",
      "|     blocks.10.attn.qkv     |     1.772M             |     0.175G  |\n",
      "|     blocks.10.attn.proj    |     0.591M             |     58.393M |\n",
      "|    blocks.10.norm2         |    1.536K              |    0.38M    |\n",
      "|     blocks.10.norm2.weight |     (768,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.10.mlp           |    4.722M              |    0.467G   |\n",
      "|     blocks.10.mlp.fc1      |     2.362M             |     0.234G  |\n",
      "|     blocks.10.mlp.fc2      |     2.36M              |     0.234G  |\n",
      "|   blocks.11                |   7.088M               |   0.717G    |\n",
      "|    blocks.11.norm1         |    1.536K              |    0.38M    |\n",
      "|     blocks.11.norm1.weight |     (768,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (768,)             |             |\n",
      "|    blocks.11.attn          |    2.362M              |    0.249G   |\n",
      "|     blocks.11.attn.qkv     |     1.772M             |     0.175G  |\n",
      "|     blocks.11.attn.proj    |     0.591M             |     58.393M |\n",
      "|    blocks.11.norm2         |    1.536K              |    0.38M    |\n",
      "|     blocks.11.norm2.weight |     (768,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (768,)             |             |\n",
      "|    blocks.11.mlp           |    4.722M              |    0.467G   |\n",
      "|     blocks.11.mlp.fc1      |     2.362M             |     0.234G  |\n",
      "|     blocks.11.mlp.fc2      |     2.36M              |     0.234G  |\n",
      "|  norm                      |  1.536K                |  0.38M      |\n",
      "|   norm.weight              |   (768,)               |             |\n",
      "|   norm.bias                |   (768,)               |             |\n",
      "|  head                      |  0.769M                |  0.768M     |\n",
      "|   head.weight              |   (1000, 768)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 86.57M, #flops: 13.63G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.59M, #flops: 0.12G\n",
      "    (proj): Conv2d(\n",
      "      3, 768, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.59M, #flops: 0.12G\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 85.05M, #flops: 13.51G\n",
      "    (0): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 7.09M, #flops: 1.46G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.52G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.35G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.76M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.93G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.46G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 7.09M, #flops: 1.3G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.47G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.31G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 0.1G\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.68M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.84G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.42G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 7.09M, #flops: 1.03G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.36G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 7.09M, #flops: 1.03G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.36G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 7.09M, #flops: 1.03G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.36G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.25G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 83.17M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.54M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.67G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.33G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 7.09M, #flops: 0.72G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.25G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.18G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.47G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 7.09M, #flops: 0.72G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.25G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.18G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.47G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 7.09M, #flops: 0.72G\n",
      "      (norm1): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 2.36M, #flops: 0.25G\n",
      "        (qkv): Linear(\n",
      "          in_features=768, out_features=2304, bias=True\n",
      "          #params: 1.77M, #flops: 0.18G\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=768, out_features=768, bias=True\n",
      "          #params: 0.59M, #flops: 58.39M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (768,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 1.54K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 4.72M, #flops: 0.47G\n",
      "        (fc1): Linear(\n",
      "          in_features=768, out_features=3072, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=3072, out_features=768, bias=True\n",
      "          #params: 2.36M, #flops: 0.23G\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (768,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 1.54K, #flops: 0.38M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=768, out_features=1000, bias=True\n",
      "    #params: 0.77M, #flops: 0.77M\n",
      "  )\n",
      ")\n",
      "13628340480\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -B\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 12\n",
    "mlp_ratio = 4.\n",
    "dims = 768\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.8,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33d1708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Test: [  0/391]\tTime  2.257 ( 2.257)\tLoss 2.9628e-01 (2.9628e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.123 ( 0.235)\tLoss 4.9533e-01 (5.8144e-01)\tAcc@1  87.50 ( 87.31)\tAcc@5  98.44 ( 97.69)\n",
      "Test: [ 40/391]\tTime  0.132 ( 0.184)\tLoss 5.4839e-01 (6.2589e-01)\tAcc@1  92.97 ( 86.34)\tAcc@5  94.53 ( 97.31)\n",
      "Test: [ 60/391]\tTime  0.130 ( 0.167)\tLoss 8.0301e-01 (5.8726e-01)\tAcc@1  84.38 ( 87.72)\tAcc@5  95.31 ( 97.59)\n",
      "Test: [ 80/391]\tTime  0.130 ( 0.158)\tLoss 6.2037e-01 (6.2592e-01)\tAcc@1  85.94 ( 86.60)\tAcc@5  97.66 ( 97.47)\n",
      "Test: [100/391]\tTime  0.143 ( 0.153)\tLoss 5.7356e-01 (6.2968e-01)\tAcc@1  90.62 ( 86.47)\tAcc@5  97.66 ( 97.50)\n",
      "Test: [120/391]\tTime  0.126 ( 0.149)\tLoss 5.2043e-01 (6.3914e-01)\tAcc@1  90.62 ( 86.27)\tAcc@5  99.22 ( 97.50)\n",
      "Test: [140/391]\tTime  0.125 ( 0.147)\tLoss 9.7657e-01 (6.2979e-01)\tAcc@1  75.78 ( 86.34)\tAcc@5  98.44 ( 97.61)\n",
      "Test: [160/391]\tTime  0.130 ( 0.145)\tLoss 7.9671e-01 (6.4034e-01)\tAcc@1  81.25 ( 86.18)\tAcc@5  95.31 ( 97.45)\n",
      "Test: [180/391]\tTime  0.134 ( 0.143)\tLoss 1.4904e+00 (6.7806e-01)\tAcc@1  64.06 ( 85.21)\tAcc@5  92.19 ( 97.07)\n",
      "Test: [200/391]\tTime  0.132 ( 0.142)\tLoss 7.8700e-01 (7.1767e-01)\tAcc@1  79.69 ( 84.22)\tAcc@5  94.53 ( 96.69)\n",
      "Test: [220/391]\tTime  0.126 ( 0.141)\tLoss 3.5606e-01 (7.3124e-01)\tAcc@1  89.84 ( 83.85)\tAcc@5 100.00 ( 96.54)\n",
      "Test: [240/391]\tTime  0.136 ( 0.140)\tLoss 8.5000e-01 (7.3930e-01)\tAcc@1  89.06 ( 83.81)\tAcc@5  90.62 ( 96.37)\n",
      "Test: [260/391]\tTime  0.131 ( 0.140)\tLoss 6.5552e-01 (7.6766e-01)\tAcc@1  85.16 ( 82.98)\tAcc@5  98.44 ( 96.11)\n",
      "Test: [280/391]\tTime  0.126 ( 0.139)\tLoss 9.4052e-01 (7.7667e-01)\tAcc@1  78.12 ( 82.80)\tAcc@5  95.31 ( 96.00)\n",
      "Test: [300/391]\tTime  0.139 ( 0.139)\tLoss 7.5176e-01 (7.9033e-01)\tAcc@1  87.50 ( 82.54)\tAcc@5  94.53 ( 95.83)\n",
      "Test: [320/391]\tTime  0.131 ( 0.138)\tLoss 5.2500e-01 (8.0054e-01)\tAcc@1  89.84 ( 82.35)\tAcc@5  97.66 ( 95.70)\n",
      "Test: [340/391]\tTime  0.126 ( 0.138)\tLoss 6.7317e-01 (8.1215e-01)\tAcc@1  84.38 ( 82.02)\tAcc@5  98.44 ( 95.60)\n",
      "Test: [360/391]\tTime  0.139 ( 0.138)\tLoss 1.2165e+00 (8.2253e-01)\tAcc@1  76.56 ( 81.74)\tAcc@5  93.75 ( 95.54)\n",
      "Test: [380/391]\tTime  0.127 ( 0.137)\tLoss 6.9673e-01 (8.2176e-01)\tAcc@1  83.59 ( 81.76)\tAcc@5  99.22 ( 95.56)\n",
      " * Acc@1 81.794 Acc@5 95.592\n"
     ]
    }
   ],
   "source": [
    "# no pruning\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 2 --retain_rate_list 1 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3af57138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Test: [  0/391]\tTime  2.094 ( 2.094)\tLoss 2.9283e-01 (2.9283e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 20/391]\tTime  0.201 ( 0.216)\tLoss 5.0721e-01 (5.9673e-01)\tAcc@1  89.06 ( 87.28)\tAcc@5  98.44 ( 97.81)\n",
      "Test: [ 40/391]\tTime  0.173 ( 0.172)\tLoss 5.9670e-01 (6.4031e-01)\tAcc@1  91.41 ( 86.20)\tAcc@5  93.75 ( 97.37)\n",
      "Test: [ 60/391]\tTime  0.101 ( 0.152)\tLoss 7.8141e-01 (5.9826e-01)\tAcc@1  83.59 ( 87.69)\tAcc@5  96.09 ( 97.61)\n",
      "Test: [ 80/391]\tTime  0.108 ( 0.141)\tLoss 6.6434e-01 (6.3982e-01)\tAcc@1  84.38 ( 86.44)\tAcc@5  98.44 ( 97.48)\n",
      "Test: [100/391]\tTime  0.100 ( 0.134)\tLoss 6.3170e-01 (6.4460e-01)\tAcc@1  89.84 ( 86.25)\tAcc@5  96.88 ( 97.52)\n",
      "Test: [120/391]\tTime  0.114 ( 0.134)\tLoss 5.2331e-01 (6.5416e-01)\tAcc@1  89.84 ( 85.95)\tAcc@5  99.22 ( 97.56)\n",
      "Test: [140/391]\tTime  0.109 ( 0.132)\tLoss 9.4773e-01 (6.4558e-01)\tAcc@1  75.78 ( 86.00)\tAcc@5  99.22 ( 97.66)\n",
      "Test: [160/391]\tTime  0.252 ( 0.131)\tLoss 8.6179e-01 (6.5758e-01)\tAcc@1  82.03 ( 85.85)\tAcc@5  93.75 ( 97.47)\n",
      "Test: [180/391]\tTime  0.120 ( 0.129)\tLoss 1.5193e+00 (6.9847e-01)\tAcc@1  63.28 ( 84.83)\tAcc@5  91.41 ( 97.05)\n",
      "Test: [200/391]\tTime  0.102 ( 0.127)\tLoss 8.2187e-01 (7.4016e-01)\tAcc@1  78.12 ( 83.78)\tAcc@5  95.31 ( 96.58)\n",
      "Test: [220/391]\tTime  0.102 ( 0.125)\tLoss 4.0536e-01 (7.5636e-01)\tAcc@1  91.41 ( 83.41)\tAcc@5  99.22 ( 96.44)\n",
      "Test: [240/391]\tTime  0.102 ( 0.124)\tLoss 8.4871e-01 (7.6530e-01)\tAcc@1  87.50 ( 83.31)\tAcc@5  92.97 ( 96.27)\n",
      "Test: [260/391]\tTime  0.102 ( 0.123)\tLoss 7.4227e-01 (7.9608e-01)\tAcc@1  83.59 ( 82.46)\tAcc@5  98.44 ( 95.97)\n",
      "Test: [280/391]\tTime  0.101 ( 0.123)\tLoss 9.6135e-01 (8.0625e-01)\tAcc@1  79.69 ( 82.23)\tAcc@5  94.53 ( 95.80)\n",
      "Test: [300/391]\tTime  0.108 ( 0.122)\tLoss 7.2787e-01 (8.2059e-01)\tAcc@1  89.06 ( 81.99)\tAcc@5  96.09 ( 95.61)\n",
      "Test: [320/391]\tTime  0.101 ( 0.123)\tLoss 5.3282e-01 (8.3354e-01)\tAcc@1  89.06 ( 81.74)\tAcc@5  97.66 ( 95.42)\n",
      "Test: [340/391]\tTime  0.110 ( 0.122)\tLoss 6.5285e-01 (8.4634e-01)\tAcc@1  83.59 ( 81.40)\tAcc@5  98.44 ( 95.30)\n",
      "Test: [360/391]\tTime  0.109 ( 0.121)\tLoss 1.2307e+00 (8.5773e-01)\tAcc@1  73.44 ( 81.11)\tAcc@5  94.53 ( 95.24)\n",
      "Test: [380/391]\tTime  0.102 ( 0.122)\tLoss 7.3511e-01 (8.5710e-01)\tAcc@1  82.03 ( 81.12)\tAcc@5  99.22 ( 95.27)\n",
      " * Acc@1 81.156 Acc@5 95.304\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "51276321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Test: [  0/391]\tTime  2.319 ( 2.319)\tLoss 2.9164e-01 (2.9164e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.293 ( 0.235)\tLoss 4.7100e-01 (5.9716e-01)\tAcc@1  89.84 ( 86.98)\tAcc@5  99.22 ( 97.73)\n",
      "Test: [ 40/391]\tTime  0.184 ( 0.184)\tLoss 5.9715e-01 (6.3801e-01)\tAcc@1  90.62 ( 86.11)\tAcc@5  93.75 ( 97.35)\n",
      "Test: [ 60/391]\tTime  0.131 ( 0.161)\tLoss 8.0250e-01 (5.9460e-01)\tAcc@1  82.81 ( 87.60)\tAcc@5  95.31 ( 97.57)\n",
      "Test: [ 80/391]\tTime  0.189 ( 0.150)\tLoss 6.4538e-01 (6.3503e-01)\tAcc@1  83.59 ( 86.38)\tAcc@5  98.44 ( 97.42)\n",
      "Test: [100/391]\tTime  0.113 ( 0.143)\tLoss 6.3054e-01 (6.3956e-01)\tAcc@1  89.06 ( 86.20)\tAcc@5  97.66 ( 97.44)\n",
      "Test: [120/391]\tTime  0.203 ( 0.141)\tLoss 5.1793e-01 (6.4852e-01)\tAcc@1  89.84 ( 85.94)\tAcc@5  99.22 ( 97.47)\n",
      "Test: [140/391]\tTime  0.247 ( 0.137)\tLoss 9.7857e-01 (6.3828e-01)\tAcc@1  75.00 ( 86.04)\tAcc@5  98.44 ( 97.57)\n",
      "Test: [160/391]\tTime  0.306 ( 0.139)\tLoss 8.0307e-01 (6.4921e-01)\tAcc@1  83.59 ( 85.92)\tAcc@5  93.75 ( 97.39)\n",
      "Test: [180/391]\tTime  0.173 ( 0.136)\tLoss 1.4769e+00 (6.8961e-01)\tAcc@1  64.06 ( 84.94)\tAcc@5  91.41 ( 97.00)\n",
      "Test: [200/391]\tTime  0.103 ( 0.133)\tLoss 7.9334e-01 (7.3319e-01)\tAcc@1  79.69 ( 83.89)\tAcc@5  94.53 ( 96.54)\n",
      "Test: [220/391]\tTime  0.143 ( 0.131)\tLoss 4.0466e-01 (7.5035e-01)\tAcc@1  92.19 ( 83.46)\tAcc@5  99.22 ( 96.40)\n",
      "Test: [240/391]\tTime  0.122 ( 0.130)\tLoss 8.2643e-01 (7.5986e-01)\tAcc@1  88.28 ( 83.34)\tAcc@5  92.97 ( 96.23)\n",
      "Test: [260/391]\tTime  0.219 ( 0.129)\tLoss 7.0310e-01 (7.9045e-01)\tAcc@1  79.69 ( 82.43)\tAcc@5  97.66 ( 95.95)\n",
      "Test: [280/391]\tTime  0.109 ( 0.129)\tLoss 9.8786e-01 (7.9988e-01)\tAcc@1  78.12 ( 82.25)\tAcc@5  94.53 ( 95.84)\n",
      "Test: [300/391]\tTime  0.109 ( 0.128)\tLoss 7.7797e-01 (8.1497e-01)\tAcc@1  85.94 ( 81.97)\tAcc@5  96.88 ( 95.62)\n",
      "Test: [320/391]\tTime  0.107 ( 0.127)\tLoss 5.4718e-01 (8.2776e-01)\tAcc@1  89.84 ( 81.72)\tAcc@5  97.66 ( 95.46)\n",
      "Test: [340/391]\tTime  0.103 ( 0.126)\tLoss 6.9827e-01 (8.4041e-01)\tAcc@1  82.81 ( 81.35)\tAcc@5  97.66 ( 95.36)\n",
      "Test: [360/391]\tTime  0.109 ( 0.126)\tLoss 1.3033e+00 (8.5243e-01)\tAcc@1  71.09 ( 81.05)\tAcc@5  94.53 ( 95.29)\n",
      "Test: [380/391]\tTime  0.108 ( 0.126)\tLoss 7.2670e-01 (8.5268e-01)\tAcc@1  82.81 ( 81.05)\tAcc@5  99.22 ( 95.31)\n",
      " * Acc@1 81.090 Acc@5 95.340\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8da25ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Test: [  0/391]\tTime  2.233 ( 2.233)\tLoss 2.9018e-01 (2.9018e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.22 ( 99.22)\n",
      "Test: [ 20/391]\tTime  0.193 ( 0.217)\tLoss 4.7988e-01 (5.9896e-01)\tAcc@1  88.28 ( 86.98)\tAcc@5  99.22 ( 97.81)\n",
      "Test: [ 40/391]\tTime  0.199 ( 0.170)\tLoss 6.2485e-01 (6.4062e-01)\tAcc@1  90.62 ( 86.13)\tAcc@5  94.53 ( 97.43)\n",
      "Test: [ 60/391]\tTime  0.115 ( 0.151)\tLoss 7.8889e-01 (6.0023e-01)\tAcc@1  84.38 ( 87.60)\tAcc@5  95.31 ( 97.62)\n",
      "Test: [ 80/391]\tTime  0.134 ( 0.141)\tLoss 6.7546e-01 (6.4210e-01)\tAcc@1  82.81 ( 86.23)\tAcc@5  98.44 ( 97.49)\n",
      "Test: [100/391]\tTime  0.126 ( 0.135)\tLoss 6.5787e-01 (6.4768e-01)\tAcc@1  89.84 ( 86.05)\tAcc@5  96.88 ( 97.51)\n",
      "Test: [120/391]\tTime  0.194 ( 0.133)\tLoss 5.3649e-01 (6.5720e-01)\tAcc@1  89.84 ( 85.87)\tAcc@5  99.22 ( 97.53)\n",
      "Test: [140/391]\tTime  0.109 ( 0.131)\tLoss 1.0023e+00 (6.4722e-01)\tAcc@1  75.78 ( 85.98)\tAcc@5  98.44 ( 97.68)\n",
      "Test: [160/391]\tTime  0.281 ( 0.132)\tLoss 8.2679e-01 (6.5788e-01)\tAcc@1  84.38 ( 85.87)\tAcc@5  93.75 ( 97.50)\n",
      "Test: [180/391]\tTime  0.126 ( 0.130)\tLoss 1.5252e+00 (6.9832e-01)\tAcc@1  66.41 ( 84.93)\tAcc@5  91.41 ( 97.07)\n",
      "Test: [200/391]\tTime  0.106 ( 0.127)\tLoss 8.1143e-01 (7.4105e-01)\tAcc@1  80.47 ( 83.85)\tAcc@5  94.53 ( 96.58)\n",
      "Test: [220/391]\tTime  0.109 ( 0.126)\tLoss 4.1932e-01 (7.5714e-01)\tAcc@1  91.41 ( 83.44)\tAcc@5  99.22 ( 96.42)\n",
      "Test: [240/391]\tTime  0.102 ( 0.126)\tLoss 8.6748e-01 (7.6668e-01)\tAcc@1  86.72 ( 83.34)\tAcc@5  92.19 ( 96.27)\n",
      "Test: [260/391]\tTime  0.102 ( 0.124)\tLoss 7.1207e-01 (7.9723e-01)\tAcc@1  82.81 ( 82.44)\tAcc@5  97.66 ( 95.97)\n",
      "Test: [280/391]\tTime  0.104 ( 0.124)\tLoss 9.6348e-01 (8.0689e-01)\tAcc@1  78.12 ( 82.26)\tAcc@5  96.88 ( 95.84)\n",
      "Test: [300/391]\tTime  0.102 ( 0.123)\tLoss 7.3550e-01 (8.2146e-01)\tAcc@1  88.28 ( 82.03)\tAcc@5  96.88 ( 95.65)\n",
      "Test: [320/391]\tTime  0.102 ( 0.122)\tLoss 5.1709e-01 (8.3356e-01)\tAcc@1  89.84 ( 81.80)\tAcc@5  97.66 ( 95.49)\n",
      "Test: [340/391]\tTime  0.112 ( 0.122)\tLoss 6.5975e-01 (8.4584e-01)\tAcc@1  84.38 ( 81.45)\tAcc@5  98.44 ( 95.38)\n",
      "Test: [360/391]\tTime  0.103 ( 0.122)\tLoss 1.2967e+00 (8.5754e-01)\tAcc@1  71.09 ( 81.13)\tAcc@5  92.97 ( 95.31)\n",
      "Test: [380/391]\tTime  0.103 ( 0.122)\tLoss 6.9899e-01 (8.5752e-01)\tAcc@1  82.81 ( 81.11)\tAcc@5  99.22 ( 95.33)\n",
      " * Acc@1 81.158 Acc@5 95.358\n"
     ]
    }
   ],
   "source": [
    "# average after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b320a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/notebooks/../zero_shot_pruning_eval.py\", line 259, in <module>\n",
      "    main(args)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/notebooks/../zero_shot_pruning_eval.py\", line 157, in main\n",
      "    validate(data_loader_val, model, criterion)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/notebooks/../zero_shot_pruning_eval.py\", line 234, in validate\n",
      "    output = model(images)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/tokenrank_vit.py\", line 488, in forward\n",
      "    x, next_mask, importance = block(x, token_mask)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/tokenrank_vit.py\", line 358, in forward\n",
      "    x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/hw4948/.local/lib/python3.9/site-packages/timm/models/layers/mlp.py\", line 27, in forward\n",
      "    x = self.act(x)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/activation.py\", line 652, in forward\n",
      "    return F.gelu(input)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/functional.py\", line 1556, in gelu\n",
      "    return torch._C._nn.gelu(input)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 296.00 MiB (GPU 0; 39.41 GiB total capacity; 995.90 MiB already allocated; 234.56 MiB free; 1.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7705aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_base_patch16_224-b5f2ef4d.pth\n",
      "number of params: 86567656\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/notebooks/../zero_shot_pruning_eval.py\", line 259, in <module>\n",
      "    main(args)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/notebooks/../zero_shot_pruning_eval.py\", line 157, in main\n",
      "    validate(data_loader_val, model, criterion)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/notebooks/../zero_shot_pruning_eval.py\", line 234, in validate\n",
      "    output = model(images)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/tokenrank_vit.py\", line 488, in forward\n",
      "    x, next_mask, importance = block(x, token_mask)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/tokenrank_vit.py\", line 353, in forward\n",
      "    y, next_mask, importance= self.attn(self.norm1(x), token_mask)\n",
      "  File \"/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/tokenrank_vit.py\", line 294, in forward\n",
      "    importance = self.get_importance(attn, None, self.tau_imp)\n",
      "  File \"/scratch/gpfs/hw4948/tokenrank_pruning/tokenrank_vit.py\", line 315, in get_importance\n",
      "    attn = attn_mean.exp()\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 39.41 GiB total capacity; 1.20 GiB already allocated; 200.56 MiB free; 1.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# after emphasizing\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_base_3' --prune_list 0 3 6 9 --retain_rate_list 0.6 0.42 0.5 0.5 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9e5ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 5.717M                 | 1.259G      |\n",
      "|  cls_token                 |  (1, 1, 192)           |             |\n",
      "|  pos_embed                 |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj          |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight  |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (192,)               |             |\n",
      "|  blocks                    |  5.338M                |  1.23G      |\n",
      "|   blocks.0                 |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.0.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.0.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.1                 |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.1.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.1.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.2                 |   0.445M               |   0.103G    |\n",
      "|    blocks.2.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.2.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.2.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.2.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.3                 |   0.445M               |   0.102G    |\n",
      "|    blocks.3.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.3.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.3.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.3.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.3.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.3.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.3.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.3.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.3.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.3.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.4                 |   0.445M               |   0.102G    |\n",
      "|    blocks.4.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.4.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.4.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.4.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.4.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.4.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.4.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.4.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.4.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.4.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.5                 |   0.445M               |   0.103G    |\n",
      "|    blocks.5.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.5.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.5.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.5.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.5.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.5.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.5.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.5.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.5.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.5.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.6                 |   0.445M               |   0.102G    |\n",
      "|    blocks.6.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.6.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.6.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.6.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.6.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.6.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.6.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.6.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.6.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.6.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.7                 |   0.445M               |   0.102G    |\n",
      "|    blocks.7.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.7.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.7.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.7.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.7.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.7.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.7.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.7.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.7.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.7.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.8                 |   0.445M               |   0.103G    |\n",
      "|    blocks.8.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.8.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.8.attn           |    0.148M              |    44.146M  |\n",
      "|     blocks.8.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.8.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.8.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.8.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.8.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.8.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.8.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.9                 |   0.445M               |   0.102G    |\n",
      "|    blocks.9.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.9.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.9.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.9.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.9.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.9.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.9.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.9.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.9.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.9.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.10                |   0.445M               |   0.102G    |\n",
      "|    blocks.10.norm1         |    0.384K              |    0.189M   |\n",
      "|     blocks.10.norm1.weight |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.10.attn          |    0.148M              |    43.951M  |\n",
      "|     blocks.10.attn.qkv     |     0.111M             |     21.787M |\n",
      "|     blocks.10.attn.proj    |     37.056K            |     7.262M  |\n",
      "|    blocks.10.norm2         |    0.384K              |    0.189M   |\n",
      "|     blocks.10.norm2.weight |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.10.mlp           |    0.296M              |    58.098M  |\n",
      "|     blocks.10.mlp.fc1      |     0.148M             |     29.049M |\n",
      "|     blocks.10.mlp.fc2      |     0.148M             |     29.049M |\n",
      "|   blocks.11                |   0.445M               |   0.102G    |\n",
      "|    blocks.11.norm1         |    0.384K              |    0.189M   |\n",
      "|     blocks.11.norm1.weight |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.11.attn          |    0.148M              |    43.951M  |\n",
      "|     blocks.11.attn.qkv     |     0.111M             |     21.787M |\n",
      "|     blocks.11.attn.proj    |     37.056K            |     7.262M  |\n",
      "|    blocks.11.norm2         |    0.384K              |    0.189M   |\n",
      "|     blocks.11.norm2.weight |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.11.mlp           |    0.296M              |    58.098M  |\n",
      "|     blocks.11.mlp.fc1      |     0.148M             |     29.049M |\n",
      "|     blocks.11.mlp.fc2      |     0.148M             |     29.049M |\n",
      "|  norm                      |  0.384K                |  0.189M     |\n",
      "|   norm.weight              |   (192,)               |             |\n",
      "|   norm.bias                |   (192,)               |             |\n",
      "|  head                      |  0.193M                |  0.192M     |\n",
      "|   head.weight              |   (1000, 192)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 5.72M, #flops: 1.26G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 1.23G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 44.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 0.19M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      ")\n",
      "1258993335\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -T\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 3\n",
    "mlp_ratio = 4.\n",
    "dims = 192\n",
    "qkv_bias = True\n",
    "retain_rate_list = [1,1,1]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907a34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 5.717M                 | 0.963G      |\n",
      "|  cls_token                 |  (1, 1, 192)           |             |\n",
      "|  pos_embed                 |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj          |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight  |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (192,)               |             |\n",
      "|  blocks                    |  5.338M                |  0.933G     |\n",
      "|   blocks.0                 |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.0.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.0.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.1                 |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.1.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.1.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.2                 |   0.445M               |   0.102G    |\n",
      "|    blocks.2.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.2.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.2.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.2.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.3                 |   0.445M               |   90.669M   |\n",
      "|    blocks.3.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.3.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.3.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.3.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.3.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.3.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.3.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.3.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.4                 |   0.445M               |   90.669M   |\n",
      "|    blocks.4.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.4.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.4.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.4.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.4.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.4.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.4.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.4.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.5                 |   0.445M               |   90.669M   |\n",
      "|    blocks.5.norm1          |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.5.attn           |    0.148M              |    38.13M   |\n",
      "|     blocks.5.attn.qkv      |     0.111M             |     19.575M |\n",
      "|     blocks.5.attn.proj     |     37.056K            |     6.525M  |\n",
      "|    blocks.5.norm2          |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.5.mlp            |    0.296M              |    52.199M  |\n",
      "|     blocks.5.mlp.fc1       |     0.148M             |     26.1M   |\n",
      "|     blocks.5.mlp.fc2       |     0.148M             |     26.1M   |\n",
      "|   blocks.6                 |   0.445M               |   70.279M   |\n",
      "|    blocks.6.norm1          |    0.384K              |    0.135M   |\n",
      "|     blocks.6.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.6.attn           |    0.148M              |    28.426M  |\n",
      "|     blocks.6.attn.qkv      |     0.111M             |     15.593M |\n",
      "|     blocks.6.attn.proj     |     37.056K            |     5.198M  |\n",
      "|    blocks.6.norm2          |    0.384K              |    0.135M   |\n",
      "|     blocks.6.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.6.mlp            |    0.296M              |    41.583M  |\n",
      "|     blocks.6.mlp.fc1       |     0.148M             |     20.791M |\n",
      "|     blocks.6.mlp.fc2       |     0.148M             |     20.791M |\n",
      "|   blocks.7                 |   0.445M               |   70.279M   |\n",
      "|    blocks.7.norm1          |    0.384K              |    0.135M   |\n",
      "|     blocks.7.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.7.attn           |    0.148M              |    28.426M  |\n",
      "|     blocks.7.attn.qkv      |     0.111M             |     15.593M |\n",
      "|     blocks.7.attn.proj     |     37.056K            |     5.198M  |\n",
      "|    blocks.7.norm2          |    0.384K              |    0.135M   |\n",
      "|     blocks.7.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.7.mlp            |    0.296M              |    41.583M  |\n",
      "|     blocks.7.mlp.fc1       |     0.148M             |     20.791M |\n",
      "|     blocks.7.mlp.fc2       |     0.148M             |     20.791M |\n",
      "|   blocks.8                 |   0.445M               |   70.279M   |\n",
      "|    blocks.8.norm1          |    0.384K              |    0.135M   |\n",
      "|     blocks.8.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.8.attn           |    0.148M              |    28.426M  |\n",
      "|     blocks.8.attn.qkv      |     0.111M             |     15.593M |\n",
      "|     blocks.8.attn.proj     |     37.056K            |     5.198M  |\n",
      "|    blocks.8.norm2          |    0.384K              |    0.135M   |\n",
      "|     blocks.8.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.8.mlp            |    0.296M              |    41.583M  |\n",
      "|     blocks.8.mlp.fc1       |     0.148M             |     20.791M |\n",
      "|     blocks.8.mlp.fc2       |     0.148M             |     20.791M |\n",
      "|   blocks.9                 |   0.445M               |   47.748M   |\n",
      "|    blocks.9.norm1          |    0.384K              |    95.04K   |\n",
      "|     blocks.9.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.9.attn           |    0.148M              |    18.362M  |\n",
      "|     blocks.9.attn.qkv      |     0.111M             |     10.949M |\n",
      "|     blocks.9.attn.proj     |     37.056K            |     3.65M   |\n",
      "|    blocks.9.norm2          |    0.384K              |    95.04K   |\n",
      "|     blocks.9.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.9.mlp            |    0.296M              |    29.196M  |\n",
      "|     blocks.9.mlp.fc1       |     0.148M             |     14.598M |\n",
      "|     blocks.9.mlp.fc2       |     0.148M             |     14.598M |\n",
      "|   blocks.10                |   0.445M               |   47.748M   |\n",
      "|    blocks.10.norm1         |    0.384K              |    95.04K   |\n",
      "|     blocks.10.norm1.weight |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.10.attn          |    0.148M              |    18.362M  |\n",
      "|     blocks.10.attn.qkv     |     0.111M             |     10.949M |\n",
      "|     blocks.10.attn.proj    |     37.056K            |     3.65M   |\n",
      "|    blocks.10.norm2         |    0.384K              |    95.04K   |\n",
      "|     blocks.10.norm2.weight |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.10.mlp           |    0.296M              |    29.196M  |\n",
      "|     blocks.10.mlp.fc1      |     0.148M             |     14.598M |\n",
      "|     blocks.10.mlp.fc2      |     0.148M             |     14.598M |\n",
      "|   blocks.11                |   0.445M               |   47.748M   |\n",
      "|    blocks.11.norm1         |    0.384K              |    95.04K   |\n",
      "|     blocks.11.norm1.weight |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.11.attn          |    0.148M              |    18.362M  |\n",
      "|     blocks.11.attn.qkv     |     0.111M             |     10.949M |\n",
      "|     blocks.11.attn.proj    |     37.056K            |     3.65M   |\n",
      "|    blocks.11.norm2         |    0.384K              |    95.04K   |\n",
      "|     blocks.11.norm2.weight |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.11.mlp           |    0.296M              |    29.196M  |\n",
      "|     blocks.11.mlp.fc1      |     0.148M             |     14.598M |\n",
      "|     blocks.11.mlp.fc2      |     0.148M             |     14.598M |\n",
      "|  norm                      |  0.384K                |  95.04K     |\n",
      "|   norm.weight              |   (192,)               |             |\n",
      "|   norm.bias                |   (192,)               |             |\n",
      "|  head                      |  0.193M                |  0.192M     |\n",
      "|   head.weight              |   (1000, 192)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 5.72M, #flops: 0.96G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 0.93G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 70.28M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.59M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 41.58M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 70.28M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.59M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 41.58M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 70.28M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.59M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.2M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 41.58M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 20.79M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 47.75M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 95.04K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 18.36M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 10.95M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 3.65M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 95.04K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 29.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 47.75M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 95.04K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 18.36M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 10.95M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 3.65M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 95.04K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 29.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 47.75M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 95.04K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 18.36M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 10.95M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 3.65M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 95.04K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 29.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 14.6M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 95.04K\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      ")\n",
      "962559552\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -T\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8]\n",
    "heads = 3\n",
    "mlp_ratio = 4.\n",
    "dims = 192\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.9,0.8,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2314d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                     | #parameters or shape   | #flops      |\n",
      "|:---------------------------|:-----------------------|:------------|\n",
      "| model                      | 5.717M                 | 0.849G      |\n",
      "|  cls_token                 |  (1, 1, 192)           |             |\n",
      "|  pos_embed                 |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj          |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight  |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias    |   (192,)               |             |\n",
      "|  blocks                    |  5.338M                |  0.819G     |\n",
      "|   blocks.0                 |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.0.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.0.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.1                 |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.1.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.1.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.2                 |   0.445M               |   0.102G    |\n",
      "|    blocks.2.norm1          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.2.attn           |    0.148M              |    43.951M  |\n",
      "|     blocks.2.attn.qkv      |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj     |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2          |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.2.mlp            |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1       |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2       |     0.148M             |     29.049M |\n",
      "|   blocks.3                 |   0.445M               |   79.218M   |\n",
      "|    blocks.3.norm1          |    0.384K              |    0.151M   |\n",
      "|     blocks.3.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.3.attn           |    0.148M              |    32.616M  |\n",
      "|     blocks.3.attn.qkv      |     0.111M             |     17.363M |\n",
      "|     blocks.3.attn.proj     |     37.056K            |     5.788M  |\n",
      "|    blocks.3.norm2          |    0.384K              |    0.151M   |\n",
      "|     blocks.3.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.3.mlp            |    0.296M              |    46.301M  |\n",
      "|     blocks.3.mlp.fc1       |     0.148M             |     23.151M |\n",
      "|     blocks.3.mlp.fc2       |     0.148M             |     23.151M |\n",
      "|   blocks.4                 |   0.445M               |   79.218M   |\n",
      "|    blocks.4.norm1          |    0.384K              |    0.151M   |\n",
      "|     blocks.4.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.4.attn           |    0.148M              |    32.616M  |\n",
      "|     blocks.4.attn.qkv      |     0.111M             |     17.363M |\n",
      "|     blocks.4.attn.proj     |     37.056K            |     5.788M  |\n",
      "|    blocks.4.norm2          |    0.384K              |    0.151M   |\n",
      "|     blocks.4.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.4.mlp            |    0.296M              |    46.301M  |\n",
      "|     blocks.4.mlp.fc1       |     0.148M             |     23.151M |\n",
      "|     blocks.4.mlp.fc2       |     0.148M             |     23.151M |\n",
      "|   blocks.5                 |   0.445M               |   79.218M   |\n",
      "|    blocks.5.norm1          |    0.384K              |    0.151M   |\n",
      "|     blocks.5.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.5.attn           |    0.148M              |    32.616M  |\n",
      "|     blocks.5.attn.qkv      |     0.111M             |     17.363M |\n",
      "|     blocks.5.attn.proj     |     37.056K            |     5.788M  |\n",
      "|    blocks.5.norm2          |    0.384K              |    0.151M   |\n",
      "|     blocks.5.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.5.mlp            |    0.296M              |    46.301M  |\n",
      "|     blocks.5.mlp.fc1       |     0.148M             |     23.151M |\n",
      "|     blocks.5.mlp.fc2       |     0.148M             |     23.151M |\n",
      "|   blocks.6                 |   0.445M               |   61.536M   |\n",
      "|    blocks.6.norm1          |    0.384K              |    0.12M    |\n",
      "|     blocks.6.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.6.attn           |    0.148M              |    24.432M  |\n",
      "|     blocks.6.attn.qkv      |     0.111M             |     13.824M |\n",
      "|     blocks.6.attn.proj     |     37.056K            |     4.608M  |\n",
      "|    blocks.6.norm2          |    0.384K              |    0.12M    |\n",
      "|     blocks.6.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.6.mlp            |    0.296M              |    36.864M  |\n",
      "|     blocks.6.mlp.fc1       |     0.148M             |     18.432M |\n",
      "|     blocks.6.mlp.fc2       |     0.148M             |     18.432M |\n",
      "|   blocks.7                 |   0.445M               |   61.536M   |\n",
      "|    blocks.7.norm1          |    0.384K              |    0.12M    |\n",
      "|     blocks.7.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.7.attn           |    0.148M              |    24.432M  |\n",
      "|     blocks.7.attn.qkv      |     0.111M             |     13.824M |\n",
      "|     blocks.7.attn.proj     |     37.056K            |     4.608M  |\n",
      "|    blocks.7.norm2          |    0.384K              |    0.12M    |\n",
      "|     blocks.7.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.7.mlp            |    0.296M              |    36.864M  |\n",
      "|     blocks.7.mlp.fc1       |     0.148M             |     18.432M |\n",
      "|     blocks.7.mlp.fc2       |     0.148M             |     18.432M |\n",
      "|   blocks.8                 |   0.445M               |   61.536M   |\n",
      "|    blocks.8.norm1          |    0.384K              |    0.12M    |\n",
      "|     blocks.8.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.8.attn           |    0.148M              |    24.432M  |\n",
      "|     blocks.8.attn.qkv      |     0.111M             |     13.824M |\n",
      "|     blocks.8.attn.proj     |     37.056K            |     4.608M  |\n",
      "|    blocks.8.norm2          |    0.384K              |    0.12M    |\n",
      "|     blocks.8.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.8.mlp            |    0.296M              |    36.864M  |\n",
      "|     blocks.8.mlp.fc1       |     0.148M             |     18.432M |\n",
      "|     blocks.8.mlp.fc2       |     0.148M             |     18.432M |\n",
      "|   blocks.9                 |   0.445M               |   41.56M    |\n",
      "|    blocks.9.norm1          |    0.384K              |    83.52K   |\n",
      "|     blocks.9.norm1.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias    |     (192,)             |             |\n",
      "|    blocks.9.attn           |    0.148M              |    15.735M  |\n",
      "|     blocks.9.attn.qkv      |     0.111M             |     9.622M  |\n",
      "|     blocks.9.attn.proj     |     37.056K            |     3.207M  |\n",
      "|    blocks.9.norm2          |    0.384K              |    83.52K   |\n",
      "|     blocks.9.norm2.weight  |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias    |     (192,)             |             |\n",
      "|    blocks.9.mlp            |    0.296M              |    25.657M  |\n",
      "|     blocks.9.mlp.fc1       |     0.148M             |     12.829M |\n",
      "|     blocks.9.mlp.fc2       |     0.148M             |     12.829M |\n",
      "|   blocks.10                |   0.445M               |   28.53M    |\n",
      "|    blocks.10.norm1         |    0.384K              |    58.56K   |\n",
      "|     blocks.10.norm1.weight |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.10.attn          |    0.148M              |    10.424M  |\n",
      "|     blocks.10.attn.qkv     |     0.111M             |     6.746M  |\n",
      "|     blocks.10.attn.proj    |     37.056K            |     2.249M  |\n",
      "|    blocks.10.norm2         |    0.384K              |    58.56K   |\n",
      "|     blocks.10.norm2.weight |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.10.mlp           |    0.296M              |    17.99M   |\n",
      "|     blocks.10.mlp.fc1      |     0.148M             |     8.995M  |\n",
      "|     blocks.10.mlp.fc2      |     0.148M             |     8.995M  |\n",
      "|   blocks.11                |   0.445M               |   19.814M   |\n",
      "|    blocks.11.norm1         |    0.384K              |    41.28K   |\n",
      "|     blocks.11.norm1.weight |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias   |     (192,)             |             |\n",
      "|    blocks.11.attn          |    0.148M              |    7.051M   |\n",
      "|     blocks.11.attn.qkv     |     0.111M             |     4.755M  |\n",
      "|     blocks.11.attn.proj    |     37.056K            |     1.585M  |\n",
      "|    blocks.11.norm2         |    0.384K              |    41.28K   |\n",
      "|     blocks.11.norm2.weight |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias   |     (192,)             |             |\n",
      "|    blocks.11.mlp           |    0.296M              |    12.681M  |\n",
      "|     blocks.11.mlp.fc1      |     0.148M             |     6.341M  |\n",
      "|     blocks.11.mlp.fc2      |     0.148M             |     6.341M  |\n",
      "|  norm                      |  0.384K                |  28.8K      |\n",
      "|   norm.weight              |   (192,)               |             |\n",
      "|   norm.bias                |   (192,)               |             |\n",
      "|  head                      |  0.193M                |  0.192M     |\n",
      "|   head.weight              |   (1000, 192)          |             |\n",
      "|   head.bias                |   (1000,)              |             |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "TokenRankVisionTransformer(\n",
      "  #params: 5.72M, #flops: 0.85G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 0.82G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 79.22M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 32.62M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.36M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.3M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 79.22M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 32.62M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.36M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.3M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 79.22M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 32.62M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.36M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.79M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.3M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.15M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 61.54M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 24.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 13.82M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 4.61M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 36.86M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 61.54M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 24.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 13.82M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 4.61M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 36.86M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 61.54M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 24.43M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 13.82M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 4.61M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 36.86M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 18.43M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 41.56M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 83.52K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 15.74M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 9.62M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 3.21M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 83.52K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 25.66M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 12.83M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 12.83M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 28.53M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 58.56K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 10.42M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 6.75M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 2.25M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 58.56K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 17.99M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 8.99M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 8.99M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 19.81M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 41.28K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 7.05M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 4.76M\n",
      "        )\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 1.59M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 41.28K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 12.68M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 6.34M\n",
      "        )\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 28.8K\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      ")\n",
      "848572032\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT -T\n",
    "patch_size = 16\n",
    "layers = 12\n",
    "prune_list = [2,5,8,9,10,11]\n",
    "heads = 3\n",
    "mlp_ratio = 4.\n",
    "dims = 192\n",
    "qkv_bias = True\n",
    "retain_rate_list = [0.8,0.8,0.7,0.7,0.7,0.7]\n",
    "tau_imp = 1\n",
    "input_size=224\n",
    "model = TokenRankVisionTransformer(\n",
    "                prune_list=prune_list, patch_size=patch_size,  embed_dim=dims, depth=layers,\n",
    "                num_heads=heads, mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,retain_rate_list = retain_rate_list,\n",
    "                tau_imp=tau_imp)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02a41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  2.094 ( 2.094)\tLoss 6.5117e-01 (6.5117e-01)\tAcc@1  85.94 ( 85.94)\tAcc@5  96.09 ( 96.09)\n",
      "Test: [ 20/391]\tTime  0.901 ( 0.216)\tLoss 1.0334e+00 (9.2292e-01)\tAcc@1  78.91 ( 80.02)\tAcc@5  93.75 ( 94.49)\n",
      "Test: [ 40/391]\tTime  0.933 ( 0.171)\tLoss 1.0122e+00 (9.4529e-01)\tAcc@1  79.69 ( 79.31)\tAcc@5  92.19 ( 94.26)\n",
      "Test: [ 60/391]\tTime  0.856 ( 0.152)\tLoss 1.0657e+00 (9.1658e-01)\tAcc@1  78.12 ( 80.65)\tAcc@5  92.97 ( 94.38)\n",
      "Test: [ 80/391]\tTime  0.851 ( 0.142)\tLoss 1.1886e+00 (9.9081e-01)\tAcc@1  72.66 ( 77.98)\tAcc@5  90.62 ( 94.09)\n",
      "Test: [100/391]\tTime  0.793 ( 0.136)\tLoss 9.4779e-01 (9.9308e-01)\tAcc@1  79.69 ( 77.72)\tAcc@5  95.31 ( 94.21)\n",
      "Test: [120/391]\tTime  0.766 ( 0.134)\tLoss 8.1804e-01 (9.9517e-01)\tAcc@1  82.03 ( 77.46)\tAcc@5  96.88 ( 94.30)\n",
      "Test: [140/391]\tTime  0.749 ( 0.132)\tLoss 1.1997e+00 (9.8386e-01)\tAcc@1  71.88 ( 77.67)\tAcc@5  93.75 ( 94.46)\n",
      "Test: [160/391]\tTime  1.037 ( 0.134)\tLoss 1.1630e+00 (9.9520e-01)\tAcc@1  71.88 ( 77.40)\tAcc@5  92.19 ( 94.24)\n",
      "Test: [180/391]\tTime  0.843 ( 0.132)\tLoss 1.8009e+00 (1.0543e+00)\tAcc@1  50.78 ( 75.96)\tAcc@5  85.94 ( 93.56)\n",
      "Test: [200/391]\tTime  0.616 ( 0.129)\tLoss 1.2083e+00 (1.1167e+00)\tAcc@1  69.53 ( 74.57)\tAcc@5  93.75 ( 92.74)\n",
      "Test: [220/391]\tTime  0.573 ( 0.128)\tLoss 9.5867e-01 (1.1442e+00)\tAcc@1  79.69 ( 74.00)\tAcc@5  92.97 ( 92.28)\n",
      "Test: [240/391]\tTime  0.598 ( 0.127)\tLoss 1.3719e+00 (1.1644e+00)\tAcc@1  72.66 ( 73.64)\tAcc@5  85.94 ( 91.96)\n",
      "Test: [260/391]\tTime  0.651 ( 0.126)\tLoss 1.5776e+00 (1.2049e+00)\tAcc@1  64.06 ( 72.51)\tAcc@5  88.28 ( 91.47)\n",
      "Test: [280/391]\tTime  0.443 ( 0.126)\tLoss 1.5503e+00 (1.2268e+00)\tAcc@1  60.94 ( 72.08)\tAcc@5  89.84 ( 91.19)\n",
      "Test: [300/391]\tTime  0.237 ( 0.124)\tLoss 1.2493e+00 (1.2501e+00)\tAcc@1  75.00 ( 71.63)\tAcc@5  88.28 ( 90.82)\n",
      "Test: [320/391]\tTime  0.169 ( 0.123)\tLoss 8.1251e-01 (1.2681e+00)\tAcc@1  86.72 ( 71.28)\tAcc@5  95.31 ( 90.52)\n",
      "Test: [340/391]\tTime  0.271 ( 0.123)\tLoss 1.0800e+00 (1.2917e+00)\tAcc@1  75.00 ( 70.71)\tAcc@5  95.31 ( 90.22)\n",
      "Test: [360/391]\tTime  0.275 ( 0.123)\tLoss 1.6453e+00 (1.3071e+00)\tAcc@1  60.16 ( 70.37)\tAcc@5  89.06 ( 90.05)\n",
      "Test: [380/391]\tTime  0.562 ( 0.123)\tLoss 1.1884e+00 (1.2971e+00)\tAcc@1  73.44 ( 70.59)\tAcc@5  94.53 ( 90.21)\n",
      " * Acc@1 70.778 Acc@5 90.290\n"
     ]
    }
   ],
   "source": [
    "# empharsizing after average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 9 10 11 --retain_rate_list 0.9 0.8 0.9 0.9 0.9 0.9 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af5274a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  2.054 ( 2.054)\tLoss 7.1200e-01 (7.1200e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  95.31 ( 95.31)\n",
      "Test: [ 20/391]\tTime  1.022 ( 0.229)\tLoss 1.0543e+00 (9.7437e-01)\tAcc@1  81.25 ( 79.46)\tAcc@5  92.97 ( 93.71)\n",
      "Test: [ 40/391]\tTime  1.089 ( 0.185)\tLoss 1.0446e+00 (1.0014e+00)\tAcc@1  78.91 ( 78.39)\tAcc@5  89.84 ( 93.46)\n",
      "Test: [ 60/391]\tTime  1.008 ( 0.167)\tLoss 1.1876e+00 (9.8588e-01)\tAcc@1  77.34 ( 79.18)\tAcc@5  92.19 ( 93.62)\n",
      "Test: [ 80/391]\tTime  1.031 ( 0.157)\tLoss 1.2243e+00 (1.0636e+00)\tAcc@1  71.88 ( 76.34)\tAcc@5  92.19 ( 93.40)\n",
      "Test: [100/391]\tTime  0.912 ( 0.151)\tLoss 9.8041e-01 (1.0646e+00)\tAcc@1  78.91 ( 76.17)\tAcc@5  95.31 ( 93.54)\n",
      "Test: [120/391]\tTime  0.985 ( 0.150)\tLoss 8.9318e-01 (1.0652e+00)\tAcc@1  76.56 ( 75.88)\tAcc@5  96.88 ( 93.67)\n",
      "Test: [140/391]\tTime  0.854 ( 0.147)\tLoss 1.2168e+00 (1.0515e+00)\tAcc@1  71.09 ( 76.20)\tAcc@5  96.88 ( 93.86)\n",
      "Test: [160/391]\tTime  1.122 ( 0.149)\tLoss 1.2193e+00 (1.0574e+00)\tAcc@1  71.09 ( 76.15)\tAcc@5  92.19 ( 93.69)\n",
      "Test: [180/391]\tTime  0.896 ( 0.146)\tLoss 1.8323e+00 (1.1160e+00)\tAcc@1  54.69 ( 74.69)\tAcc@5  85.94 ( 92.96)\n",
      "Test: [200/391]\tTime  0.724 ( 0.142)\tLoss 1.4003e+00 (1.1766e+00)\tAcc@1  64.06 ( 73.42)\tAcc@5  91.41 ( 92.13)\n",
      "Test: [220/391]\tTime  0.583 ( 0.140)\tLoss 9.8032e-01 (1.2023e+00)\tAcc@1  79.69 ( 72.90)\tAcc@5  95.31 ( 91.70)\n",
      "Test: [240/391]\tTime  0.631 ( 0.140)\tLoss 1.4718e+00 (1.2220e+00)\tAcc@1  75.78 ( 72.55)\tAcc@5  84.38 ( 91.42)\n",
      "Test: [260/391]\tTime  0.741 ( 0.139)\tLoss 1.6543e+00 (1.2606e+00)\tAcc@1  60.94 ( 71.49)\tAcc@5  86.72 ( 90.93)\n",
      "Test: [280/391]\tTime  0.822 ( 0.138)\tLoss 1.4977e+00 (1.2823e+00)\tAcc@1  63.28 ( 71.07)\tAcc@5  92.19 ( 90.62)\n",
      "Test: [300/391]\tTime  0.557 ( 0.136)\tLoss 1.3342e+00 (1.3033e+00)\tAcc@1  73.44 ( 70.65)\tAcc@5  85.94 ( 90.27)\n",
      "Test: [320/391]\tTime  0.505 ( 0.135)\tLoss 8.4730e-01 (1.3186e+00)\tAcc@1  88.28 ( 70.32)\tAcc@5  95.31 ( 90.04)\n",
      "Test: [340/391]\tTime  0.480 ( 0.135)\tLoss 1.2164e+00 (1.3431e+00)\tAcc@1  68.75 ( 69.70)\tAcc@5  93.75 ( 89.71)\n",
      "Test: [360/391]\tTime  0.218 ( 0.133)\tLoss 1.6348e+00 (1.3559e+00)\tAcc@1  61.72 ( 69.36)\tAcc@5  88.28 ( 89.57)\n",
      "Test: [380/391]\tTime  0.510 ( 0.134)\tLoss 1.1508e+00 (1.3452e+00)\tAcc@1  72.66 ( 69.61)\tAcc@5  94.53 ( 89.72)\n",
      " * Acc@1 69.806 Acc@5 89.820\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 9 10 11 --retain_rate_list 0.9 0.8 0.9 0.9 0.9 0.9 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c9fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  2.190 ( 2.190)\tLoss 7.2974e-01 (7.2974e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  95.31 ( 95.31)\n",
      "Test: [ 20/391]\tTime  1.141 ( 0.245)\tLoss 1.0566e+00 (9.8417e-01)\tAcc@1  78.12 ( 78.79)\tAcc@5  94.53 ( 94.20)\n",
      "Test: [ 40/391]\tTime  1.048 ( 0.192)\tLoss 1.1588e+00 (1.0091e+00)\tAcc@1  79.69 ( 78.24)\tAcc@5  90.62 ( 93.75)\n",
      "Test: [ 60/391]\tTime  0.990 ( 0.170)\tLoss 1.1293e+00 (9.8887e-01)\tAcc@1  76.56 ( 79.12)\tAcc@5  92.97 ( 93.90)\n",
      "Test: [ 80/391]\tTime  1.051 ( 0.159)\tLoss 1.2499e+00 (1.0673e+00)\tAcc@1  70.31 ( 76.44)\tAcc@5  90.62 ( 93.60)\n",
      "Test: [100/391]\tTime  1.049 ( 0.153)\tLoss 1.0644e+00 (1.0689e+00)\tAcc@1  78.12 ( 76.05)\tAcc@5  94.53 ( 93.74)\n",
      "Test: [120/391]\tTime  1.002 ( 0.151)\tLoss 8.6259e-01 (1.0698e+00)\tAcc@1  81.25 ( 75.78)\tAcc@5  96.09 ( 93.81)\n",
      "Test: [140/391]\tTime  0.851 ( 0.148)\tLoss 1.2842e+00 (1.0600e+00)\tAcc@1  67.19 ( 75.91)\tAcc@5  93.75 ( 93.95)\n",
      "Test: [160/391]\tTime  1.082 ( 0.149)\tLoss 1.3112e+00 (1.0728e+00)\tAcc@1  70.31 ( 75.78)\tAcc@5  91.41 ( 93.68)\n",
      "Test: [180/391]\tTime  0.998 ( 0.146)\tLoss 1.9698e+00 (1.1423e+00)\tAcc@1  48.44 ( 74.19)\tAcc@5  84.38 ( 92.81)\n",
      "Test: [200/391]\tTime  0.848 ( 0.142)\tLoss 1.4187e+00 (1.2140e+00)\tAcc@1  62.50 ( 72.75)\tAcc@5  90.62 ( 91.78)\n",
      "Test: [220/391]\tTime  0.953 ( 0.142)\tLoss 1.1913e+00 (1.2486e+00)\tAcc@1  76.56 ( 72.12)\tAcc@5  91.41 ( 91.20)\n",
      "Test: [240/391]\tTime  0.727 ( 0.141)\tLoss 1.5211e+00 (1.2742e+00)\tAcc@1  71.09 ( 71.76)\tAcc@5  82.03 ( 90.80)\n",
      "Test: [260/391]\tTime  0.837 ( 0.140)\tLoss 1.6608e+00 (1.3174e+00)\tAcc@1  63.28 ( 70.67)\tAcc@5  87.50 ( 90.24)\n",
      "Test: [280/391]\tTime  0.834 ( 0.139)\tLoss 1.6865e+00 (1.3433e+00)\tAcc@1  59.38 ( 70.20)\tAcc@5  87.50 ( 89.86)\n",
      "Test: [300/391]\tTime  0.795 ( 0.138)\tLoss 1.3831e+00 (1.3693e+00)\tAcc@1  71.09 ( 69.72)\tAcc@5  85.94 ( 89.43)\n",
      "Test: [320/391]\tTime  0.639 ( 0.137)\tLoss 9.3325e-01 (1.3900e+00)\tAcc@1  85.16 ( 69.33)\tAcc@5  95.31 ( 89.07)\n",
      "Test: [340/391]\tTime  0.689 ( 0.137)\tLoss 1.1275e+00 (1.4157e+00)\tAcc@1  73.44 ( 68.73)\tAcc@5  95.31 ( 88.76)\n",
      "Test: [360/391]\tTime  0.263 ( 0.136)\tLoss 1.8804e+00 (1.4343e+00)\tAcc@1  58.59 ( 68.32)\tAcc@5  88.28 ( 88.55)\n",
      "Test: [380/391]\tTime  0.426 ( 0.136)\tLoss 1.4139e+00 (1.4230e+00)\tAcc@1  66.41 ( 68.54)\tAcc@5  92.97 ( 88.72)\n",
      " * Acc@1 68.720 Acc@5 88.812\n"
     ]
    }
   ],
   "source": [
    "# empharsizing after average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 9 10 11 --retain_rate_list 0.8 0.8 0.7 0.7 0.7 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb91954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  2.102 ( 2.102)\tLoss 1.0357e+00 (1.0357e+00)\tAcc@1  81.25 ( 81.25)\tAcc@5  95.31 ( 95.31)\n",
      "Test: [ 20/391]\tTime  1.055 ( 0.233)\tLoss 1.3838e+00 (1.2149e+00)\tAcc@1  77.34 ( 75.78)\tAcc@5  89.06 ( 92.56)\n",
      "Test: [ 40/391]\tTime  1.042 ( 0.184)\tLoss 1.3204e+00 (1.2433e+00)\tAcc@1  74.22 ( 75.15)\tAcc@5  89.84 ( 92.09)\n",
      "Test: [ 60/391]\tTime  0.865 ( 0.161)\tLoss 1.3326e+00 (1.2276e+00)\tAcc@1  69.53 ( 75.70)\tAcc@5  92.19 ( 92.26)\n",
      "Test: [ 80/391]\tTime  0.951 ( 0.151)\tLoss 1.5748e+00 (1.3123e+00)\tAcc@1  64.06 ( 72.44)\tAcc@5  89.84 ( 91.74)\n",
      "Test: [100/391]\tTime  0.810 ( 0.145)\tLoss 1.2127e+00 (1.3045e+00)\tAcc@1  75.00 ( 72.23)\tAcc@5  92.97 ( 91.98)\n",
      "Test: [120/391]\tTime  1.002 ( 0.144)\tLoss 1.1032e+00 (1.2975e+00)\tAcc@1  73.44 ( 71.99)\tAcc@5  95.31 ( 92.16)\n",
      "Test: [140/391]\tTime  0.833 ( 0.140)\tLoss 1.4249e+00 (1.2853e+00)\tAcc@1  64.84 ( 72.33)\tAcc@5  93.75 ( 92.28)\n",
      "Test: [160/391]\tTime  1.125 ( 0.142)\tLoss 1.4073e+00 (1.2889e+00)\tAcc@1  71.09 ( 72.31)\tAcc@5  89.84 ( 92.07)\n",
      "Test: [180/391]\tTime  0.829 ( 0.139)\tLoss 2.1239e+00 (1.3517e+00)\tAcc@1  47.66 ( 70.78)\tAcc@5  79.69 ( 91.24)\n",
      "Test: [200/391]\tTime  0.638 ( 0.136)\tLoss 1.7774e+00 (1.4113e+00)\tAcc@1  55.47 ( 69.56)\tAcc@5  84.38 ( 90.45)\n",
      "Test: [220/391]\tTime  0.639 ( 0.135)\tLoss 1.2172e+00 (1.4401e+00)\tAcc@1  73.44 ( 69.04)\tAcc@5  91.41 ( 89.96)\n",
      "Test: [240/391]\tTime  0.365 ( 0.134)\tLoss 1.8171e+00 (1.4618e+00)\tAcc@1  67.19 ( 68.76)\tAcc@5  82.81 ( 89.57)\n",
      "Test: [260/391]\tTime  0.288 ( 0.133)\tLoss 1.9311e+00 (1.5009e+00)\tAcc@1  55.47 ( 67.72)\tAcc@5  85.16 ( 89.05)\n",
      "Test: [280/391]\tTime  0.036 ( 0.132)\tLoss 1.7462e+00 (1.5254e+00)\tAcc@1  64.84 ( 67.29)\tAcc@5  87.50 ( 88.67)\n",
      "Test: [300/391]\tTime  0.034 ( 0.132)\tLoss 1.5523e+00 (1.5467e+00)\tAcc@1  71.88 ( 66.90)\tAcc@5  85.16 ( 88.24)\n",
      "Test: [320/391]\tTime  0.034 ( 0.131)\tLoss 1.1709e+00 (1.5629e+00)\tAcc@1  82.81 ( 66.65)\tAcc@5  92.19 ( 87.98)\n",
      "Test: [340/391]\tTime  0.025 ( 0.131)\tLoss 1.5283e+00 (1.5894e+00)\tAcc@1  64.06 ( 65.99)\tAcc@5  92.97 ( 87.60)\n",
      "Test: [360/391]\tTime  0.025 ( 0.131)\tLoss 1.7685e+00 (1.6016e+00)\tAcc@1  59.38 ( 65.71)\tAcc@5  92.19 ( 87.45)\n",
      "Test: [380/391]\tTime  0.025 ( 0.130)\tLoss 1.3393e+00 (1.5881e+00)\tAcc@1  70.31 ( 65.99)\tAcc@5  92.97 ( 87.63)\n",
      " * Acc@1 66.210 Acc@5 87.764\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 9 10 11 --retain_rate_list 0.8 0.8 0.7 0.7 0.7 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951f21f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  2.172 ( 2.172)\tLoss 6.3284e-01 (6.3284e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  96.09 ( 96.09)\n",
      "Test: [ 20/391]\tTime  1.127 ( 0.245)\tLoss 1.0295e+00 (9.1775e-01)\tAcc@1  78.91 ( 80.02)\tAcc@5  92.97 ( 94.49)\n",
      "Test: [ 40/391]\tTime  1.047 ( 0.192)\tLoss 1.0082e+00 (9.4325e-01)\tAcc@1  79.69 ( 79.44)\tAcc@5  92.19 ( 94.13)\n",
      "Test: [ 60/391]\tTime  0.936 ( 0.169)\tLoss 1.0758e+00 (9.1470e-01)\tAcc@1  78.12 ( 80.57)\tAcc@5  92.19 ( 94.25)\n",
      "Test: [ 80/391]\tTime  1.082 ( 0.161)\tLoss 1.2096e+00 (9.9096e-01)\tAcc@1  71.88 ( 77.85)\tAcc@5  90.62 ( 94.06)\n",
      "Test: [100/391]\tTime  1.154 ( 0.157)\tLoss 9.5112e-01 (9.9289e-01)\tAcc@1  79.69 ( 77.57)\tAcc@5  95.31 ( 94.22)\n",
      "Test: [120/391]\tTime  1.034 ( 0.154)\tLoss 8.1775e-01 (9.9467e-01)\tAcc@1  82.81 ( 77.31)\tAcc@5  96.88 ( 94.31)\n",
      "Test: [140/391]\tTime  0.759 ( 0.150)\tLoss 1.2089e+00 (9.8236e-01)\tAcc@1  71.09 ( 77.55)\tAcc@5  95.31 ( 94.48)\n",
      "Test: [160/391]\tTime  1.014 ( 0.152)\tLoss 1.1680e+00 (9.9338e-01)\tAcc@1  71.88 ( 77.36)\tAcc@5  91.41 ( 94.26)\n",
      "Test: [180/391]\tTime  0.748 ( 0.149)\tLoss 1.8172e+00 (1.0528e+00)\tAcc@1  50.78 ( 75.96)\tAcc@5  86.72 ( 93.53)\n",
      "Test: [200/391]\tTime  0.466 ( 0.146)\tLoss 1.2353e+00 (1.1155e+00)\tAcc@1  69.53 ( 74.53)\tAcc@5  92.97 ( 92.66)\n",
      "Test: [220/391]\tTime  0.419 ( 0.144)\tLoss 9.6937e-01 (1.1438e+00)\tAcc@1  78.12 ( 73.99)\tAcc@5  91.41 ( 92.20)\n",
      "Test: [240/391]\tTime  0.359 ( 0.143)\tLoss 1.3630e+00 (1.1639e+00)\tAcc@1  74.22 ( 73.64)\tAcc@5  85.94 ( 91.87)\n",
      "Test: [260/391]\tTime  0.330 ( 0.142)\tLoss 1.6110e+00 (1.2048e+00)\tAcc@1  60.94 ( 72.47)\tAcc@5  85.94 ( 91.36)\n",
      "Test: [280/391]\tTime  0.028 ( 0.142)\tLoss 1.5441e+00 (1.2266e+00)\tAcc@1  59.38 ( 72.00)\tAcc@5  88.28 ( 91.10)\n",
      "Test: [300/391]\tTime  0.028 ( 0.141)\tLoss 1.2507e+00 (1.2498e+00)\tAcc@1  74.22 ( 71.58)\tAcc@5  88.28 ( 90.72)\n",
      "Test: [320/391]\tTime  0.028 ( 0.140)\tLoss 8.2515e-01 (1.2678e+00)\tAcc@1  86.72 ( 71.25)\tAcc@5  95.31 ( 90.45)\n",
      "Test: [340/391]\tTime  0.028 ( 0.140)\tLoss 1.0747e+00 (1.2915e+00)\tAcc@1  74.22 ( 70.67)\tAcc@5  95.31 ( 90.15)\n",
      "Test: [360/391]\tTime  0.028 ( 0.140)\tLoss 1.6555e+00 (1.3069e+00)\tAcc@1  59.38 ( 70.33)\tAcc@5  89.06 ( 89.98)\n",
      "Test: [380/391]\tTime  0.028 ( 0.139)\tLoss 1.2195e+00 (1.2968e+00)\tAcc@1  71.88 ( 70.53)\tAcc@5  94.53 ( 90.14)\n",
      " * Acc@1 70.734 Acc@5 90.216\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 9 10 11 --retain_rate_list 0.9 0.8 0.9 0.9 0.9 0.9 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470176c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  2.093 ( 2.093)\tLoss 5.9033e-01 (5.9033e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  97.66 ( 97.66)\n",
      "Test: [ 20/391]\tTime  0.964 ( 0.230)\tLoss 9.9164e-01 (8.7250e-01)\tAcc@1  80.47 ( 81.32)\tAcc@5  93.75 ( 95.39)\n",
      "Test: [ 40/391]\tTime  0.974 ( 0.183)\tLoss 9.1898e-01 (8.9883e-01)\tAcc@1  80.47 ( 80.34)\tAcc@5  91.41 ( 94.84)\n",
      "Test: [ 60/391]\tTime  0.852 ( 0.162)\tLoss 1.0682e+00 (8.7181e-01)\tAcc@1  78.91 ( 81.19)\tAcc@5  93.75 ( 94.95)\n",
      "Test: [ 80/391]\tTime  0.837 ( 0.150)\tLoss 1.1435e+00 (9.4586e-01)\tAcc@1  71.88 ( 78.51)\tAcc@5  91.41 ( 94.75)\n",
      "Test: [100/391]\tTime  0.834 ( 0.142)\tLoss 9.0172e-01 (9.4893e-01)\tAcc@1  82.81 ( 78.32)\tAcc@5  96.88 ( 94.83)\n",
      "Test: [120/391]\tTime  0.905 ( 0.140)\tLoss 8.0397e-01 (9.5092e-01)\tAcc@1  82.03 ( 78.10)\tAcc@5  96.09 ( 94.86)\n",
      "Test: [140/391]\tTime  0.944 ( 0.137)\tLoss 1.1538e+00 (9.3728e-01)\tAcc@1  73.44 ( 78.42)\tAcc@5  96.09 ( 94.99)\n",
      "Test: [160/391]\tTime  1.000 ( 0.139)\tLoss 1.1015e+00 (9.4778e-01)\tAcc@1  76.56 ( 78.32)\tAcc@5  90.62 ( 94.73)\n",
      "Test: [180/391]\tTime  0.816 ( 0.136)\tLoss 1.7376e+00 (1.0033e+00)\tAcc@1  55.47 ( 77.01)\tAcc@5  86.72 ( 94.07)\n",
      "Test: [200/391]\tTime  0.695 ( 0.132)\tLoss 1.1826e+00 (1.0626e+00)\tAcc@1  69.53 ( 75.63)\tAcc@5  92.19 ( 93.30)\n",
      "Test: [220/391]\tTime  0.514 ( 0.131)\tLoss 8.1046e-01 (1.0876e+00)\tAcc@1  82.81 ( 75.11)\tAcc@5  96.09 ( 92.90)\n",
      "Test: [240/391]\tTime  0.713 ( 0.130)\tLoss 1.3548e+00 (1.1056e+00)\tAcc@1  77.34 ( 74.82)\tAcc@5  85.16 ( 92.64)\n",
      "Test: [260/391]\tTime  0.695 ( 0.129)\tLoss 1.5158e+00 (1.1452e+00)\tAcc@1  63.28 ( 73.71)\tAcc@5  89.06 ( 92.16)\n",
      "Test: [280/391]\tTime  0.743 ( 0.129)\tLoss 1.3970e+00 (1.1646e+00)\tAcc@1  63.28 ( 73.30)\tAcc@5  92.97 ( 91.92)\n",
      "Test: [300/391]\tTime  0.554 ( 0.127)\tLoss 1.2684e+00 (1.1849e+00)\tAcc@1  75.78 ( 72.95)\tAcc@5  88.28 ( 91.62)\n",
      "Test: [320/391]\tTime  0.454 ( 0.126)\tLoss 7.6387e-01 (1.2003e+00)\tAcc@1  87.50 ( 72.67)\tAcc@5  96.88 ( 91.39)\n",
      "Test: [340/391]\tTime  0.482 ( 0.125)\tLoss 1.0923e+00 (1.2226e+00)\tAcc@1  71.09 ( 72.09)\tAcc@5  96.09 ( 91.10)\n",
      "Test: [360/391]\tTime  0.452 ( 0.125)\tLoss 1.4843e+00 (1.2355e+00)\tAcc@1  64.84 ( 71.73)\tAcc@5  92.19 ( 90.94)\n",
      "Test: [380/391]\tTime  0.755 ( 0.125)\tLoss 1.1094e+00 (1.2269e+00)\tAcc@1  73.44 ( 71.95)\tAcc@5  94.53 ( 91.05)\n",
      " * Acc@1 72.136 Acc@5 91.128\n"
     ]
    }
   ],
   "source": [
    "# no pruning\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 --retain_rate_list 1 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7b0b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hw4948/.conda/envs/tokenrank/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "# missing keys= []\n",
      "# unexpected keys= []\n",
      "successfully loaded from pre-trained weights: ../model_weights/deit_tiny_patch16_224-a1311bcf.pth\n",
      "number of params: 5717416\n",
      "Test: [  0/391]\tTime  1.906 ( 1.906)\tLoss 6.6532e-01 (6.6532e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  96.09 ( 96.09)\n",
      "Test: [ 20/391]\tTime  0.904 ( 0.213)\tLoss 1.0347e+00 (9.2631e-01)\tAcc@1  79.69 ( 79.84)\tAcc@5  92.97 ( 94.46)\n",
      "Test: [ 40/391]\tTime  0.987 ( 0.172)\tLoss 1.0321e+00 (9.4774e-01)\tAcc@1  78.91 ( 79.17)\tAcc@5  91.41 ( 94.21)\n",
      "Test: [ 60/391]\tTime  0.867 ( 0.152)\tLoss 1.0741e+00 (9.2113e-01)\tAcc@1  77.34 ( 80.35)\tAcc@5  92.97 ( 94.33)\n",
      "Test: [ 80/391]\tTime  0.896 ( 0.143)\tLoss 1.1870e+00 (9.9649e-01)\tAcc@1  72.66 ( 77.75)\tAcc@5  90.62 ( 94.04)\n",
      "Test: [100/391]\tTime  0.807 ( 0.137)\tLoss 9.5334e-01 (9.9929e-01)\tAcc@1  78.91 ( 77.45)\tAcc@5  95.31 ( 94.17)\n",
      "Test: [120/391]\tTime  0.773 ( 0.135)\tLoss 8.1237e-01 (1.0011e+00)\tAcc@1  82.03 ( 77.24)\tAcc@5  96.88 ( 94.24)\n",
      "Test: [140/391]\tTime  0.751 ( 0.132)\tLoss 1.1936e+00 (9.8967e-01)\tAcc@1  70.31 ( 77.40)\tAcc@5  93.75 ( 94.39)\n",
      "Test: [160/391]\tTime  0.967 ( 0.134)\tLoss 1.1867e+00 (1.0018e+00)\tAcc@1  73.44 ( 77.12)\tAcc@5  92.19 ( 94.16)\n",
      "Test: [180/391]\tTime  0.845 ( 0.132)\tLoss 1.8283e+00 (1.0630e+00)\tAcc@1  50.00 ( 75.62)\tAcc@5  85.94 ( 93.43)\n",
      "Test: [200/391]\tTime  0.622 ( 0.128)\tLoss 1.1960e+00 (1.1271e+00)\tAcc@1  69.53 ( 74.21)\tAcc@5  93.75 ( 92.57)\n",
      "Test: [220/391]\tTime  0.445 ( 0.127)\tLoss 9.6362e-01 (1.1560e+00)\tAcc@1  78.91 ( 73.63)\tAcc@5  92.19 ( 92.07)\n",
      "Test: [240/391]\tTime  0.409 ( 0.127)\tLoss 1.4159e+00 (1.1781e+00)\tAcc@1  72.66 ( 73.24)\tAcc@5  85.94 ( 91.72)\n",
      "Test: [260/391]\tTime  0.335 ( 0.126)\tLoss 1.6153e+00 (1.2192e+00)\tAcc@1  63.28 ( 72.17)\tAcc@5  88.28 ( 91.24)\n",
      "Test: [280/391]\tTime  0.027 ( 0.126)\tLoss 1.5719e+00 (1.2415e+00)\tAcc@1  57.81 ( 71.72)\tAcc@5  89.06 ( 90.97)\n",
      "Test: [300/391]\tTime  0.026 ( 0.125)\tLoss 1.2385e+00 (1.2653e+00)\tAcc@1  75.00 ( 71.26)\tAcc@5  88.28 ( 90.57)\n",
      "Test: [320/391]\tTime  0.036 ( 0.124)\tLoss 8.1972e-01 (1.2843e+00)\tAcc@1  87.50 ( 70.91)\tAcc@5  94.53 ( 90.26)\n",
      "Test: [340/391]\tTime  0.026 ( 0.124)\tLoss 1.0685e+00 (1.3085e+00)\tAcc@1  75.78 ( 70.31)\tAcc@5  94.53 ( 89.95)\n",
      "Test: [360/391]\tTime  0.028 ( 0.124)\tLoss 1.6508e+00 (1.3246e+00)\tAcc@1  59.38 ( 69.95)\tAcc@5  92.19 ( 89.77)\n",
      "Test: [380/391]\tTime  0.027 ( 0.123)\tLoss 1.2036e+00 (1.3144e+00)\tAcc@1  72.66 ( 70.16)\tAcc@5  94.53 ( 89.94)\n",
      " * Acc@1 70.356 Acc@5 90.030\n"
     ]
    }
   ],
   "source": [
    "# empharsizing after average\n",
    "!python3 ../zero_shot_pruning_eval.py --model_name='tokenrank_deit_tiny_3' --prune_list 2 5 8 --retain_rate_list 0.9 0.8 0.7 --data-path='/scratch/gpfs/hw4948/ILSVRC2012'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
