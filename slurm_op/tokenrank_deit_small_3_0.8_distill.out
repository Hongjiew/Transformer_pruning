/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : main.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_0/1/error.json
| distributed init (rank 0): env://
| distributed init (rank 1): env://
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
Namespace(aa='rand-m9-mstd0.5-inc1', alpha=1.0, batch_size=192, beta=0.2, clip_grad=None, color_jitter=0.4, cooldown_epochs=10, cutmix=1.0, cutmix_minmax=None, data_path='../ILSVRC2012/', data_set='IMNET', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_eval=True, dist_url='env://', distill_model=True, distillation_tau=1.0, distributed=True, drop=0.0, drop_path=0.1, epochs=30, eval=False, finetune='', gamma=0.5, gpu=0, inat_category='name', input_size=224, lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, model_name='tokenrank_deit_small_3', model_path_pretrained='./model_weights/deit_base_patch16_224', momentum=0.9, num_workers=1, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='./logs_dir/tokenrank_deit_small_3_0.8_distilled/', patience_epochs=10, pin_mem=True, rank=0, recount=1, remode='pixel', repeated_aug=True, reprob=0.25, resplit=False, resume='', retain_rate=0.8, sched='cosine', seed=0, smoothing=0.1, start_epoch=0, teacher_model='regnety_160', train_interpolation='bicubic', warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.05, world_size=2)
/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
# missing keys= []
# unexpected keys= []
successfully loaded from pre-trained weights: ./model_weights/deit_small_patch16_224-cd65a155.pth
number of params: 22050664
Start training for 30 epochs
Epoch: [0]  [   0/3336]  eta: 6:41:02  lr: 0.000001  loss: 4.4435 (4.4435)  time: 7.2131  data: 4.8540  max mem: 14799
Epoch: [0]  [  10/3336]  eta: 2:08:29  lr: 0.000001  loss: 4.4472 (4.3479)  time: 2.3179  data: 1.6746  max mem: 15050
Epoch: [0]  [  20/3336]  eta: 1:55:44  lr: 0.000001  loss: 4.4355 (4.3034)  time: 1.8383  data: 1.3846  max mem: 15050
Epoch: [0]  [  30/3336]  eta: 1:51:06  lr: 0.000001  loss: 4.4355 (4.3286)  time: 1.8505  data: 1.4168  max mem: 15050
Epoch: [0]  [  40/3336]  eta: 1:48:15  lr: 0.000001  loss: 4.3714 (4.2988)  time: 1.8412  data: 1.4162  max mem: 15050
Epoch: [0]  [  50/3336]  eta: 1:46:51  lr: 0.000001  loss: 4.2263 (4.2898)  time: 1.8497  data: 1.4318  max mem: 15050
Epoch: [0]  [  60/3336]  eta: 1:45:56  lr: 0.000001  loss: 4.2263 (4.2427)  time: 1.8782  data: 1.4625  max mem: 15050
Epoch: [0]  [  70/3336]  eta: 1:45:05  lr: 0.000001  loss: 4.2083 (4.2117)  time: 1.8789  data: 1.4694  max mem: 15050
Epoch: [0]  [  80/3336]  eta: 1:43:45  lr: 0.000001  loss: 4.2149 (4.2029)  time: 1.8255  data: 1.4166  max mem: 15050
Epoch: [0]  [  90/3336]  eta: 1:43:03  lr: 0.000001  loss: 4.2149 (4.1952)  time: 1.8134  data: 1.3941  max mem: 15050
Traceback (most recent call last):
  File "main.py", line 600, in <module>
Traceback (most recent call last):
  File "main.py", line 600, in <module>
    main(args)
  File "main.py", line 529, in main
    train_stats = train_one_epoch(
  File "/scratch/gpfs/bdedhia/tokenrank_pruning/engine.py", line 39, in train_one_epoch
    main(args)
  File "main.py", line 529, in main
    train_stats = train_one_epoch(
  File "/scratch/gpfs/bdedhia/tokenrank_pruning/engine.py", line 39, in train_one_epoch
    loss = criterion(samples, outputs, targets)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    loss = criterion(samples, outputs, targets)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/scratch/gpfs/bdedhia/tokenrank_pruning/losses.py", line 179, in forward
  File "/scratch/gpfs/bdedhia/tokenrank_pruning/losses.py", line 179, in forward
        print(f'Base loss: {self.clf_loss}, Prune Loss: {self.prune_loss}, Distillation Loss: {self.distillation_loss}')print(f'Base loss: {self.clf_loss}, Prune Loss: {self.prune_loss}, Distillation Loss: {self.distillation_loss}')

  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in __getattr__
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeErrorAttributeError: : 'TokenRankLossWithDistillationLoss' object has no attribute 'distillation_loss''TokenRankLossWithDistillationLoss' object has no attribute 'distillation_loss'

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 772650) of binary: /home/bdedhia/.conda/envs/txf_design-space/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_1/1/error.json
| distributed init (rank 0): env://
| distributed init (rank 1): env://
Traceback (most recent call last):
Traceback (most recent call last):
  File "main.py", line 600, in <module>
  File "main.py", line 600, in <module>
        main(args)main(args)

  File "main.py", line 178, in main
  File "main.py", line 178, in main
    utils.init_distributed_mode(args)    
utils.init_distributed_mode(args)  File "/scratch/gpfs/bdedhia/tokenrank_pruning/utils.py", line 235, in init_distributed_mode

  File "/scratch/gpfs/bdedhia/tokenrank_pruning/utils.py", line 235, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,    
torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    _store_based_barrier(rank, store, timeout)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 773552) of binary: /home/bdedhia/.conda/envs/txf_design-space/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_2/1/error.json
| distributed init (rank 0): env://
| distributed init (rank 1): env://
Traceback (most recent call last):
Traceback (most recent call last):
  File "main.py", line 600, in <module>
  File "main.py", line 600, in <module>
        main(args)main(args)

  File "main.py", line 178, in main
  File "main.py", line 178, in main
    utils.init_distributed_mode(args)    
utils.init_distributed_mode(args)  File "/scratch/gpfs/bdedhia/tokenrank_pruning/utils.py", line 235, in init_distributed_mode

  File "/scratch/gpfs/bdedhia/tokenrank_pruning/utils.py", line 235, in init_distributed_mode
        torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,

  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    _store_based_barrier(rank, store, timeout)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 779510) of binary: /home/bdedhia/.conda/envs/txf_design-space/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_h2e5wxvl/none_2wspcox_/attempt_3/1/error.json
| distributed init (rank 1): env://
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "main.py", line 600, in <module>
    main(args)
  File "main.py", line 178, in main
    utils.init_distributed_mode(args)
  File "/scratch/gpfs/bdedhia/tokenrank_pruning/utils.py", line 235, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "main.py", line 600, in <module>
    main(args)
  File "main.py", line 178, in main
    utils.init_distributed_mode(args)
  File "/scratch/gpfs/bdedhia/tokenrank_pruning/utils.py", line 235, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 781382) of binary: /home/bdedhia/.conda/envs/txf_design-space/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0037488937377929688 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "781382", "role": "default", "hostname": "della-i12g1", "state": "FAILED", "total_run_time": 5620, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "781383", "role": "default", "hostname": "della-i12g1", "state": "FAILED", "total_run_time": 5620, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "della-i12g1", "state": "SUCCEEDED", "total_run_time": 5620, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 781382 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/bdedhia/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
             main.py FAILED            
=======================================
Root Cause:
[0]:
  time: 2022-03-23_17:49:14
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 781382)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-03-23_17:49:14
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 781383)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

