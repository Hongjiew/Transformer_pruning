{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5a61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table, flop_count_str\n",
    "#from beit import BeitTeacher\n",
    "#from tokenrank_vit import TokenRankVisionTransformer\n",
    "#from tokenrank_beit import TokenRankBeit\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor\n",
    "import requests\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vit import VisionTransformerDiffPruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c231cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "feature_extractor = ViTFeatureExtractor()\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8b0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## diff vit pruning method\n",
      "| module                           | #parameters or shape   | #flops      |\n",
      "|:---------------------------------|:-----------------------|:------------|\n",
      "| model                            | 5.9M                   | 1.096G      |\n",
      "|  cls_token                       |  (1, 1, 192)           |             |\n",
      "|  pos_embed                       |  (1, 197, 192)         |             |\n",
      "|  patch_embed.proj                |  0.148M                |  28.901M    |\n",
      "|   patch_embed.proj.weight        |   (192, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias          |   (192,)               |             |\n",
      "|  blocks                          |  5.338M                |  1.034G     |\n",
      "|   blocks.0                       |   0.445M               |   0.102G    |\n",
      "|    blocks.0.norm1                |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.0.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.0.attn                 |    0.148M              |    43.951M  |\n",
      "|     blocks.0.attn.qkv            |     0.111M             |     21.787M |\n",
      "|     blocks.0.attn.proj           |     37.056K            |     7.262M  |\n",
      "|    blocks.0.norm2                |    0.384K              |    0.189M   |\n",
      "|     blocks.0.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.0.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.0.mlp                  |    0.296M              |    58.098M  |\n",
      "|     blocks.0.mlp.fc1             |     0.148M             |     29.049M |\n",
      "|     blocks.0.mlp.fc2             |     0.148M             |     29.049M |\n",
      "|   blocks.1                       |   0.445M               |   0.102G    |\n",
      "|    blocks.1.norm1                |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.1.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.1.attn                 |    0.148M              |    43.951M  |\n",
      "|     blocks.1.attn.qkv            |     0.111M             |     21.787M |\n",
      "|     blocks.1.attn.proj           |     37.056K            |     7.262M  |\n",
      "|    blocks.1.norm2                |    0.384K              |    0.189M   |\n",
      "|     blocks.1.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.1.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.1.mlp                  |    0.296M              |    58.098M  |\n",
      "|     blocks.1.mlp.fc1             |     0.148M             |     29.049M |\n",
      "|     blocks.1.mlp.fc2             |     0.148M             |     29.049M |\n",
      "|   blocks.2                       |   0.445M               |   0.102G    |\n",
      "|    blocks.2.norm1                |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.2.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.2.attn                 |    0.148M              |    43.951M  |\n",
      "|     blocks.2.attn.qkv            |     0.111M             |     21.787M |\n",
      "|     blocks.2.attn.proj           |     37.056K            |     7.262M  |\n",
      "|    blocks.2.norm2                |    0.384K              |    0.189M   |\n",
      "|     blocks.2.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.2.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.2.mlp                  |    0.296M              |    58.098M  |\n",
      "|     blocks.2.mlp.fc1             |     0.148M             |     29.049M |\n",
      "|     blocks.2.mlp.fc2             |     0.148M             |     29.049M |\n",
      "|   blocks.3                       |   0.445M               |   90.669M   |\n",
      "|    blocks.3.norm1                |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.3.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.3.attn                 |    0.148M              |    38.13M   |\n",
      "|     blocks.3.attn.qkv            |     0.111M             |     19.575M |\n",
      "|     blocks.3.attn.proj           |     37.056K            |     6.525M  |\n",
      "|    blocks.3.norm2                |    0.384K              |    0.17M    |\n",
      "|     blocks.3.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.3.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.3.mlp                  |    0.296M              |    52.199M  |\n",
      "|     blocks.3.mlp.fc1             |     0.148M             |     26.1M   |\n",
      "|     blocks.3.mlp.fc2             |     0.148M             |     26.1M   |\n",
      "|   blocks.4                       |   0.445M               |   90.669M   |\n",
      "|    blocks.4.norm1                |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.4.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.4.attn                 |    0.148M              |    38.13M   |\n",
      "|     blocks.4.attn.qkv            |     0.111M             |     19.575M |\n",
      "|     blocks.4.attn.proj           |     37.056K            |     6.525M  |\n",
      "|    blocks.4.norm2                |    0.384K              |    0.17M    |\n",
      "|     blocks.4.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.4.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.4.mlp                  |    0.296M              |    52.199M  |\n",
      "|     blocks.4.mlp.fc1             |     0.148M             |     26.1M   |\n",
      "|     blocks.4.mlp.fc2             |     0.148M             |     26.1M   |\n",
      "|   blocks.5                       |   0.445M               |   90.669M   |\n",
      "|    blocks.5.norm1                |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.5.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.5.attn                 |    0.148M              |    38.13M   |\n",
      "|     blocks.5.attn.qkv            |     0.111M             |     19.575M |\n",
      "|     blocks.5.attn.proj           |     37.056K            |     6.525M  |\n",
      "|    blocks.5.norm2                |    0.384K              |    0.17M    |\n",
      "|     blocks.5.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.5.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.5.mlp                  |    0.296M              |    52.199M  |\n",
      "|     blocks.5.mlp.fc1             |     0.148M             |     26.1M   |\n",
      "|     blocks.5.mlp.fc2             |     0.148M             |     26.1M   |\n",
      "|   blocks.6                       |   0.445M               |   80.35M    |\n",
      "|    blocks.6.norm1                |    0.384K              |    0.153M   |\n",
      "|     blocks.6.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.6.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.6.attn                 |    0.148M              |    33.153M  |\n",
      "|     blocks.6.attn.qkv            |     0.111M             |     17.584M |\n",
      "|     blocks.6.attn.proj           |     37.056K            |     5.861M  |\n",
      "|    blocks.6.norm2                |    0.384K              |    0.153M   |\n",
      "|     blocks.6.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.6.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.6.mlp                  |    0.296M              |    46.891M  |\n",
      "|     blocks.6.mlp.fc1             |     0.148M             |     23.446M |\n",
      "|     blocks.6.mlp.fc2             |     0.148M             |     23.446M |\n",
      "|   blocks.7                       |   0.445M               |   80.35M    |\n",
      "|    blocks.7.norm1                |    0.384K              |    0.153M   |\n",
      "|     blocks.7.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.7.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.7.attn                 |    0.148M              |    33.153M  |\n",
      "|     blocks.7.attn.qkv            |     0.111M             |     17.584M |\n",
      "|     blocks.7.attn.proj           |     37.056K            |     5.861M  |\n",
      "|    blocks.7.norm2                |    0.384K              |    0.153M   |\n",
      "|     blocks.7.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.7.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.7.mlp                  |    0.296M              |    46.891M  |\n",
      "|     blocks.7.mlp.fc1             |     0.148M             |     23.446M |\n",
      "|     blocks.7.mlp.fc2             |     0.148M             |     23.446M |\n",
      "|   blocks.8                       |   0.445M               |   80.35M    |\n",
      "|    blocks.8.norm1                |    0.384K              |    0.153M   |\n",
      "|     blocks.8.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.8.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.8.attn                 |    0.148M              |    33.153M  |\n",
      "|     blocks.8.attn.qkv            |     0.111M             |     17.584M |\n",
      "|     blocks.8.attn.proj           |     37.056K            |     5.861M  |\n",
      "|    blocks.8.norm2                |    0.384K              |    0.153M   |\n",
      "|     blocks.8.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.8.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.8.mlp                  |    0.296M              |    46.891M  |\n",
      "|     blocks.8.mlp.fc1             |     0.148M             |     23.446M |\n",
      "|     blocks.8.mlp.fc2             |     0.148M             |     23.446M |\n",
      "|   blocks.9                       |   0.445M               |   71.386M   |\n",
      "|    blocks.9.norm1                |    0.384K              |    0.137M   |\n",
      "|     blocks.9.norm1.weight        |     (192,)             |             |\n",
      "|     blocks.9.norm1.bias          |     (192,)             |             |\n",
      "|    blocks.9.attn                 |    0.148M              |    28.939M  |\n",
      "|     blocks.9.attn.qkv            |     0.111M             |     15.815M |\n",
      "|     blocks.9.attn.proj           |     37.056K            |     5.272M  |\n",
      "|    blocks.9.norm2                |    0.384K              |    0.137M   |\n",
      "|     blocks.9.norm2.weight        |     (192,)             |             |\n",
      "|     blocks.9.norm2.bias          |     (192,)             |             |\n",
      "|    blocks.9.mlp                  |    0.296M              |    42.172M  |\n",
      "|     blocks.9.mlp.fc1             |     0.148M             |     21.086M |\n",
      "|     blocks.9.mlp.fc2             |     0.148M             |     21.086M |\n",
      "|   blocks.10                      |   0.445M               |   71.386M   |\n",
      "|    blocks.10.norm1               |    0.384K              |    0.137M   |\n",
      "|     blocks.10.norm1.weight       |     (192,)             |             |\n",
      "|     blocks.10.norm1.bias         |     (192,)             |             |\n",
      "|    blocks.10.attn                |    0.148M              |    28.939M  |\n",
      "|     blocks.10.attn.qkv           |     0.111M             |     15.815M |\n",
      "|     blocks.10.attn.proj          |     37.056K            |     5.272M  |\n",
      "|    blocks.10.norm2               |    0.384K              |    0.137M   |\n",
      "|     blocks.10.norm2.weight       |     (192,)             |             |\n",
      "|     blocks.10.norm2.bias         |     (192,)             |             |\n",
      "|    blocks.10.mlp                 |    0.296M              |    42.172M  |\n",
      "|     blocks.10.mlp.fc1            |     0.148M             |     21.086M |\n",
      "|     blocks.10.mlp.fc2            |     0.148M             |     21.086M |\n",
      "|   blocks.11                      |   0.445M               |   71.386M   |\n",
      "|    blocks.11.norm1               |    0.384K              |    0.137M   |\n",
      "|     blocks.11.norm1.weight       |     (192,)             |             |\n",
      "|     blocks.11.norm1.bias         |     (192,)             |             |\n",
      "|    blocks.11.attn                |    0.148M              |    28.939M  |\n",
      "|     blocks.11.attn.qkv           |     0.111M             |     15.815M |\n",
      "|     blocks.11.attn.proj          |     37.056K            |     5.272M  |\n",
      "|    blocks.11.norm2               |    0.384K              |    0.137M   |\n",
      "|     blocks.11.norm2.weight       |     (192,)             |             |\n",
      "|     blocks.11.norm2.bias         |     (192,)             |             |\n",
      "|    blocks.11.mlp                 |    0.296M              |    42.172M  |\n",
      "|     blocks.11.mlp.fc1            |     0.148M             |     21.086M |\n",
      "|     blocks.11.mlp.fc2            |     0.148M             |     21.086M |\n",
      "|  norm                            |  0.384K                |  0.137M     |\n",
      "|   norm.weight                    |   (192,)               |             |\n",
      "|   norm.bias                      |   (192,)               |             |\n",
      "|  head                            |  0.193M                |  0.192M     |\n",
      "|   head.weight                    |   (1000, 192)          |             |\n",
      "|   head.bias                      |   (1000,)              |             |\n",
      "|  score_predictor                 |  0.182M                |  32.309M    |\n",
      "|   score_predictor.0              |   60.722K              |   11.948M   |\n",
      "|    score_predictor.0.in_conv     |    37.44K              |    7.414M   |\n",
      "|     score_predictor.0.in_conv.0  |     0.384K             |     0.188M  |\n",
      "|     score_predictor.0.in_conv.1  |     37.056K            |     7.225M  |\n",
      "|    score_predictor.0.out_conv    |    23.282K             |    4.535M   |\n",
      "|     score_predictor.0.out_conv.0 |     18.528K            |     3.613M  |\n",
      "|     score_predictor.0.out_conv.2 |     4.656K             |     0.903M  |\n",
      "|     score_predictor.0.out_conv.4 |     98                 |     18.816K |\n",
      "|   score_predictor.1              |   60.722K              |   10.729M   |\n",
      "|    score_predictor.1.in_conv     |    37.44K              |    6.657M   |\n",
      "|     score_predictor.1.in_conv.0  |     0.384K             |     0.169M  |\n",
      "|     score_predictor.1.in_conv.1  |     37.056K            |     6.488M  |\n",
      "|    score_predictor.1.out_conv    |    23.282K             |    4.072M   |\n",
      "|     score_predictor.1.out_conv.0 |     18.528K            |     3.244M  |\n",
      "|     score_predictor.1.out_conv.2 |     4.656K             |     0.811M  |\n",
      "|     score_predictor.1.out_conv.4 |     98                 |     16.896K |\n",
      "|   score_predictor.2              |   60.722K              |   9.632M    |\n",
      "|    score_predictor.2.in_conv     |    37.44K              |    5.976M   |\n",
      "|     score_predictor.2.in_conv.0  |     0.384K             |     0.152M  |\n",
      "|     score_predictor.2.in_conv.1  |     37.056K            |     5.825M  |\n",
      "|    score_predictor.2.out_conv    |    23.282K             |    3.655M   |\n",
      "|     score_predictor.2.out_conv.0 |     18.528K            |     2.912M  |\n",
      "|     score_predictor.2.out_conv.2 |     4.656K             |     0.728M  |\n",
      "|     score_predictor.2.out_conv.4 |     98                 |     15.168K |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "VisionTransformerDiffPruning(\n",
      "  #params: 5.9M, #flops: 1.1G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.15M, #flops: 28.9M\n",
      "    (proj): Conv2d(\n",
      "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.15M, #flops: 28.9M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 5.34M, #flops: 1.03G\n",
      "    (0): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.44M, #flops: 0.1G\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 43.95M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 21.79M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.26M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 58.1M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.44M, #flops: 90.67M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 38.13M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 19.57M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.52M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.17M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 52.2M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 26.1M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.44M, #flops: 80.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 33.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.58M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.86M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.89M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.44M, #flops: 80.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 33.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.58M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.86M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.89M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.44M, #flops: 80.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 33.15M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 17.58M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.86M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.15M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 46.89M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 23.45M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.44M, #flops: 71.39M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.94M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.81M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.27M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 42.17M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.44M, #flops: 71.39M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.94M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.81M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.27M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 42.17M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.44M, #flops: 71.39M\n",
      "      (norm1): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.15M, #flops: 28.94M\n",
      "        (qkv): Linear(\n",
      "          in_features=192, out_features=576, bias=True\n",
      "          #params: 0.11M, #flops: 15.81M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.27M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (192,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.38K, #flops: 0.14M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.3M, #flops: 42.17M\n",
      "        (fc1): Linear(\n",
      "          in_features=192, out_features=768, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=768, out_features=192, bias=True\n",
      "          #params: 0.15M, #flops: 21.09M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (192,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.38K, #flops: 0.14M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=192, out_features=1000, bias=True\n",
      "    #params: 0.19M, #flops: 0.19M\n",
      "  )\n",
      "  (score_predictor): ModuleList(\n",
      "    #params: 0.18M, #flops: 32.31M\n",
      "    (0): PredictorLG(\n",
      "      #params: 60.72K, #flops: 11.95M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 37.44K, #flops: 7.41M\n",
      "        (0): LayerNorm(\n",
      "          (192,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.38K, #flops: 0.19M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 7.23M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 23.28K, #flops: 4.53M\n",
      "        (0): Linear(\n",
      "          in_features=192, out_features=96, bias=True\n",
      "          #params: 18.53K, #flops: 3.61M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=96, out_features=48, bias=True\n",
      "          #params: 4.66K, #flops: 0.9M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=48, out_features=2, bias=True\n",
      "          #params: 98, #flops: 18.82K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (1): PredictorLG(\n",
      "      #params: 60.72K, #flops: 10.73M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 37.44K, #flops: 6.66M\n",
      "        (0): LayerNorm(\n",
      "          (192,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.38K, #flops: 0.17M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 6.49M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 23.28K, #flops: 4.07M\n",
      "        (0): Linear(\n",
      "          in_features=192, out_features=96, bias=True\n",
      "          #params: 18.53K, #flops: 3.24M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=96, out_features=48, bias=True\n",
      "          #params: 4.66K, #flops: 0.81M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=48, out_features=2, bias=True\n",
      "          #params: 98, #flops: 16.9K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (2): PredictorLG(\n",
      "      #params: 60.72K, #flops: 9.63M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 37.44K, #flops: 5.98M\n",
      "        (0): LayerNorm(\n",
      "          (192,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.38K, #flops: 0.15M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=192, out_features=192, bias=True\n",
      "          #params: 37.06K, #flops: 5.82M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 23.28K, #flops: 3.66M\n",
      "        (0): Linear(\n",
      "          in_features=192, out_features=96, bias=True\n",
      "          #params: 18.53K, #flops: 2.91M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=96, out_features=48, bias=True\n",
      "          #params: 4.66K, #flops: 0.73M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=48, out_features=2, bias=True\n",
      "          #params: 98, #flops: 15.17K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "1096035456\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT-tiny\n",
    "base_rate = 0.9\n",
    "model_path = \"logs/dynamic-vit_deit-tiny-0.9/checkpoint_best.pth\"\n",
    "\n",
    "PRUNING_LOC = [3,6,9]\n",
    "KEEP_RATE = [base_rate, base_rate ** 2, base_rate ** 3]\n",
    "model = VisionTransformerDiffPruning(\n",
    "            patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True, \n",
    "            pruning_loc=PRUNING_LOC, token_ratio=KEEP_RATE\n",
    "            )\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "model = model.eval()\n",
    "#device = 'cuda:0'\n",
    "#model = model.to(device)\n",
    "#inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2bd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## diff vit pruning method\n",
      "| module                           | #parameters or shape   | #flops      |\n",
      "|:---------------------------------|:-----------------------|:------------|\n",
      "| model                            | 10.305M                | 1.379G      |\n",
      "|  cls_token                       |  (1, 1, 256)           |             |\n",
      "|  pos_embed                       |  (1, 197, 256)         |             |\n",
      "|  patch_embed.proj                |  0.197M                |  38.535M    |\n",
      "|   patch_embed.proj.weight        |   (256, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias          |   (256,)               |             |\n",
      "|  blocks                          |  9.477M                |  1.294G     |\n",
      "|   blocks.0                       |   0.79M                |   0.175G    |\n",
      "|    blocks.0.norm1                |    0.512K              |    0.252M   |\n",
      "|     blocks.0.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.0.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.0.attn                 |    0.263M              |    71.513M  |\n",
      "|     blocks.0.attn.qkv            |     0.197M             |     38.732M |\n",
      "|     blocks.0.attn.proj           |     65.792K            |     12.911M |\n",
      "|    blocks.0.norm2                |    0.512K              |    0.252M   |\n",
      "|     blocks.0.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.0.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.0.mlp                  |    0.526M              |    0.103G   |\n",
      "|     blocks.0.mlp.fc1             |     0.263M             |     51.642M |\n",
      "|     blocks.0.mlp.fc2             |     0.262M             |     51.642M |\n",
      "|   blocks.1                       |   0.79M                |   0.175G    |\n",
      "|    blocks.1.norm1                |    0.512K              |    0.252M   |\n",
      "|     blocks.1.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.1.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.1.attn                 |    0.263M              |    71.513M  |\n",
      "|     blocks.1.attn.qkv            |     0.197M             |     38.732M |\n",
      "|     blocks.1.attn.proj           |     65.792K            |     12.911M |\n",
      "|    blocks.1.norm2                |    0.512K              |    0.252M   |\n",
      "|     blocks.1.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.1.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.1.mlp                  |    0.526M              |    0.103G   |\n",
      "|     blocks.1.mlp.fc1             |     0.263M             |     51.642M |\n",
      "|     blocks.1.mlp.fc2             |     0.262M             |     51.642M |\n",
      "|   blocks.2                       |   0.79M                |   0.175G    |\n",
      "|    blocks.2.norm1                |    0.512K              |    0.252M   |\n",
      "|     blocks.2.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.2.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.2.attn                 |    0.263M              |    71.513M  |\n",
      "|     blocks.2.attn.qkv            |     0.197M             |     38.732M |\n",
      "|     blocks.2.attn.proj           |     65.792K            |     12.911M |\n",
      "|    blocks.2.norm2                |    0.512K              |    0.252M   |\n",
      "|     blocks.2.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.2.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.2.mlp                  |    0.526M              |    0.103G   |\n",
      "|     blocks.2.mlp.fc1             |     0.263M             |     51.642M |\n",
      "|     blocks.2.mlp.fc2             |     0.262M             |     51.642M |\n",
      "|   blocks.3                       |   0.79M                |   0.119G    |\n",
      "|    blocks.3.norm1                |    0.512K              |    0.177M   |\n",
      "|     blocks.3.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.3.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.3.attn                 |    0.263M              |    45.926M  |\n",
      "|     blocks.3.attn.qkv            |     0.197M             |     27.132M |\n",
      "|     blocks.3.attn.proj           |     65.792K            |     9.044M  |\n",
      "|    blocks.3.norm2                |    0.512K              |    0.177M   |\n",
      "|     blocks.3.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.3.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.3.mlp                  |    0.526M              |    72.352M  |\n",
      "|     blocks.3.mlp.fc1             |     0.263M             |     36.176M |\n",
      "|     blocks.3.mlp.fc2             |     0.262M             |     36.176M |\n",
      "|   blocks.4                       |   0.79M                |   0.119G    |\n",
      "|    blocks.4.norm1                |    0.512K              |    0.177M   |\n",
      "|     blocks.4.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.4.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.4.attn                 |    0.263M              |    45.926M  |\n",
      "|     blocks.4.attn.qkv            |     0.197M             |     27.132M |\n",
      "|     blocks.4.attn.proj           |     65.792K            |     9.044M  |\n",
      "|    blocks.4.norm2                |    0.512K              |    0.177M   |\n",
      "|     blocks.4.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.4.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.4.mlp                  |    0.526M              |    72.352M  |\n",
      "|     blocks.4.mlp.fc1             |     0.263M             |     36.176M |\n",
      "|     blocks.4.mlp.fc2             |     0.262M             |     36.176M |\n",
      "|   blocks.5                       |   0.79M                |   0.119G    |\n",
      "|    blocks.5.norm1                |    0.512K              |    0.177M   |\n",
      "|     blocks.5.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.5.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.5.attn                 |    0.263M              |    45.926M  |\n",
      "|     blocks.5.attn.qkv            |     0.197M             |     27.132M |\n",
      "|     blocks.5.attn.proj           |     65.792K            |     9.044M  |\n",
      "|    blocks.5.norm2                |    0.512K              |    0.177M   |\n",
      "|     blocks.5.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.5.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.5.mlp                  |    0.526M              |    72.352M  |\n",
      "|     blocks.5.mlp.fc1             |     0.263M             |     36.176M |\n",
      "|     blocks.5.mlp.fc2             |     0.262M             |     36.176M |\n",
      "|   blocks.6                       |   0.79M                |   81.35M    |\n",
      "|    blocks.6.norm1                |    0.512K              |    0.124M   |\n",
      "|     blocks.6.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.6.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.6.attn                 |    0.263M              |    30.245M  |\n",
      "|     blocks.6.attn.qkv            |     0.197M             |     19.071M |\n",
      "|     blocks.6.attn.proj           |     65.792K            |     6.357M  |\n",
      "|    blocks.6.norm2                |    0.512K              |    0.124M   |\n",
      "|     blocks.6.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.6.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.6.mlp                  |    0.526M              |    50.856M  |\n",
      "|     blocks.6.mlp.fc1             |     0.263M             |     25.428M |\n",
      "|     blocks.6.mlp.fc2             |     0.262M             |     25.428M |\n",
      "|   blocks.7                       |   0.79M                |   81.35M    |\n",
      "|    blocks.7.norm1                |    0.512K              |    0.124M   |\n",
      "|     blocks.7.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.7.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.7.attn                 |    0.263M              |    30.245M  |\n",
      "|     blocks.7.attn.qkv            |     0.197M             |     19.071M |\n",
      "|     blocks.7.attn.proj           |     65.792K            |     6.357M  |\n",
      "|    blocks.7.norm2                |    0.512K              |    0.124M   |\n",
      "|     blocks.7.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.7.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.7.mlp                  |    0.526M              |    50.856M  |\n",
      "|     blocks.7.mlp.fc1             |     0.263M             |     25.428M |\n",
      "|     blocks.7.mlp.fc2             |     0.262M             |     25.428M |\n",
      "|   blocks.8                       |   0.79M                |   81.35M    |\n",
      "|    blocks.8.norm1                |    0.512K              |    0.124M   |\n",
      "|     blocks.8.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.8.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.8.attn                 |    0.263M              |    30.245M  |\n",
      "|     blocks.8.attn.qkv            |     0.197M             |     19.071M |\n",
      "|     blocks.8.attn.proj           |     65.792K            |     6.357M  |\n",
      "|    blocks.8.norm2                |    0.512K              |    0.124M   |\n",
      "|     blocks.8.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.8.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.8.mlp                  |    0.526M              |    50.856M  |\n",
      "|     blocks.8.mlp.fc1             |     0.263M             |     25.428M |\n",
      "|     blocks.8.mlp.fc2             |     0.262M             |     25.428M |\n",
      "|   blocks.9                       |   0.79M                |   56.019M   |\n",
      "|    blocks.9.norm1                |    0.512K              |    87.04K   |\n",
      "|     blocks.9.norm1.weight        |     (256,)             |             |\n",
      "|     blocks.9.norm1.bias          |     (256,)             |             |\n",
      "|    blocks.9.attn                 |    0.263M              |    20.193M  |\n",
      "|     blocks.9.attn.qkv            |     0.197M             |     13.369M |\n",
      "|     blocks.9.attn.proj           |     65.792K            |     4.456M  |\n",
      "|    blocks.9.norm2                |    0.512K              |    87.04K   |\n",
      "|     blocks.9.norm2.weight        |     (256,)             |             |\n",
      "|     blocks.9.norm2.bias          |     (256,)             |             |\n",
      "|    blocks.9.mlp                  |    0.526M              |    35.652M  |\n",
      "|     blocks.9.mlp.fc1             |     0.263M             |     17.826M |\n",
      "|     blocks.9.mlp.fc2             |     0.262M             |     17.826M |\n",
      "|   blocks.10                      |   0.79M                |   56.019M   |\n",
      "|    blocks.10.norm1               |    0.512K              |    87.04K   |\n",
      "|     blocks.10.norm1.weight       |     (256,)             |             |\n",
      "|     blocks.10.norm1.bias         |     (256,)             |             |\n",
      "|    blocks.10.attn                |    0.263M              |    20.193M  |\n",
      "|     blocks.10.attn.qkv           |     0.197M             |     13.369M |\n",
      "|     blocks.10.attn.proj          |     65.792K            |     4.456M  |\n",
      "|    blocks.10.norm2               |    0.512K              |    87.04K   |\n",
      "|     blocks.10.norm2.weight       |     (256,)             |             |\n",
      "|     blocks.10.norm2.bias         |     (256,)             |             |\n",
      "|    blocks.10.mlp                 |    0.526M              |    35.652M  |\n",
      "|     blocks.10.mlp.fc1            |     0.263M             |     17.826M |\n",
      "|     blocks.10.mlp.fc2            |     0.262M             |     17.826M |\n",
      "|   blocks.11                      |   0.79M                |   56.019M   |\n",
      "|    blocks.11.norm1               |    0.512K              |    87.04K   |\n",
      "|     blocks.11.norm1.weight       |     (256,)             |             |\n",
      "|     blocks.11.norm1.bias         |     (256,)             |             |\n",
      "|    blocks.11.attn                |    0.263M              |    20.193M  |\n",
      "|     blocks.11.attn.qkv           |     0.197M             |     13.369M |\n",
      "|     blocks.11.attn.proj          |     65.792K            |     4.456M  |\n",
      "|    blocks.11.norm2               |    0.512K              |    87.04K   |\n",
      "|     blocks.11.norm2.weight       |     (256,)             |             |\n",
      "|     blocks.11.norm2.bias         |     (256,)             |             |\n",
      "|    blocks.11.mlp                 |    0.526M              |    35.652M  |\n",
      "|     blocks.11.mlp.fc1            |     0.263M             |     17.826M |\n",
      "|     blocks.11.mlp.fc2            |     0.262M             |     17.826M |\n",
      "|  norm                            |  0.512K                |  87.04K     |\n",
      "|   norm.weight                    |   (256,)               |             |\n",
      "|   norm.bias                      |   (256,)               |             |\n",
      "|  head                            |  0.257M                |  0.256M     |\n",
      "|   head.weight                    |   (1000, 256)          |             |\n",
      "|   head.bias                      |   (1000,)              |             |\n",
      "|  score_predictor                 |  0.323M                |  46.291M    |\n",
      "|   score_predictor.0              |   0.108M               |   21.149M   |\n",
      "|    score_predictor.0.in_conv     |    66.304K             |    13.096M  |\n",
      "|     score_predictor.0.in_conv.0  |     0.512K             |     0.251M  |\n",
      "|     score_predictor.0.in_conv.1  |     65.792K            |     12.845M |\n",
      "|    score_predictor.0.out_conv    |    41.282K             |    8.053M   |\n",
      "|     score_predictor.0.out_conv.0 |     32.896K            |     6.423M  |\n",
      "|     score_predictor.0.out_conv.2 |     8.256K             |     1.606M  |\n",
      "|     score_predictor.0.out_conv.4 |     0.13K              |     25.088K |\n",
      "|   score_predictor.1              |   0.108M               |   14.783M   |\n",
      "|    score_predictor.1.in_conv     |    66.304K             |    9.154M   |\n",
      "|     score_predictor.1.in_conv.0  |     0.512K             |     0.175M  |\n",
      "|     score_predictor.1.in_conv.1  |     65.792K            |     8.978M  |\n",
      "|    score_predictor.1.out_conv    |    41.282K             |    5.629M   |\n",
      "|     score_predictor.1.out_conv.0 |     32.896K            |     4.489M  |\n",
      "|     score_predictor.1.out_conv.2 |     8.256K             |     1.122M  |\n",
      "|     score_predictor.1.out_conv.4 |     0.13K              |     17.536K |\n",
      "|   score_predictor.2              |   0.108M               |   10.359M   |\n",
      "|    score_predictor.2.in_conv     |    66.304K             |    6.414M   |\n",
      "|     score_predictor.2.in_conv.0  |     0.512K             |     0.123M  |\n",
      "|     score_predictor.2.in_conv.1  |     65.792K            |     6.291M  |\n",
      "|    score_predictor.2.out_conv    |    41.282K             |    3.944M   |\n",
      "|     score_predictor.2.out_conv.0 |     32.896K            |     3.146M  |\n",
      "|     score_predictor.2.out_conv.2 |     8.256K             |     0.786M  |\n",
      "|     score_predictor.2.out_conv.4 |     0.13K              |     12.288K |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "VisionTransformerDiffPruning(\n",
      "  #params: 10.3M, #flops: 1.38G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.2M, #flops: 38.54M\n",
      "    (proj): Conv2d(\n",
      "      3, 256, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.2M, #flops: 38.54M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 9.48M, #flops: 1.29G\n",
      "    (0): Block(\n",
      "      #params: 0.79M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.25M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 71.51M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 38.73M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 12.91M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.25M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 0.1G\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 51.64M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 51.64M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 0.79M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.25M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 71.51M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 38.73M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 12.91M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.25M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 0.1G\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 51.64M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 51.64M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 0.79M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.25M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 71.51M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 38.73M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 12.91M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.25M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 0.1G\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 51.64M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 51.64M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 0.79M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 45.93M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 27.13M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 9.04M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 72.35M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 36.18M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 36.18M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 0.79M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 45.93M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 27.13M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 9.04M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 72.35M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 36.18M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 36.18M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 0.79M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.18M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 45.93M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 27.13M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 9.04M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.18M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 72.35M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 36.18M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 36.18M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 0.79M, #flops: 81.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 30.25M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 19.07M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 6.36M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 50.86M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 25.43M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 25.43M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 0.79M, #flops: 81.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 30.25M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 19.07M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 6.36M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 50.86M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 25.43M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 25.43M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 0.79M, #flops: 81.35M\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.12M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 30.25M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 19.07M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 6.36M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 0.12M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 50.86M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 25.43M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 25.43M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 0.79M, #flops: 56.02M\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 87.04K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 20.19M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 13.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 4.46M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 87.04K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 35.65M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 17.83M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 17.83M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 0.79M, #flops: 56.02M\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 87.04K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 20.19M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 13.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 4.46M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 87.04K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 35.65M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 17.83M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 17.83M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 0.79M, #flops: 56.02M\n",
      "      (norm1): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 87.04K\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.26M, #flops: 20.19M\n",
      "        (qkv): Linear(\n",
      "          in_features=256, out_features=768, bias=True\n",
      "          #params: 0.2M, #flops: 13.37M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 4.46M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (256,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.51K, #flops: 87.04K\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 0.53M, #flops: 35.65M\n",
      "        (fc1): Linear(\n",
      "          in_features=256, out_features=1024, bias=True\n",
      "          #params: 0.26M, #flops: 17.83M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1024, out_features=256, bias=True\n",
      "          #params: 0.26M, #flops: 17.83M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (256,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.51K, #flops: 87.04K\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=256, out_features=1000, bias=True\n",
      "    #params: 0.26M, #flops: 0.26M\n",
      "  )\n",
      "  (score_predictor): ModuleList(\n",
      "    #params: 0.32M, #flops: 46.29M\n",
      "    (0): PredictorLG(\n",
      "      #params: 0.11M, #flops: 21.15M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 66.3K, #flops: 13.1M\n",
      "        (0): LayerNorm(\n",
      "          (256,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.51K, #flops: 0.25M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 12.85M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 41.28K, #flops: 8.05M\n",
      "        (0): Linear(\n",
      "          in_features=256, out_features=128, bias=True\n",
      "          #params: 32.9K, #flops: 6.42M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=128, out_features=64, bias=True\n",
      "          #params: 8.26K, #flops: 1.61M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=64, out_features=2, bias=True\n",
      "          #params: 0.13K, #flops: 25.09K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (1): PredictorLG(\n",
      "      #params: 0.11M, #flops: 14.78M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 66.3K, #flops: 9.15M\n",
      "        (0): LayerNorm(\n",
      "          (256,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.51K, #flops: 0.18M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 8.98M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 41.28K, #flops: 5.63M\n",
      "        (0): Linear(\n",
      "          in_features=256, out_features=128, bias=True\n",
      "          #params: 32.9K, #flops: 4.49M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=128, out_features=64, bias=True\n",
      "          #params: 8.26K, #flops: 1.12M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=64, out_features=2, bias=True\n",
      "          #params: 0.13K, #flops: 17.54K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (2): PredictorLG(\n",
      "      #params: 0.11M, #flops: 10.36M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 66.3K, #flops: 6.41M\n",
      "        (0): LayerNorm(\n",
      "          (256,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.51K, #flops: 0.12M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=256, out_features=256, bias=True\n",
      "          #params: 65.79K, #flops: 6.29M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 41.28K, #flops: 3.94M\n",
      "        (0): Linear(\n",
      "          in_features=256, out_features=128, bias=True\n",
      "          #params: 32.9K, #flops: 3.15M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=128, out_features=64, bias=True\n",
      "          #params: 8.26K, #flops: 0.79M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=64, out_features=2, bias=True\n",
      "          #params: 0.13K, #flops: 12.29K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "1379073920\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT-256\n",
    "base_rate = 0.7\n",
    "model_path = \"models/dynamic-vit_256_r0.7.pth\"\n",
    "\n",
    "PRUNING_LOC = [3,6,9]\n",
    "KEEP_RATE = [base_rate, base_rate ** 2, base_rate ** 3]\n",
    "model = VisionTransformerDiffPruning(\n",
    "            patch_size=16, embed_dim=256, depth=12, num_heads=4, mlp_ratio=4, qkv_bias=True, \n",
    "            pruning_loc=PRUNING_LOC, token_ratio=KEEP_RATE\n",
    "            )\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "model = model.eval()\n",
    "#device = 'cuda:0'\n",
    "#model = model.to(device)\n",
    "#inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1341fa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## diff vit pruning method\n",
      "| module                           | #parameters or shape   | #flops      |\n",
      "|:---------------------------------|:-----------------------|:------------|\n",
      "| model                            | 22.774M                | 2.988G      |\n",
      "|  cls_token                       |  (1, 1, 384)           |             |\n",
      "|  pos_embed                       |  (1, 197, 384)         |             |\n",
      "|  patch_embed.proj                |  0.295M                |  57.803M    |\n",
      "|   patch_embed.proj.weight        |   (384, 3, 16, 16)     |             |\n",
      "|   patch_embed.proj.bias          |   (384,)               |             |\n",
      "|  blocks                          |  21.294M               |  2.826G     |\n",
      "|   blocks.0                       |   1.774M               |   0.379G    |\n",
      "|    blocks.0.norm1                |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.0.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.0.attn                 |    0.591M              |    0.146G   |\n",
      "|     blocks.0.attn.qkv            |     0.444M             |     87.146M |\n",
      "|     blocks.0.attn.proj           |     0.148M             |     29.049M |\n",
      "|    blocks.0.norm2                |    0.768K              |    0.378M   |\n",
      "|     blocks.0.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.0.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.0.mlp                  |    1.182M              |    0.232G   |\n",
      "|     blocks.0.mlp.fc1             |     0.591M             |     0.116G  |\n",
      "|     blocks.0.mlp.fc2             |     0.59M              |     0.116G  |\n",
      "|   blocks.1                       |   1.774M               |   0.379G    |\n",
      "|    blocks.1.norm1                |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.1.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.1.attn                 |    0.591M              |    0.146G   |\n",
      "|     blocks.1.attn.qkv            |     0.444M             |     87.146M |\n",
      "|     blocks.1.attn.proj           |     0.148M             |     29.049M |\n",
      "|    blocks.1.norm2                |    0.768K              |    0.378M   |\n",
      "|     blocks.1.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.1.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.1.mlp                  |    1.182M              |    0.232G   |\n",
      "|     blocks.1.mlp.fc1             |     0.591M             |     0.116G  |\n",
      "|     blocks.1.mlp.fc2             |     0.59M              |     0.116G  |\n",
      "|   blocks.2                       |   1.774M               |   0.379G    |\n",
      "|    blocks.2.norm1                |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.2.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.2.attn                 |    0.591M              |    0.146G   |\n",
      "|     blocks.2.attn.qkv            |     0.444M             |     87.146M |\n",
      "|     blocks.2.attn.proj           |     0.148M             |     29.049M |\n",
      "|    blocks.2.norm2                |    0.768K              |    0.378M   |\n",
      "|     blocks.2.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.2.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.2.mlp                  |    1.182M              |    0.232G   |\n",
      "|     blocks.2.mlp.fc1             |     0.591M             |     0.116G  |\n",
      "|     blocks.2.mlp.fc2             |     0.59M              |     0.116G  |\n",
      "|   blocks.3                       |   1.774M               |   0.259G    |\n",
      "|    blocks.3.norm1                |    0.768K              |    0.265M   |\n",
      "|     blocks.3.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.3.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.3.attn                 |    0.591M              |    96.022M  |\n",
      "|     blocks.3.attn.qkv            |     0.444M             |     61.047M |\n",
      "|     blocks.3.attn.proj           |     0.148M             |     20.349M |\n",
      "|    blocks.3.norm2                |    0.768K              |    0.265M   |\n",
      "|     blocks.3.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.3.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.3.mlp                  |    1.182M              |    0.163G   |\n",
      "|     blocks.3.mlp.fc1             |     0.591M             |     81.396M |\n",
      "|     blocks.3.mlp.fc2             |     0.59M              |     81.396M |\n",
      "|   blocks.4                       |   1.774M               |   0.259G    |\n",
      "|    blocks.4.norm1                |    0.768K              |    0.265M   |\n",
      "|     blocks.4.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.4.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.4.attn                 |    0.591M              |    96.022M  |\n",
      "|     blocks.4.attn.qkv            |     0.444M             |     61.047M |\n",
      "|     blocks.4.attn.proj           |     0.148M             |     20.349M |\n",
      "|    blocks.4.norm2                |    0.768K              |    0.265M   |\n",
      "|     blocks.4.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.4.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.4.mlp                  |    1.182M              |    0.163G   |\n",
      "|     blocks.4.mlp.fc1             |     0.591M             |     81.396M |\n",
      "|     blocks.4.mlp.fc2             |     0.59M              |     81.396M |\n",
      "|   blocks.5                       |   1.774M               |   0.259G    |\n",
      "|    blocks.5.norm1                |    0.768K              |    0.265M   |\n",
      "|     blocks.5.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.5.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.5.attn                 |    0.591M              |    96.022M  |\n",
      "|     blocks.5.attn.qkv            |     0.444M             |     61.047M |\n",
      "|     blocks.5.attn.proj           |     0.148M             |     20.349M |\n",
      "|    blocks.5.norm2                |    0.768K              |    0.265M   |\n",
      "|     blocks.5.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.5.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.5.mlp                  |    1.182M              |    0.163G   |\n",
      "|     blocks.5.mlp.fc1             |     0.591M             |     81.396M |\n",
      "|     blocks.5.mlp.fc2             |     0.59M              |     81.396M |\n",
      "|   blocks.6                       |   1.774M               |   0.179G    |\n",
      "|    blocks.6.norm1                |    0.768K              |    0.186M   |\n",
      "|     blocks.6.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.6.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.6.attn                 |    0.591M              |    64.439M  |\n",
      "|     blocks.6.attn.qkv            |     0.444M             |     42.91M  |\n",
      "|     blocks.6.attn.proj           |     0.148M             |     14.303M |\n",
      "|    blocks.6.norm2                |    0.768K              |    0.186M   |\n",
      "|     blocks.6.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.6.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.6.mlp                  |    1.182M              |    0.114G   |\n",
      "|     blocks.6.mlp.fc1             |     0.591M             |     57.213M |\n",
      "|     blocks.6.mlp.fc2             |     0.59M              |     57.213M |\n",
      "|   blocks.7                       |   1.774M               |   0.179G    |\n",
      "|    blocks.7.norm1                |    0.768K              |    0.186M   |\n",
      "|     blocks.7.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.7.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.7.attn                 |    0.591M              |    64.439M  |\n",
      "|     blocks.7.attn.qkv            |     0.444M             |     42.91M  |\n",
      "|     blocks.7.attn.proj           |     0.148M             |     14.303M |\n",
      "|    blocks.7.norm2                |    0.768K              |    0.186M   |\n",
      "|     blocks.7.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.7.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.7.mlp                  |    1.182M              |    0.114G   |\n",
      "|     blocks.7.mlp.fc1             |     0.591M             |     57.213M |\n",
      "|     blocks.7.mlp.fc2             |     0.59M              |     57.213M |\n",
      "|   blocks.8                       |   1.774M               |   0.179G    |\n",
      "|    blocks.8.norm1                |    0.768K              |    0.186M   |\n",
      "|     blocks.8.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.8.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.8.attn                 |    0.591M              |    64.439M  |\n",
      "|     blocks.8.attn.qkv            |     0.444M             |     42.91M  |\n",
      "|     blocks.8.attn.proj           |     0.148M             |     14.303M |\n",
      "|    blocks.8.norm2                |    0.768K              |    0.186M   |\n",
      "|     blocks.8.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.8.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.8.mlp                  |    1.182M              |    0.114G   |\n",
      "|     blocks.8.mlp.fc1             |     0.591M             |     57.213M |\n",
      "|     blocks.8.mlp.fc2             |     0.59M              |     57.213M |\n",
      "|   blocks.9                       |   1.774M               |   0.124G    |\n",
      "|    blocks.9.norm1                |    0.768K              |    0.131M   |\n",
      "|     blocks.9.norm1.weight        |     (384,)             |             |\n",
      "|     blocks.9.norm1.bias          |     (384,)             |             |\n",
      "|    blocks.9.attn                 |    0.591M              |    43.659M  |\n",
      "|     blocks.9.attn.qkv            |     0.444M             |     30.081M |\n",
      "|     blocks.9.attn.proj           |     0.148M             |     10.027M |\n",
      "|    blocks.9.norm2                |    0.768K              |    0.131M   |\n",
      "|     blocks.9.norm2.weight        |     (384,)             |             |\n",
      "|     blocks.9.norm2.bias          |     (384,)             |             |\n",
      "|    blocks.9.mlp                  |    1.182M              |    80.216M  |\n",
      "|     blocks.9.mlp.fc1             |     0.591M             |     40.108M |\n",
      "|     blocks.9.mlp.fc2             |     0.59M              |     40.108M |\n",
      "|   blocks.10                      |   1.774M               |   0.124G    |\n",
      "|    blocks.10.norm1               |    0.768K              |    0.131M   |\n",
      "|     blocks.10.norm1.weight       |     (384,)             |             |\n",
      "|     blocks.10.norm1.bias         |     (384,)             |             |\n",
      "|    blocks.10.attn                |    0.591M              |    43.659M  |\n",
      "|     blocks.10.attn.qkv           |     0.444M             |     30.081M |\n",
      "|     blocks.10.attn.proj          |     0.148M             |     10.027M |\n",
      "|    blocks.10.norm2               |    0.768K              |    0.131M   |\n",
      "|     blocks.10.norm2.weight       |     (384,)             |             |\n",
      "|     blocks.10.norm2.bias         |     (384,)             |             |\n",
      "|    blocks.10.mlp                 |    1.182M              |    80.216M  |\n",
      "|     blocks.10.mlp.fc1            |     0.591M             |     40.108M |\n",
      "|     blocks.10.mlp.fc2            |     0.59M              |     40.108M |\n",
      "|   blocks.11                      |   1.774M               |   0.124G    |\n",
      "|    blocks.11.norm1               |    0.768K              |    0.131M   |\n",
      "|     blocks.11.norm1.weight       |     (384,)             |             |\n",
      "|     blocks.11.norm1.bias         |     (384,)             |             |\n",
      "|    blocks.11.attn                |    0.591M              |    43.659M  |\n",
      "|     blocks.11.attn.qkv           |     0.444M             |     30.081M |\n",
      "|     blocks.11.attn.proj          |     0.148M             |     10.027M |\n",
      "|    blocks.11.norm2               |    0.768K              |    0.131M   |\n",
      "|     blocks.11.norm2.weight       |     (384,)             |             |\n",
      "|     blocks.11.norm2.bias         |     (384,)             |             |\n",
      "|    blocks.11.mlp                 |    1.182M              |    80.216M  |\n",
      "|     blocks.11.mlp.fc1            |     0.591M             |     40.108M |\n",
      "|     blocks.11.mlp.fc2            |     0.59M              |     40.108M |\n",
      "|  norm                            |  0.768K                |  0.131M     |\n",
      "|   norm.weight                    |   (384,)               |             |\n",
      "|   norm.bias                      |   (384,)               |             |\n",
      "|  head                            |  0.385M                |  0.384M     |\n",
      "|   head.weight                    |   (1000, 384)          |             |\n",
      "|   head.bias                      |   (1000,)              |             |\n",
      "|  score_predictor                 |  0.724M                |  0.104G     |\n",
      "|   score_predictor.0              |   0.241M               |   47.379M   |\n",
      "|    score_predictor.0.in_conv     |    0.149M              |    29.278M  |\n",
      "|     score_predictor.0.in_conv.0  |     0.768K             |     0.376M  |\n",
      "|     score_predictor.0.in_conv.1  |     0.148M             |     28.901M |\n",
      "|    score_predictor.0.out_conv    |    92.642K             |    18.101M  |\n",
      "|     score_predictor.0.out_conv.0 |     73.92K             |     14.451M |\n",
      "|     score_predictor.0.out_conv.2 |     18.528K            |     3.613M  |\n",
      "|     score_predictor.0.out_conv.4 |     0.194K             |     37.632K |\n",
      "|   score_predictor.1              |   0.241M               |   33.117M   |\n",
      "|    score_predictor.1.in_conv     |    0.149M              |    20.465M  |\n",
      "|     score_predictor.1.in_conv.0  |     0.768K             |     0.263M  |\n",
      "|     score_predictor.1.in_conv.1  |     0.148M             |     20.201M |\n",
      "|    score_predictor.1.out_conv    |    92.642K             |    12.652M  |\n",
      "|     score_predictor.1.out_conv.0 |     73.92K             |     10.101M |\n",
      "|     score_predictor.1.out_conv.2 |     18.528K            |     2.525M  |\n",
      "|     score_predictor.1.out_conv.4 |     0.194K             |     26.304K |\n",
      "|   score_predictor.2              |   0.241M               |   23.206M   |\n",
      "|    score_predictor.2.in_conv     |    0.149M              |    14.34M   |\n",
      "|     score_predictor.2.in_conv.0  |     0.768K             |     0.184M  |\n",
      "|     score_predictor.2.in_conv.1  |     0.148M             |     14.156M |\n",
      "|    score_predictor.2.out_conv    |    92.642K             |    8.866M   |\n",
      "|     score_predictor.2.out_conv.0 |     73.92K             |     7.078M  |\n",
      "|     score_predictor.2.out_conv.2 |     18.528K            |     1.769M  |\n",
      "|     score_predictor.2.out_conv.4 |     0.194K             |     18.432K |\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "VisionTransformerDiffPruning(\n",
      "  #params: 22.77M, #flops: 2.99G\n",
      "  (patch_embed): PatchEmbed(\n",
      "    #params: 0.3M, #flops: 57.8M\n",
      "    (proj): Conv2d(\n",
      "      3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
      "      #params: 0.3M, #flops: 57.8M\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    #params: 21.29M, #flops: 2.83G\n",
      "    (0): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      #params: 1.77M, #flops: 0.38G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 0.15G\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 87.15M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 29.05M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.38M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.23G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 0.12G\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      #params: 1.77M, #flops: 0.26G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 96.02M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 61.05M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.35M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.26M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.16G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 81.4M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 64.44M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.91M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.3M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 57.21M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 57.21M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 64.44M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.91M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.3M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 57.21M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 57.21M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      #params: 1.77M, #flops: 0.18G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 64.44M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 42.91M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.3M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.19M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 0.11G\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 57.21M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 57.21M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 43.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 30.08M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.03M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 80.22M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 40.11M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 40.11M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 43.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 30.08M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.03M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 80.22M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 40.11M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 40.11M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      #params: 1.77M, #flops: 0.12G\n",
      "      (norm1): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        #params: 0.59M, #flops: 43.66M\n",
      "        (qkv): Linear(\n",
      "          in_features=384, out_features=1152, bias=True\n",
      "          #params: 0.44M, #flops: 30.08M\n",
      "        )\n",
      "        (attn_drop): Dropout(\n",
      "          p=0.0, inplace=False\n",
      "          #params: 0, #flops: N/A\n",
      "        )\n",
      "        (proj): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 10.03M\n",
      "        )\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(#params: 0, #flops: N/A)\n",
      "      (norm2): LayerNorm(\n",
      "        (384,), eps=1e-06, elementwise_affine=True\n",
      "        #params: 0.77K, #flops: 0.13M\n",
      "      )\n",
      "      (mlp): Mlp(\n",
      "        #params: 1.18M, #flops: 80.22M\n",
      "        (fc1): Linear(\n",
      "          in_features=384, out_features=1536, bias=True\n",
      "          #params: 0.59M, #flops: 40.11M\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(\n",
      "          in_features=1536, out_features=384, bias=True\n",
      "          #params: 0.59M, #flops: 40.11M\n",
      "        )\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm(\n",
      "    (384,), eps=1e-06, elementwise_affine=True\n",
      "    #params: 0.77K, #flops: 0.13M\n",
      "  )\n",
      "  (pre_logits): Identity(#params: 0, #flops: N/A)\n",
      "  (head): Linear(\n",
      "    in_features=384, out_features=1000, bias=True\n",
      "    #params: 0.39M, #flops: 0.38M\n",
      "  )\n",
      "  (score_predictor): ModuleList(\n",
      "    #params: 0.72M, #flops: 0.1G\n",
      "    (0): PredictorLG(\n",
      "      #params: 0.24M, #flops: 47.38M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 0.15M, #flops: 29.28M\n",
      "        (0): LayerNorm(\n",
      "          (384,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.77K, #flops: 0.38M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 28.9M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 92.64K, #flops: 18.1M\n",
      "        (0): Linear(\n",
      "          in_features=384, out_features=192, bias=True\n",
      "          #params: 73.92K, #flops: 14.45M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=192, out_features=96, bias=True\n",
      "          #params: 18.53K, #flops: 3.61M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=96, out_features=2, bias=True\n",
      "          #params: 0.19K, #flops: 37.63K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (1): PredictorLG(\n",
      "      #params: 0.24M, #flops: 33.12M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 0.15M, #flops: 20.46M\n",
      "        (0): LayerNorm(\n",
      "          (384,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.77K, #flops: 0.26M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 20.2M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 92.64K, #flops: 12.65M\n",
      "        (0): Linear(\n",
      "          in_features=384, out_features=192, bias=True\n",
      "          #params: 73.92K, #flops: 10.1M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=192, out_features=96, bias=True\n",
      "          #params: 18.53K, #flops: 2.53M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=96, out_features=2, bias=True\n",
      "          #params: 0.19K, #flops: 26.3K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (2): PredictorLG(\n",
      "      #params: 0.24M, #flops: 23.21M\n",
      "      (in_conv): Sequential(\n",
      "        #params: 0.15M, #flops: 14.34M\n",
      "        (0): LayerNorm(\n",
      "          (384,), eps=1e-05, elementwise_affine=True\n",
      "          #params: 0.77K, #flops: 0.18M\n",
      "        )\n",
      "        (1): Linear(\n",
      "          in_features=384, out_features=384, bias=True\n",
      "          #params: 0.15M, #flops: 14.16M\n",
      "        )\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (out_conv): Sequential(\n",
      "        #params: 92.64K, #flops: 8.87M\n",
      "        (0): Linear(\n",
      "          in_features=384, out_features=192, bias=True\n",
      "          #params: 73.92K, #flops: 7.08M\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): Linear(\n",
      "          in_features=192, out_features=96, bias=True\n",
      "          #params: 18.53K, #flops: 1.77M\n",
      "        )\n",
      "        (3): GELU()\n",
      "        (4): Linear(\n",
      "          in_features=96, out_features=2, bias=True\n",
      "          #params: 0.19K, #flops: 18.43K\n",
      "        )\n",
      "        (5): LogSoftmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2987611968\n"
     ]
    }
   ],
   "source": [
    "#Testing DEIT-small\n",
    "base_rate = 0.7\n",
    "model_path = \"models/dynamic-vit_384_r0.7.pth\"\n",
    "\n",
    "PRUNING_LOC = [3,6,9]\n",
    "KEEP_RATE = [base_rate, base_rate ** 2, base_rate ** 3]\n",
    "model = VisionTransformerDiffPruning(\n",
    "            patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True, \n",
    "            pruning_loc=PRUNING_LOC, token_ratio=KEEP_RATE\n",
    "            )\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "model = model.eval()\n",
    "#device = 'cuda:0'\n",
    "#model = model.to(device)\n",
    "#inputs = inputs.to(device)\n",
    "# op = model(inputs)\n",
    "flop = FlopCountAnalysis(model, inputs)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print(flop_count_str(flop))\n",
    "print(flop.total())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
